{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1663598306493,
     "user": {
      "displayName": "Bowen Jumbo",
      "userId": "15176400549218670876"
     },
     "user_tz": -60
    },
    "id": "ieTqKotPCdhe"
   },
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "# Createing a path variable \n",
    "config_file = './Yolo_1000_train_150000_test/config.yaml'\n",
    "\n",
    "# Set video type \n",
    "# ! for different projects, the video types could be different. Make sure the type is correct! \n",
    "VideoType = 'mp4'  \n",
    "\n",
    "# Set video path\n",
    "# ! make sure the video path contains the video file name. For example, in this project, m3v1mp4.mp4\n",
    "vid_path_0 = ['./Video/Yolo/Yolo_output_0.mp4']\n",
    "vid_path_1 = ['./Video/Yolo/Yolo_output_1.mp4'] #Enter the list of videos or folder to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465589,
     "status": "ok",
     "timestamp": 1663149080012,
     "user": {
      "displayName": "Bowen Jumbo",
      "userId": "15176400549218670876"
     },
     "user_tz": -60
    },
    "id": "kbD-tu1CcxLb",
    "outputId": "f346af49-f9a8-4448-9803-ed5c0cae2fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:7, net_type:resnet_50, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:8, net_type:resnet_101, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:9, net_type:resnet_152, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:10, net_type:mobilenet_v2_1.0, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:11, net_type:mobilenet_v2_0.75, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:12, net_type:mobilenet_v2_0.5, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Downloading a ImageNet-pretrained model from https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz....\n",
      "You passed a split with the following fraction: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle index:13, net_type:mobilenet_v2_0.35, augmenter_type:imgaug, trainsetindex:0, frozen shuffle ID:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training dataset with different networks\n",
    "deeplabcut.create_training_model_comparison(path_config_file,num_shuffles=1,net_types=['resnet_50', 'resnet_101','resnet_152', 'mobilenet_v2_1.0','mobilenet_v2_0.75', 'mobilenet_v2_0.5', 'mobilenet_v2_0.35'],augmenter_types=['imgaug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbikpbWHI9Z-",
    "outputId": "230258d6-eb40-49a7-ac54-7f3ef334b3f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle7.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle7.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 150000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/content/drive/My '\n",
      "                 'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/content/drive/My '\n",
      "                    'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle7/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading ImageNet-pretrained resnet_50\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 500\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle7/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle7.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle7.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 150000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0511 lr: 0.005\n",
      "iteration: 200 loss: 0.0202 lr: 0.005\n",
      "iteration: 300 loss: 0.0179 lr: 0.005\n",
      "iteration: 400 loss: 0.0172 lr: 0.005\n",
      "iteration: 500 loss: 0.0157 lr: 0.005\n",
      "iteration: 600 loss: 0.0158 lr: 0.005\n",
      "iteration: 700 loss: 0.0129 lr: 0.005\n",
      "iteration: 800 loss: 0.0139 lr: 0.005\n",
      "iteration: 900 loss: 0.0133 lr: 0.005\n",
      "iteration: 1000 loss: 0.0117 lr: 0.005\n",
      "iteration: 1100 loss: 0.0111 lr: 0.005\n",
      "iteration: 1200 loss: 0.0109 lr: 0.005\n",
      "iteration: 1300 loss: 0.0114 lr: 0.005\n",
      "iteration: 1400 loss: 0.0095 lr: 0.005\n",
      "iteration: 1500 loss: 0.0099 lr: 0.005\n",
      "iteration: 1600 loss: 0.0081 lr: 0.005\n",
      "iteration: 1700 loss: 0.0097 lr: 0.005\n",
      "iteration: 1800 loss: 0.0096 lr: 0.005\n",
      "iteration: 1900 loss: 0.0089 lr: 0.005\n",
      "iteration: 2000 loss: 0.0095 lr: 0.005\n",
      "iteration: 2100 loss: 0.0090 lr: 0.005\n",
      "iteration: 2200 loss: 0.0087 lr: 0.005\n",
      "iteration: 2300 loss: 0.0087 lr: 0.005\n",
      "iteration: 2400 loss: 0.0090 lr: 0.005\n",
      "iteration: 2500 loss: 0.0093 lr: 0.005\n",
      "iteration: 2600 loss: 0.0085 lr: 0.005\n",
      "iteration: 2700 loss: 0.0079 lr: 0.005\n",
      "iteration: 2800 loss: 0.0072 lr: 0.005\n",
      "iteration: 2900 loss: 0.0076 lr: 0.005\n",
      "iteration: 3000 loss: 0.0080 lr: 0.005\n",
      "iteration: 3100 loss: 0.0085 lr: 0.005\n",
      "iteration: 3200 loss: 0.0074 lr: 0.005\n",
      "iteration: 3300 loss: 0.0080 lr: 0.005\n",
      "iteration: 3400 loss: 0.0076 lr: 0.005\n",
      "iteration: 3500 loss: 0.0072 lr: 0.005\n",
      "iteration: 3600 loss: 0.0077 lr: 0.005\n",
      "iteration: 3700 loss: 0.0074 lr: 0.005\n",
      "iteration: 3800 loss: 0.0074 lr: 0.005\n",
      "iteration: 3900 loss: 0.0074 lr: 0.005\n",
      "iteration: 4000 loss: 0.0069 lr: 0.005\n",
      "iteration: 4100 loss: 0.0071 lr: 0.005\n",
      "iteration: 4200 loss: 0.0076 lr: 0.005\n",
      "iteration: 4300 loss: 0.0068 lr: 0.005\n",
      "iteration: 4400 loss: 0.0074 lr: 0.005\n",
      "iteration: 4500 loss: 0.0071 lr: 0.005\n",
      "iteration: 4600 loss: 0.0069 lr: 0.005\n",
      "iteration: 4700 loss: 0.0067 lr: 0.005\n",
      "iteration: 4800 loss: 0.0066 lr: 0.005\n",
      "iteration: 4900 loss: 0.0063 lr: 0.005\n",
      "iteration: 5000 loss: 0.0070 lr: 0.005\n",
      "iteration: 5100 loss: 0.0069 lr: 0.005\n",
      "iteration: 5200 loss: 0.0064 lr: 0.005\n",
      "iteration: 5300 loss: 0.0061 lr: 0.005\n",
      "iteration: 5400 loss: 0.0064 lr: 0.005\n",
      "iteration: 5500 loss: 0.0067 lr: 0.005\n",
      "iteration: 5600 loss: 0.0061 lr: 0.005\n",
      "iteration: 5700 loss: 0.0060 lr: 0.005\n",
      "iteration: 5800 loss: 0.0069 lr: 0.005\n",
      "iteration: 5900 loss: 0.0068 lr: 0.005\n",
      "iteration: 6000 loss: 0.0061 lr: 0.005\n",
      "iteration: 6100 loss: 0.0063 lr: 0.005\n",
      "iteration: 6200 loss: 0.0065 lr: 0.005\n",
      "iteration: 6300 loss: 0.0062 lr: 0.005\n",
      "iteration: 6400 loss: 0.0073 lr: 0.005\n",
      "iteration: 6500 loss: 0.0062 lr: 0.005\n",
      "iteration: 6600 loss: 0.0061 lr: 0.005\n",
      "iteration: 6700 loss: 0.0056 lr: 0.005\n",
      "iteration: 6800 loss: 0.0057 lr: 0.005\n",
      "iteration: 6900 loss: 0.0063 lr: 0.005\n",
      "iteration: 7000 loss: 0.0059 lr: 0.005\n",
      "iteration: 7100 loss: 0.0068 lr: 0.005\n",
      "iteration: 7200 loss: 0.0063 lr: 0.005\n",
      "iteration: 7300 loss: 0.0058 lr: 0.005\n",
      "iteration: 7400 loss: 0.0054 lr: 0.005\n",
      "iteration: 7500 loss: 0.0060 lr: 0.005\n",
      "iteration: 7600 loss: 0.0058 lr: 0.005\n",
      "iteration: 7700 loss: 0.0052 lr: 0.005\n",
      "iteration: 7800 loss: 0.0061 lr: 0.005\n",
      "iteration: 7900 loss: 0.0063 lr: 0.005\n",
      "iteration: 8000 loss: 0.0055 lr: 0.005\n",
      "iteration: 8100 loss: 0.0059 lr: 0.005\n",
      "iteration: 8200 loss: 0.0050 lr: 0.005\n",
      "iteration: 8300 loss: 0.0057 lr: 0.005\n",
      "iteration: 8400 loss: 0.0045 lr: 0.005\n",
      "iteration: 8500 loss: 0.0055 lr: 0.005\n",
      "iteration: 8600 loss: 0.0065 lr: 0.005\n",
      "iteration: 8700 loss: 0.0064 lr: 0.005\n",
      "iteration: 8800 loss: 0.0057 lr: 0.005\n",
      "iteration: 8900 loss: 0.0062 lr: 0.005\n",
      "iteration: 9000 loss: 0.0054 lr: 0.005\n",
      "iteration: 9100 loss: 0.0051 lr: 0.005\n",
      "iteration: 9200 loss: 0.0055 lr: 0.005\n",
      "iteration: 9300 loss: 0.0056 lr: 0.005\n",
      "iteration: 9400 loss: 0.0055 lr: 0.005\n",
      "iteration: 9500 loss: 0.0056 lr: 0.005\n",
      "iteration: 9600 loss: 0.0055 lr: 0.005\n",
      "iteration: 9700 loss: 0.0066 lr: 0.005\n",
      "iteration: 9800 loss: 0.0052 lr: 0.005\n",
      "iteration: 9900 loss: 0.0054 lr: 0.005\n",
      "iteration: 10000 loss: 0.0056 lr: 0.005\n",
      "iteration: 10100 loss: 0.0057 lr: 0.005\n",
      "iteration: 10200 loss: 0.0053 lr: 0.005\n",
      "iteration: 10300 loss: 0.0056 lr: 0.005\n",
      "iteration: 10400 loss: 0.0051 lr: 0.005\n",
      "iteration: 10500 loss: 0.0059 lr: 0.005\n",
      "iteration: 10600 loss: 0.0055 lr: 0.005\n",
      "iteration: 10700 loss: 0.0049 lr: 0.005\n",
      "iteration: 10800 loss: 0.0045 lr: 0.005\n",
      "iteration: 10900 loss: 0.0050 lr: 0.005\n",
      "iteration: 11000 loss: 0.0044 lr: 0.005\n",
      "iteration: 11100 loss: 0.0053 lr: 0.005\n",
      "iteration: 11200 loss: 0.0046 lr: 0.005\n",
      "iteration: 11300 loss: 0.0054 lr: 0.005\n",
      "iteration: 11400 loss: 0.0047 lr: 0.005\n",
      "iteration: 11500 loss: 0.0060 lr: 0.005\n",
      "iteration: 11600 loss: 0.0047 lr: 0.005\n",
      "iteration: 11700 loss: 0.0054 lr: 0.005\n",
      "iteration: 11800 loss: 0.0053 lr: 0.005\n",
      "iteration: 11900 loss: 0.0053 lr: 0.005\n",
      "iteration: 12000 loss: 0.0045 lr: 0.005\n",
      "iteration: 12100 loss: 0.0052 lr: 0.005\n",
      "iteration: 12200 loss: 0.0047 lr: 0.005\n",
      "iteration: 12300 loss: 0.0044 lr: 0.005\n",
      "iteration: 12400 loss: 0.0046 lr: 0.005\n",
      "iteration: 12500 loss: 0.0045 lr: 0.005\n",
      "iteration: 12600 loss: 0.0051 lr: 0.005\n",
      "iteration: 12700 loss: 0.0047 lr: 0.005\n",
      "iteration: 12800 loss: 0.0049 lr: 0.005\n",
      "iteration: 12900 loss: 0.0048 lr: 0.005\n",
      "iteration: 13000 loss: 0.0050 lr: 0.005\n",
      "iteration: 13100 loss: 0.0047 lr: 0.005\n",
      "iteration: 13200 loss: 0.0055 lr: 0.005\n",
      "iteration: 13300 loss: 0.0051 lr: 0.005\n",
      "iteration: 13400 loss: 0.0057 lr: 0.005\n",
      "iteration: 13500 loss: 0.0048 lr: 0.005\n",
      "iteration: 13600 loss: 0.0049 lr: 0.005\n",
      "iteration: 13700 loss: 0.0051 lr: 0.005\n",
      "iteration: 13800 loss: 0.0060 lr: 0.005\n",
      "iteration: 13900 loss: 0.0050 lr: 0.005\n",
      "iteration: 14000 loss: 0.0045 lr: 0.005\n",
      "iteration: 14100 loss: 0.0049 lr: 0.005\n",
      "iteration: 14200 loss: 0.0047 lr: 0.005\n",
      "iteration: 14300 loss: 0.0049 lr: 0.005\n",
      "iteration: 14400 loss: 0.0046 lr: 0.005\n",
      "iteration: 14500 loss: 0.0043 lr: 0.005\n",
      "iteration: 14600 loss: 0.0046 lr: 0.005\n",
      "iteration: 14700 loss: 0.0046 lr: 0.005\n",
      "iteration: 14800 loss: 0.0049 lr: 0.005\n",
      "iteration: 14900 loss: 0.0048 lr: 0.005\n",
      "iteration: 15000 loss: 0.0051 lr: 0.005\n",
      "iteration: 15100 loss: 0.0054 lr: 0.005\n",
      "iteration: 15200 loss: 0.0052 lr: 0.005\n",
      "iteration: 15300 loss: 0.0054 lr: 0.005\n",
      "iteration: 15400 loss: 0.0049 lr: 0.005\n",
      "iteration: 15500 loss: 0.0047 lr: 0.005\n",
      "iteration: 15600 loss: 0.0051 lr: 0.005\n",
      "iteration: 15700 loss: 0.0047 lr: 0.005\n",
      "iteration: 15800 loss: 0.0042 lr: 0.005\n",
      "iteration: 15900 loss: 0.0054 lr: 0.005\n",
      "iteration: 16000 loss: 0.0046 lr: 0.005\n",
      "iteration: 16100 loss: 0.0050 lr: 0.005\n",
      "iteration: 16200 loss: 0.0048 lr: 0.005\n",
      "iteration: 16300 loss: 0.0045 lr: 0.005\n",
      "iteration: 16400 loss: 0.0041 lr: 0.005\n",
      "iteration: 16500 loss: 0.0055 lr: 0.005\n",
      "iteration: 16600 loss: 0.0046 lr: 0.005\n",
      "iteration: 16700 loss: 0.0041 lr: 0.005\n",
      "iteration: 16800 loss: 0.0043 lr: 0.005\n",
      "iteration: 16900 loss: 0.0046 lr: 0.005\n",
      "iteration: 17000 loss: 0.0043 lr: 0.005\n",
      "iteration: 17100 loss: 0.0048 lr: 0.005\n",
      "iteration: 17200 loss: 0.0053 lr: 0.005\n",
      "iteration: 17300 loss: 0.0051 lr: 0.005\n",
      "iteration: 17400 loss: 0.0044 lr: 0.005\n",
      "iteration: 17500 loss: 0.0045 lr: 0.005\n",
      "iteration: 17600 loss: 0.0049 lr: 0.005\n",
      "iteration: 17700 loss: 0.0046 lr: 0.005\n",
      "iteration: 17800 loss: 0.0049 lr: 0.005\n",
      "iteration: 17900 loss: 0.0053 lr: 0.005\n",
      "iteration: 18000 loss: 0.0052 lr: 0.005\n",
      "iteration: 18100 loss: 0.0051 lr: 0.005\n",
      "iteration: 18200 loss: 0.0047 lr: 0.005\n",
      "iteration: 18300 loss: 0.0042 lr: 0.005\n",
      "iteration: 18400 loss: 0.0049 lr: 0.005\n",
      "iteration: 18500 loss: 0.0042 lr: 0.005\n",
      "iteration: 18600 loss: 0.0043 lr: 0.005\n",
      "iteration: 18700 loss: 0.0047 lr: 0.005\n",
      "iteration: 18800 loss: 0.0040 lr: 0.005\n",
      "iteration: 18900 loss: 0.0047 lr: 0.005\n",
      "iteration: 19000 loss: 0.0039 lr: 0.005\n",
      "iteration: 19100 loss: 0.0041 lr: 0.005\n",
      "iteration: 19200 loss: 0.0055 lr: 0.005\n",
      "iteration: 19300 loss: 0.0037 lr: 0.005\n",
      "iteration: 19400 loss: 0.0042 lr: 0.005\n",
      "iteration: 19500 loss: 0.0044 lr: 0.005\n",
      "iteration: 19600 loss: 0.0042 lr: 0.005\n",
      "iteration: 19700 loss: 0.0040 lr: 0.005\n",
      "iteration: 19800 loss: 0.0038 lr: 0.005\n",
      "iteration: 19900 loss: 0.0048 lr: 0.005\n",
      "iteration: 20000 loss: 0.0047 lr: 0.005\n",
      "iteration: 20100 loss: 0.0042 lr: 0.005\n",
      "iteration: 20200 loss: 0.0042 lr: 0.005\n",
      "iteration: 20300 loss: 0.0044 lr: 0.005\n",
      "iteration: 20400 loss: 0.0041 lr: 0.005\n",
      "iteration: 20500 loss: 0.0040 lr: 0.005\n",
      "iteration: 20600 loss: 0.0040 lr: 0.005\n",
      "iteration: 20700 loss: 0.0044 lr: 0.005\n",
      "iteration: 20800 loss: 0.0043 lr: 0.005\n",
      "iteration: 20900 loss: 0.0049 lr: 0.005\n",
      "iteration: 21000 loss: 0.0051 lr: 0.005\n",
      "iteration: 21100 loss: 0.0043 lr: 0.005\n",
      "iteration: 21200 loss: 0.0044 lr: 0.005\n",
      "iteration: 21300 loss: 0.0045 lr: 0.005\n",
      "iteration: 21400 loss: 0.0036 lr: 0.005\n",
      "iteration: 21500 loss: 0.0040 lr: 0.005\n",
      "iteration: 21600 loss: 0.0040 lr: 0.005\n",
      "iteration: 21700 loss: 0.0042 lr: 0.005\n",
      "iteration: 21800 loss: 0.0046 lr: 0.005\n",
      "iteration: 21900 loss: 0.0042 lr: 0.005\n",
      "iteration: 22000 loss: 0.0038 lr: 0.005\n",
      "iteration: 22100 loss: 0.0042 lr: 0.005\n",
      "iteration: 22200 loss: 0.0037 lr: 0.005\n",
      "iteration: 22300 loss: 0.0044 lr: 0.005\n",
      "iteration: 22400 loss: 0.0037 lr: 0.005\n",
      "iteration: 22500 loss: 0.0046 lr: 0.005\n",
      "iteration: 22600 loss: 0.0036 lr: 0.005\n",
      "iteration: 22700 loss: 0.0048 lr: 0.005\n",
      "iteration: 22800 loss: 0.0045 lr: 0.005\n",
      "iteration: 22900 loss: 0.0041 lr: 0.005\n",
      "iteration: 23000 loss: 0.0043 lr: 0.005\n",
      "iteration: 23100 loss: 0.0038 lr: 0.005\n",
      "iteration: 23200 loss: 0.0042 lr: 0.005\n",
      "iteration: 23300 loss: 0.0039 lr: 0.005\n",
      "iteration: 23400 loss: 0.0046 lr: 0.005\n",
      "iteration: 23500 loss: 0.0041 lr: 0.005\n",
      "iteration: 23600 loss: 0.0040 lr: 0.005\n",
      "iteration: 23700 loss: 0.0041 lr: 0.005\n",
      "iteration: 23800 loss: 0.0047 lr: 0.005\n",
      "iteration: 23900 loss: 0.0035 lr: 0.005\n",
      "iteration: 24000 loss: 0.0039 lr: 0.005\n",
      "iteration: 24100 loss: 0.0046 lr: 0.005\n",
      "iteration: 24200 loss: 0.0045 lr: 0.005\n",
      "iteration: 24300 loss: 0.0036 lr: 0.005\n",
      "iteration: 24400 loss: 0.0040 lr: 0.005\n",
      "iteration: 24500 loss: 0.0039 lr: 0.005\n",
      "iteration: 24600 loss: 0.0043 lr: 0.005\n",
      "iteration: 24700 loss: 0.0036 lr: 0.005\n",
      "iteration: 24800 loss: 0.0044 lr: 0.005\n",
      "iteration: 24900 loss: 0.0048 lr: 0.005\n",
      "iteration: 25000 loss: 0.0046 lr: 0.005\n",
      "iteration: 25100 loss: 0.0040 lr: 0.005\n",
      "iteration: 25200 loss: 0.0040 lr: 0.005\n",
      "iteration: 25300 loss: 0.0043 lr: 0.005\n",
      "iteration: 25400 loss: 0.0046 lr: 0.005\n",
      "iteration: 25500 loss: 0.0042 lr: 0.005\n",
      "iteration: 25600 loss: 0.0042 lr: 0.005\n",
      "iteration: 25700 loss: 0.0044 lr: 0.005\n",
      "iteration: 25800 loss: 0.0045 lr: 0.005\n",
      "iteration: 25900 loss: 0.0037 lr: 0.005\n",
      "iteration: 26000 loss: 0.0041 lr: 0.005\n",
      "iteration: 26100 loss: 0.0043 lr: 0.005\n",
      "iteration: 26200 loss: 0.0038 lr: 0.005\n",
      "iteration: 26300 loss: 0.0034 lr: 0.005\n",
      "iteration: 26400 loss: 0.0036 lr: 0.005\n",
      "iteration: 26500 loss: 0.0042 lr: 0.005\n",
      "iteration: 26600 loss: 0.0034 lr: 0.005\n",
      "iteration: 26700 loss: 0.0042 lr: 0.005\n",
      "iteration: 26800 loss: 0.0035 lr: 0.005\n",
      "iteration: 26900 loss: 0.0046 lr: 0.005\n",
      "iteration: 27000 loss: 0.0040 lr: 0.005\n",
      "iteration: 27100 loss: 0.0040 lr: 0.005\n",
      "iteration: 27200 loss: 0.0040 lr: 0.005\n",
      "iteration: 27300 loss: 0.0045 lr: 0.005\n",
      "iteration: 27400 loss: 0.0039 lr: 0.005\n",
      "iteration: 27500 loss: 0.0043 lr: 0.005\n",
      "iteration: 27600 loss: 0.0036 lr: 0.005\n",
      "iteration: 27700 loss: 0.0044 lr: 0.005\n",
      "iteration: 27800 loss: 0.0040 lr: 0.005\n",
      "iteration: 27900 loss: 0.0045 lr: 0.005\n",
      "iteration: 28000 loss: 0.0035 lr: 0.005\n",
      "iteration: 28100 loss: 0.0046 lr: 0.005\n",
      "iteration: 28200 loss: 0.0040 lr: 0.005\n",
      "iteration: 28300 loss: 0.0042 lr: 0.005\n",
      "iteration: 28400 loss: 0.0039 lr: 0.005\n",
      "iteration: 28500 loss: 0.0044 lr: 0.005\n",
      "iteration: 28600 loss: 0.0041 lr: 0.005\n",
      "iteration: 28700 loss: 0.0042 lr: 0.005\n",
      "iteration: 28800 loss: 0.0042 lr: 0.005\n",
      "iteration: 28900 loss: 0.0039 lr: 0.005\n",
      "iteration: 29000 loss: 0.0041 lr: 0.005\n",
      "iteration: 29100 loss: 0.0045 lr: 0.005\n",
      "iteration: 29200 loss: 0.0039 lr: 0.005\n",
      "iteration: 29300 loss: 0.0042 lr: 0.005\n",
      "iteration: 29400 loss: 0.0040 lr: 0.005\n",
      "iteration: 29500 loss: 0.0040 lr: 0.005\n",
      "iteration: 29600 loss: 0.0035 lr: 0.005\n",
      "iteration: 29700 loss: 0.0042 lr: 0.005\n",
      "iteration: 29800 loss: 0.0039 lr: 0.005\n",
      "iteration: 29900 loss: 0.0045 lr: 0.005\n",
      "iteration: 30000 loss: 0.0047 lr: 0.005\n",
      "iteration: 30100 loss: 0.0039 lr: 0.005\n",
      "iteration: 30200 loss: 0.0038 lr: 0.005\n",
      "iteration: 30300 loss: 0.0037 lr: 0.005\n",
      "iteration: 30400 loss: 0.0039 lr: 0.005\n",
      "iteration: 30500 loss: 0.0037 lr: 0.005\n",
      "iteration: 30600 loss: 0.0041 lr: 0.005\n",
      "iteration: 30700 loss: 0.0045 lr: 0.005\n",
      "iteration: 30800 loss: 0.0035 lr: 0.005\n",
      "iteration: 30900 loss: 0.0043 lr: 0.005\n",
      "iteration: 31000 loss: 0.0041 lr: 0.005\n",
      "iteration: 31100 loss: 0.0037 lr: 0.005\n",
      "iteration: 31200 loss: 0.0036 lr: 0.005\n",
      "iteration: 31300 loss: 0.0035 lr: 0.005\n",
      "iteration: 31400 loss: 0.0036 lr: 0.005\n",
      "iteration: 31500 loss: 0.0036 lr: 0.005\n",
      "iteration: 31600 loss: 0.0033 lr: 0.005\n",
      "iteration: 31700 loss: 0.0044 lr: 0.005\n",
      "iteration: 31800 loss: 0.0040 lr: 0.005\n",
      "iteration: 31900 loss: 0.0040 lr: 0.005\n",
      "iteration: 32000 loss: 0.0035 lr: 0.005\n",
      "iteration: 32100 loss: 0.0039 lr: 0.005\n",
      "iteration: 32200 loss: 0.0036 lr: 0.005\n",
      "iteration: 32300 loss: 0.0038 lr: 0.005\n",
      "iteration: 32400 loss: 0.0037 lr: 0.005\n",
      "iteration: 32500 loss: 0.0031 lr: 0.005\n",
      "iteration: 32600 loss: 0.0032 lr: 0.005\n",
      "iteration: 32700 loss: 0.0034 lr: 0.005\n",
      "iteration: 32800 loss: 0.0038 lr: 0.005\n",
      "iteration: 32900 loss: 0.0044 lr: 0.005\n",
      "iteration: 33000 loss: 0.0040 lr: 0.005\n",
      "iteration: 33100 loss: 0.0039 lr: 0.005\n",
      "iteration: 33200 loss: 0.0036 lr: 0.005\n",
      "iteration: 33300 loss: 0.0039 lr: 0.005\n",
      "iteration: 33400 loss: 0.0040 lr: 0.005\n",
      "iteration: 33500 loss: 0.0049 lr: 0.005\n",
      "iteration: 33600 loss: 0.0039 lr: 0.005\n",
      "iteration: 33700 loss: 0.0040 lr: 0.005\n",
      "iteration: 33800 loss: 0.0034 lr: 0.005\n",
      "iteration: 33900 loss: 0.0040 lr: 0.005\n",
      "iteration: 34000 loss: 0.0040 lr: 0.005\n",
      "iteration: 34100 loss: 0.0043 lr: 0.005\n",
      "iteration: 34200 loss: 0.0042 lr: 0.005\n",
      "iteration: 34300 loss: 0.0038 lr: 0.005\n",
      "iteration: 34400 loss: 0.0038 lr: 0.005\n",
      "iteration: 34500 loss: 0.0035 lr: 0.005\n",
      "iteration: 34600 loss: 0.0033 lr: 0.005\n",
      "iteration: 34700 loss: 0.0044 lr: 0.005\n",
      "iteration: 34800 loss: 0.0039 lr: 0.005\n",
      "iteration: 34900 loss: 0.0037 lr: 0.005\n",
      "iteration: 35000 loss: 0.0038 lr: 0.005\n",
      "iteration: 35100 loss: 0.0034 lr: 0.005\n",
      "iteration: 35200 loss: 0.0038 lr: 0.005\n",
      "iteration: 35300 loss: 0.0038 lr: 0.005\n",
      "iteration: 35400 loss: 0.0036 lr: 0.005\n",
      "iteration: 35500 loss: 0.0040 lr: 0.005\n",
      "iteration: 35600 loss: 0.0036 lr: 0.005\n",
      "iteration: 35700 loss: 0.0037 lr: 0.005\n",
      "iteration: 35800 loss: 0.0042 lr: 0.005\n",
      "iteration: 35900 loss: 0.0037 lr: 0.005\n",
      "iteration: 36000 loss: 0.0035 lr: 0.005\n",
      "iteration: 36100 loss: 0.0035 lr: 0.005\n",
      "iteration: 36200 loss: 0.0039 lr: 0.005\n",
      "iteration: 36300 loss: 0.0034 lr: 0.005\n",
      "iteration: 36400 loss: 0.0046 lr: 0.005\n",
      "iteration: 36500 loss: 0.0034 lr: 0.005\n",
      "iteration: 36600 loss: 0.0038 lr: 0.005\n",
      "iteration: 36700 loss: 0.0035 lr: 0.005\n",
      "iteration: 36800 loss: 0.0040 lr: 0.005\n",
      "iteration: 36900 loss: 0.0039 lr: 0.005\n",
      "iteration: 37000 loss: 0.0034 lr: 0.005\n",
      "iteration: 37100 loss: 0.0039 lr: 0.005\n",
      "iteration: 37200 loss: 0.0041 lr: 0.005\n",
      "iteration: 37300 loss: 0.0035 lr: 0.005\n",
      "iteration: 37400 loss: 0.0038 lr: 0.005\n",
      "iteration: 37500 loss: 0.0033 lr: 0.005\n",
      "iteration: 37600 loss: 0.0040 lr: 0.005\n",
      "iteration: 37700 loss: 0.0038 lr: 0.005\n",
      "iteration: 37800 loss: 0.0038 lr: 0.005\n",
      "iteration: 37900 loss: 0.0034 lr: 0.005\n",
      "iteration: 38000 loss: 0.0034 lr: 0.005\n",
      "iteration: 38100 loss: 0.0037 lr: 0.005\n",
      "iteration: 38200 loss: 0.0040 lr: 0.005\n",
      "iteration: 38300 loss: 0.0035 lr: 0.005\n",
      "iteration: 38400 loss: 0.0031 lr: 0.005\n",
      "iteration: 38500 loss: 0.0040 lr: 0.005\n",
      "iteration: 38600 loss: 0.0038 lr: 0.005\n",
      "iteration: 38700 loss: 0.0033 lr: 0.005\n",
      "iteration: 38800 loss: 0.0034 lr: 0.005\n",
      "iteration: 38900 loss: 0.0037 lr: 0.005\n",
      "iteration: 39000 loss: 0.0039 lr: 0.005\n",
      "iteration: 39100 loss: 0.0038 lr: 0.005\n",
      "iteration: 39200 loss: 0.0037 lr: 0.005\n",
      "iteration: 39300 loss: 0.0032 lr: 0.005\n",
      "iteration: 39400 loss: 0.0035 lr: 0.005\n",
      "iteration: 39500 loss: 0.0033 lr: 0.005\n",
      "iteration: 39600 loss: 0.0034 lr: 0.005\n",
      "iteration: 39700 loss: 0.0037 lr: 0.005\n",
      "iteration: 39800 loss: 0.0042 lr: 0.005\n",
      "iteration: 39900 loss: 0.0034 lr: 0.005\n",
      "iteration: 40000 loss: 0.0032 lr: 0.005\n",
      "iteration: 40100 loss: 0.0037 lr: 0.005\n",
      "iteration: 40200 loss: 0.0028 lr: 0.005\n",
      "iteration: 40300 loss: 0.0034 lr: 0.005\n",
      "iteration: 40400 loss: 0.0035 lr: 0.005\n",
      "iteration: 40500 loss: 0.0034 lr: 0.005\n",
      "iteration: 40600 loss: 0.0039 lr: 0.005\n",
      "iteration: 40700 loss: 0.0034 lr: 0.005\n",
      "iteration: 40800 loss: 0.0034 lr: 0.005\n",
      "iteration: 40900 loss: 0.0036 lr: 0.005\n",
      "iteration: 41000 loss: 0.0036 lr: 0.005\n",
      "iteration: 41100 loss: 0.0041 lr: 0.005\n",
      "iteration: 41200 loss: 0.0036 lr: 0.005\n",
      "iteration: 41300 loss: 0.0035 lr: 0.005\n",
      "iteration: 41400 loss: 0.0034 lr: 0.005\n",
      "iteration: 41500 loss: 0.0038 lr: 0.005\n",
      "iteration: 41600 loss: 0.0037 lr: 0.005\n",
      "iteration: 41700 loss: 0.0032 lr: 0.005\n",
      "iteration: 41800 loss: 0.0038 lr: 0.005\n",
      "iteration: 41900 loss: 0.0033 lr: 0.005\n",
      "iteration: 42000 loss: 0.0033 lr: 0.005\n",
      "iteration: 42100 loss: 0.0036 lr: 0.005\n",
      "iteration: 42200 loss: 0.0032 lr: 0.005\n",
      "iteration: 42300 loss: 0.0044 lr: 0.005\n",
      "iteration: 42400 loss: 0.0035 lr: 0.005\n",
      "iteration: 42500 loss: 0.0031 lr: 0.005\n",
      "iteration: 42600 loss: 0.0034 lr: 0.005\n",
      "iteration: 42700 loss: 0.0035 lr: 0.005\n",
      "iteration: 42800 loss: 0.0035 lr: 0.005\n",
      "iteration: 42900 loss: 0.0033 lr: 0.005\n",
      "iteration: 43000 loss: 0.0032 lr: 0.005\n",
      "iteration: 43100 loss: 0.0044 lr: 0.005\n",
      "iteration: 43200 loss: 0.0033 lr: 0.005\n",
      "iteration: 43300 loss: 0.0032 lr: 0.005\n",
      "iteration: 43400 loss: 0.0037 lr: 0.005\n",
      "iteration: 43500 loss: 0.0033 lr: 0.005\n",
      "iteration: 43600 loss: 0.0033 lr: 0.005\n",
      "iteration: 43700 loss: 0.0033 lr: 0.005\n",
      "iteration: 43800 loss: 0.0030 lr: 0.005\n",
      "iteration: 43900 loss: 0.0031 lr: 0.005\n",
      "iteration: 44000 loss: 0.0037 lr: 0.005\n",
      "iteration: 44100 loss: 0.0030 lr: 0.005\n",
      "iteration: 44200 loss: 0.0038 lr: 0.005\n",
      "iteration: 44300 loss: 0.0035 lr: 0.005\n",
      "iteration: 44400 loss: 0.0035 lr: 0.005\n",
      "iteration: 44500 loss: 0.0035 lr: 0.005\n",
      "iteration: 44600 loss: 0.0034 lr: 0.005\n",
      "iteration: 44700 loss: 0.0034 lr: 0.005\n",
      "iteration: 44800 loss: 0.0033 lr: 0.005\n",
      "iteration: 44900 loss: 0.0037 lr: 0.005\n",
      "iteration: 45000 loss: 0.0034 lr: 0.005\n",
      "iteration: 45100 loss: 0.0037 lr: 0.005\n",
      "iteration: 45200 loss: 0.0034 lr: 0.005\n",
      "iteration: 45300 loss: 0.0032 lr: 0.005\n",
      "iteration: 45400 loss: 0.0034 lr: 0.005\n",
      "iteration: 45500 loss: 0.0031 lr: 0.005\n",
      "iteration: 45600 loss: 0.0034 lr: 0.005\n",
      "iteration: 45700 loss: 0.0030 lr: 0.005\n",
      "iteration: 45800 loss: 0.0044 lr: 0.005\n",
      "iteration: 45900 loss: 0.0033 lr: 0.005\n",
      "iteration: 46000 loss: 0.0030 lr: 0.005\n",
      "iteration: 46100 loss: 0.0031 lr: 0.005\n",
      "iteration: 46200 loss: 0.0034 lr: 0.005\n",
      "iteration: 46300 loss: 0.0033 lr: 0.005\n",
      "iteration: 46400 loss: 0.0035 lr: 0.005\n",
      "iteration: 46500 loss: 0.0034 lr: 0.005\n",
      "iteration: 46600 loss: 0.0033 lr: 0.005\n",
      "iteration: 46700 loss: 0.0035 lr: 0.005\n",
      "iteration: 46800 loss: 0.0037 lr: 0.005\n",
      "iteration: 46900 loss: 0.0030 lr: 0.005\n",
      "iteration: 47000 loss: 0.0027 lr: 0.005\n",
      "iteration: 47100 loss: 0.0033 lr: 0.005\n",
      "iteration: 47200 loss: 0.0033 lr: 0.005\n",
      "iteration: 47300 loss: 0.0031 lr: 0.005\n",
      "iteration: 47400 loss: 0.0030 lr: 0.005\n",
      "iteration: 47500 loss: 0.0034 lr: 0.005\n",
      "iteration: 47600 loss: 0.0030 lr: 0.005\n",
      "iteration: 47700 loss: 0.0033 lr: 0.005\n",
      "iteration: 47800 loss: 0.0036 lr: 0.005\n",
      "iteration: 47900 loss: 0.0034 lr: 0.005\n",
      "iteration: 48000 loss: 0.0031 lr: 0.005\n",
      "iteration: 48100 loss: 0.0030 lr: 0.005\n",
      "iteration: 48200 loss: 0.0033 lr: 0.005\n",
      "iteration: 48300 loss: 0.0043 lr: 0.005\n",
      "iteration: 48400 loss: 0.0035 lr: 0.005\n",
      "iteration: 48500 loss: 0.0032 lr: 0.005\n",
      "iteration: 48600 loss: 0.0038 lr: 0.005\n",
      "iteration: 48700 loss: 0.0033 lr: 0.005\n",
      "iteration: 48800 loss: 0.0028 lr: 0.005\n",
      "iteration: 48900 loss: 0.0031 lr: 0.005\n",
      "iteration: 49000 loss: 0.0033 lr: 0.005\n",
      "iteration: 49100 loss: 0.0030 lr: 0.005\n",
      "iteration: 49200 loss: 0.0030 lr: 0.005\n",
      "iteration: 49300 loss: 0.0032 lr: 0.005\n",
      "iteration: 49400 loss: 0.0034 lr: 0.005\n",
      "iteration: 49500 loss: 0.0034 lr: 0.005\n",
      "iteration: 49600 loss: 0.0034 lr: 0.005\n",
      "iteration: 49700 loss: 0.0035 lr: 0.005\n",
      "iteration: 49800 loss: 0.0033 lr: 0.005\n",
      "iteration: 49900 loss: 0.0033 lr: 0.005\n",
      "iteration: 50000 loss: 0.0034 lr: 0.005\n",
      "iteration: 50100 loss: 0.0034 lr: 0.005\n",
      "iteration: 50200 loss: 0.0034 lr: 0.005\n",
      "iteration: 50300 loss: 0.0033 lr: 0.005\n",
      "iteration: 50400 loss: 0.0042 lr: 0.005\n",
      "iteration: 50500 loss: 0.0041 lr: 0.005\n",
      "iteration: 50600 loss: 0.0029 lr: 0.005\n",
      "iteration: 50700 loss: 0.0035 lr: 0.005\n",
      "iteration: 50800 loss: 0.0031 lr: 0.005\n",
      "iteration: 50900 loss: 0.0033 lr: 0.005\n",
      "iteration: 51000 loss: 0.0027 lr: 0.005\n",
      "iteration: 51100 loss: 0.0035 lr: 0.005\n",
      "iteration: 51200 loss: 0.0035 lr: 0.005\n",
      "iteration: 51300 loss: 0.0033 lr: 0.005\n",
      "iteration: 51400 loss: 0.0027 lr: 0.005\n",
      "iteration: 51500 loss: 0.0036 lr: 0.005\n",
      "iteration: 51600 loss: 0.0035 lr: 0.005\n",
      "iteration: 51700 loss: 0.0033 lr: 0.005\n",
      "iteration: 51800 loss: 0.0035 lr: 0.005\n",
      "iteration: 51900 loss: 0.0036 lr: 0.005\n",
      "iteration: 52000 loss: 0.0030 lr: 0.005\n",
      "iteration: 52100 loss: 0.0032 lr: 0.005\n",
      "iteration: 52200 loss: 0.0029 lr: 0.005\n",
      "iteration: 52300 loss: 0.0032 lr: 0.005\n",
      "iteration: 52400 loss: 0.0034 lr: 0.005\n",
      "iteration: 52500 loss: 0.0033 lr: 0.005\n",
      "iteration: 52600 loss: 0.0038 lr: 0.005\n",
      "iteration: 52700 loss: 0.0035 lr: 0.005\n",
      "iteration: 52800 loss: 0.0034 lr: 0.005\n",
      "iteration: 52900 loss: 0.0034 lr: 0.005\n",
      "iteration: 53000 loss: 0.0033 lr: 0.005\n",
      "iteration: 53100 loss: 0.0029 lr: 0.005\n",
      "iteration: 53200 loss: 0.0031 lr: 0.005\n",
      "iteration: 53300 loss: 0.0028 lr: 0.005\n",
      "iteration: 53400 loss: 0.0036 lr: 0.005\n",
      "iteration: 53500 loss: 0.0026 lr: 0.005\n",
      "iteration: 53600 loss: 0.0027 lr: 0.005\n",
      "iteration: 53700 loss: 0.0030 lr: 0.005\n",
      "iteration: 53800 loss: 0.0034 lr: 0.005\n",
      "iteration: 53900 loss: 0.0035 lr: 0.005\n",
      "iteration: 54000 loss: 0.0034 lr: 0.005\n",
      "iteration: 54100 loss: 0.0034 lr: 0.005\n",
      "iteration: 54200 loss: 0.0033 lr: 0.005\n",
      "iteration: 54300 loss: 0.0030 lr: 0.005\n",
      "iteration: 54400 loss: 0.0036 lr: 0.005\n",
      "iteration: 54500 loss: 0.0035 lr: 0.005\n",
      "iteration: 54600 loss: 0.0034 lr: 0.005\n",
      "iteration: 54700 loss: 0.0036 lr: 0.005\n",
      "iteration: 54800 loss: 0.0029 lr: 0.005\n",
      "iteration: 54900 loss: 0.0028 lr: 0.005\n",
      "iteration: 55000 loss: 0.0036 lr: 0.005\n",
      "iteration: 55100 loss: 0.0032 lr: 0.005\n",
      "iteration: 55200 loss: 0.0030 lr: 0.005\n",
      "iteration: 55300 loss: 0.0035 lr: 0.005\n",
      "iteration: 55400 loss: 0.0031 lr: 0.005\n",
      "iteration: 55500 loss: 0.0032 lr: 0.005\n",
      "iteration: 55600 loss: 0.0029 lr: 0.005\n",
      "iteration: 55700 loss: 0.0033 lr: 0.005\n",
      "iteration: 55800 loss: 0.0032 lr: 0.005\n",
      "iteration: 55900 loss: 0.0030 lr: 0.005\n",
      "iteration: 56000 loss: 0.0030 lr: 0.005\n",
      "iteration: 56100 loss: 0.0029 lr: 0.005\n",
      "iteration: 56200 loss: 0.0038 lr: 0.005\n",
      "iteration: 56300 loss: 0.0035 lr: 0.005\n",
      "iteration: 56400 loss: 0.0030 lr: 0.005\n",
      "iteration: 56500 loss: 0.0033 lr: 0.005\n",
      "iteration: 56600 loss: 0.0036 lr: 0.005\n",
      "iteration: 56700 loss: 0.0031 lr: 0.005\n",
      "iteration: 56800 loss: 0.0030 lr: 0.005\n",
      "iteration: 56900 loss: 0.0031 lr: 0.005\n",
      "iteration: 57000 loss: 0.0030 lr: 0.005\n",
      "iteration: 57100 loss: 0.0026 lr: 0.005\n",
      "iteration: 57200 loss: 0.0033 lr: 0.005\n",
      "iteration: 57300 loss: 0.0037 lr: 0.005\n",
      "iteration: 57400 loss: 0.0028 lr: 0.005\n",
      "iteration: 57500 loss: 0.0031 lr: 0.005\n",
      "iteration: 57600 loss: 0.0030 lr: 0.005\n",
      "iteration: 57700 loss: 0.0028 lr: 0.005\n",
      "iteration: 57800 loss: 0.0028 lr: 0.005\n",
      "iteration: 57900 loss: 0.0034 lr: 0.005\n",
      "iteration: 58000 loss: 0.0032 lr: 0.005\n",
      "iteration: 58100 loss: 0.0028 lr: 0.005\n",
      "iteration: 58200 loss: 0.0028 lr: 0.005\n",
      "iteration: 58300 loss: 0.0028 lr: 0.005\n",
      "iteration: 58400 loss: 0.0032 lr: 0.005\n",
      "iteration: 58500 loss: 0.0032 lr: 0.005\n",
      "iteration: 58600 loss: 0.0031 lr: 0.005\n",
      "iteration: 58700 loss: 0.0037 lr: 0.005\n",
      "iteration: 58800 loss: 0.0031 lr: 0.005\n",
      "iteration: 58900 loss: 0.0027 lr: 0.005\n",
      "iteration: 59000 loss: 0.0028 lr: 0.005\n",
      "iteration: 59100 loss: 0.0035 lr: 0.005\n",
      "iteration: 59200 loss: 0.0035 lr: 0.005\n",
      "iteration: 59300 loss: 0.0034 lr: 0.005\n",
      "iteration: 59400 loss: 0.0038 lr: 0.005\n",
      "iteration: 59500 loss: 0.0033 lr: 0.005\n",
      "iteration: 59600 loss: 0.0031 lr: 0.005\n",
      "iteration: 59700 loss: 0.0031 lr: 0.005\n",
      "iteration: 59800 loss: 0.0031 lr: 0.005\n",
      "iteration: 59900 loss: 0.0030 lr: 0.005\n",
      "iteration: 60000 loss: 0.0032 lr: 0.005\n",
      "iteration: 60100 loss: 0.0031 lr: 0.005\n",
      "iteration: 60200 loss: 0.0030 lr: 0.005\n",
      "iteration: 60300 loss: 0.0032 lr: 0.005\n",
      "iteration: 60400 loss: 0.0032 lr: 0.005\n",
      "iteration: 60500 loss: 0.0030 lr: 0.005\n",
      "iteration: 60600 loss: 0.0029 lr: 0.005\n",
      "iteration: 60700 loss: 0.0034 lr: 0.005\n",
      "iteration: 60800 loss: 0.0030 lr: 0.005\n",
      "iteration: 60900 loss: 0.0035 lr: 0.005\n",
      "iteration: 61000 loss: 0.0036 lr: 0.005\n",
      "iteration: 61100 loss: 0.0032 lr: 0.005\n",
      "iteration: 61200 loss: 0.0029 lr: 0.005\n",
      "iteration: 61300 loss: 0.0032 lr: 0.005\n",
      "iteration: 61400 loss: 0.0029 lr: 0.005\n",
      "iteration: 61500 loss: 0.0029 lr: 0.005\n",
      "iteration: 61600 loss: 0.0034 lr: 0.005\n",
      "iteration: 61700 loss: 0.0029 lr: 0.005\n",
      "iteration: 61800 loss: 0.0029 lr: 0.005\n",
      "iteration: 61900 loss: 0.0031 lr: 0.005\n",
      "iteration: 62000 loss: 0.0030 lr: 0.005\n",
      "iteration: 62100 loss: 0.0028 lr: 0.005\n",
      "iteration: 62200 loss: 0.0033 lr: 0.005\n",
      "iteration: 62300 loss: 0.0027 lr: 0.005\n",
      "iteration: 62400 loss: 0.0029 lr: 0.005\n",
      "iteration: 62500 loss: 0.0031 lr: 0.005\n",
      "iteration: 62600 loss: 0.0030 lr: 0.005\n",
      "iteration: 62700 loss: 0.0025 lr: 0.005\n",
      "iteration: 62800 loss: 0.0027 lr: 0.005\n",
      "iteration: 62900 loss: 0.0028 lr: 0.005\n",
      "iteration: 63000 loss: 0.0035 lr: 0.005\n",
      "iteration: 63100 loss: 0.0032 lr: 0.005\n",
      "iteration: 63200 loss: 0.0031 lr: 0.005\n",
      "iteration: 63300 loss: 0.0033 lr: 0.005\n",
      "iteration: 63400 loss: 0.0027 lr: 0.005\n",
      "iteration: 63500 loss: 0.0035 lr: 0.005\n",
      "iteration: 63600 loss: 0.0030 lr: 0.005\n",
      "iteration: 63700 loss: 0.0029 lr: 0.005\n",
      "iteration: 63800 loss: 0.0028 lr: 0.005\n",
      "iteration: 63900 loss: 0.0032 lr: 0.005\n",
      "iteration: 64000 loss: 0.0029 lr: 0.005\n",
      "iteration: 64100 loss: 0.0038 lr: 0.005\n",
      "iteration: 64200 loss: 0.0037 lr: 0.005\n",
      "iteration: 64300 loss: 0.0029 lr: 0.005\n",
      "iteration: 64400 loss: 0.0028 lr: 0.005\n",
      "iteration: 64500 loss: 0.0031 lr: 0.005\n",
      "iteration: 64600 loss: 0.0034 lr: 0.005\n",
      "iteration: 64700 loss: 0.0029 lr: 0.005\n",
      "iteration: 64800 loss: 0.0029 lr: 0.005\n",
      "iteration: 64900 loss: 0.0027 lr: 0.005\n",
      "iteration: 65000 loss: 0.0033 lr: 0.005\n",
      "iteration: 65100 loss: 0.0031 lr: 0.005\n",
      "iteration: 65200 loss: 0.0035 lr: 0.005\n",
      "iteration: 65300 loss: 0.0030 lr: 0.005\n",
      "iteration: 65400 loss: 0.0034 lr: 0.005\n",
      "iteration: 65500 loss: 0.0030 lr: 0.005\n",
      "iteration: 65600 loss: 0.0033 lr: 0.005\n",
      "iteration: 65700 loss: 0.0036 lr: 0.005\n",
      "iteration: 65800 loss: 0.0031 lr: 0.005\n",
      "iteration: 65900 loss: 0.0030 lr: 0.005\n",
      "iteration: 66000 loss: 0.0033 lr: 0.005\n",
      "iteration: 66100 loss: 0.0031 lr: 0.005\n",
      "iteration: 66200 loss: 0.0032 lr: 0.005\n",
      "iteration: 66300 loss: 0.0033 lr: 0.005\n",
      "iteration: 66400 loss: 0.0030 lr: 0.005\n",
      "iteration: 66500 loss: 0.0036 lr: 0.005\n",
      "iteration: 66600 loss: 0.0028 lr: 0.005\n",
      "iteration: 66700 loss: 0.0028 lr: 0.005\n",
      "iteration: 66800 loss: 0.0028 lr: 0.005\n",
      "iteration: 66900 loss: 0.0034 lr: 0.005\n",
      "iteration: 67000 loss: 0.0029 lr: 0.005\n",
      "iteration: 67100 loss: 0.0034 lr: 0.005\n",
      "iteration: 67200 loss: 0.0029 lr: 0.005\n",
      "iteration: 67300 loss: 0.0034 lr: 0.005\n",
      "iteration: 67400 loss: 0.0033 lr: 0.005\n",
      "iteration: 67500 loss: 0.0033 lr: 0.005\n",
      "iteration: 67600 loss: 0.0029 lr: 0.005\n",
      "iteration: 67700 loss: 0.0034 lr: 0.005\n",
      "iteration: 67800 loss: 0.0028 lr: 0.005\n",
      "iteration: 67900 loss: 0.0036 lr: 0.005\n",
      "iteration: 68000 loss: 0.0025 lr: 0.005\n",
      "iteration: 68100 loss: 0.0025 lr: 0.005\n",
      "iteration: 68200 loss: 0.0029 lr: 0.005\n",
      "iteration: 68300 loss: 0.0028 lr: 0.005\n",
      "iteration: 68400 loss: 0.0032 lr: 0.005\n",
      "iteration: 68500 loss: 0.0032 lr: 0.005\n",
      "iteration: 68600 loss: 0.0029 lr: 0.005\n",
      "iteration: 68700 loss: 0.0033 lr: 0.005\n",
      "iteration: 68800 loss: 0.0028 lr: 0.005\n",
      "iteration: 68900 loss: 0.0032 lr: 0.005\n",
      "iteration: 69000 loss: 0.0031 lr: 0.005\n",
      "iteration: 69100 loss: 0.0032 lr: 0.005\n",
      "iteration: 69200 loss: 0.0025 lr: 0.005\n",
      "iteration: 69300 loss: 0.0029 lr: 0.005\n",
      "iteration: 69400 loss: 0.0030 lr: 0.005\n",
      "iteration: 69500 loss: 0.0032 lr: 0.005\n",
      "iteration: 69600 loss: 0.0031 lr: 0.005\n",
      "iteration: 69700 loss: 0.0032 lr: 0.005\n",
      "iteration: 69800 loss: 0.0032 lr: 0.005\n",
      "iteration: 69900 loss: 0.0029 lr: 0.005\n",
      "iteration: 70000 loss: 0.0028 lr: 0.005\n",
      "iteration: 70100 loss: 0.0028 lr: 0.005\n",
      "iteration: 70200 loss: 0.0031 lr: 0.005\n",
      "iteration: 70300 loss: 0.0030 lr: 0.005\n",
      "iteration: 70400 loss: 0.0027 lr: 0.005\n",
      "iteration: 70500 loss: 0.0029 lr: 0.005\n",
      "iteration: 70600 loss: 0.0026 lr: 0.005\n",
      "iteration: 70700 loss: 0.0032 lr: 0.005\n",
      "iteration: 70800 loss: 0.0026 lr: 0.005\n",
      "iteration: 70900 loss: 0.0027 lr: 0.005\n",
      "iteration: 71000 loss: 0.0030 lr: 0.005\n",
      "iteration: 71100 loss: 0.0029 lr: 0.005\n",
      "iteration: 71200 loss: 0.0027 lr: 0.005\n",
      "iteration: 71300 loss: 0.0036 lr: 0.005\n",
      "iteration: 71400 loss: 0.0032 lr: 0.005\n",
      "iteration: 71500 loss: 0.0032 lr: 0.005\n",
      "iteration: 71600 loss: 0.0030 lr: 0.005\n",
      "iteration: 71700 loss: 0.0027 lr: 0.005\n",
      "iteration: 71800 loss: 0.0034 lr: 0.005\n",
      "iteration: 71900 loss: 0.0029 lr: 0.005\n",
      "iteration: 72000 loss: 0.0025 lr: 0.005\n",
      "iteration: 72100 loss: 0.0027 lr: 0.005\n",
      "iteration: 72200 loss: 0.0029 lr: 0.005\n",
      "iteration: 72300 loss: 0.0032 lr: 0.005\n",
      "iteration: 72400 loss: 0.0025 lr: 0.005\n",
      "iteration: 72500 loss: 0.0028 lr: 0.005\n",
      "iteration: 72600 loss: 0.0032 lr: 0.005\n",
      "iteration: 72700 loss: 0.0032 lr: 0.005\n",
      "iteration: 72800 loss: 0.0028 lr: 0.005\n",
      "iteration: 72900 loss: 0.0029 lr: 0.005\n",
      "iteration: 73000 loss: 0.0029 lr: 0.005\n",
      "iteration: 73100 loss: 0.0027 lr: 0.005\n",
      "iteration: 73200 loss: 0.0031 lr: 0.005\n",
      "iteration: 73300 loss: 0.0027 lr: 0.005\n",
      "iteration: 73400 loss: 0.0028 lr: 0.005\n",
      "iteration: 73500 loss: 0.0029 lr: 0.005\n",
      "iteration: 73600 loss: 0.0034 lr: 0.005\n",
      "iteration: 73700 loss: 0.0034 lr: 0.005\n",
      "iteration: 73800 loss: 0.0036 lr: 0.005\n",
      "iteration: 73900 loss: 0.0029 lr: 0.005\n",
      "iteration: 74000 loss: 0.0026 lr: 0.005\n",
      "iteration: 74100 loss: 0.0033 lr: 0.005\n",
      "iteration: 74200 loss: 0.0031 lr: 0.005\n",
      "iteration: 74300 loss: 0.0030 lr: 0.005\n",
      "iteration: 74400 loss: 0.0029 lr: 0.005\n",
      "iteration: 74500 loss: 0.0034 lr: 0.005\n",
      "iteration: 74600 loss: 0.0030 lr: 0.005\n",
      "iteration: 74700 loss: 0.0030 lr: 0.005\n",
      "iteration: 74800 loss: 0.0030 lr: 0.005\n",
      "iteration: 74900 loss: 0.0031 lr: 0.005\n",
      "iteration: 75000 loss: 0.0029 lr: 0.005\n",
      "iteration: 75100 loss: 0.0029 lr: 0.005\n",
      "iteration: 75200 loss: 0.0026 lr: 0.005\n",
      "iteration: 75300 loss: 0.0030 lr: 0.005\n",
      "iteration: 75400 loss: 0.0030 lr: 0.005\n",
      "iteration: 75500 loss: 0.0027 lr: 0.005\n",
      "iteration: 75600 loss: 0.0025 lr: 0.005\n",
      "iteration: 75700 loss: 0.0027 lr: 0.005\n",
      "iteration: 75800 loss: 0.0029 lr: 0.005\n",
      "iteration: 75900 loss: 0.0028 lr: 0.005\n",
      "iteration: 76000 loss: 0.0030 lr: 0.005\n",
      "iteration: 76100 loss: 0.0031 lr: 0.005\n",
      "iteration: 76200 loss: 0.0029 lr: 0.005\n",
      "iteration: 76300 loss: 0.0031 lr: 0.005\n",
      "iteration: 76400 loss: 0.0032 lr: 0.005\n",
      "iteration: 76500 loss: 0.0031 lr: 0.005\n",
      "iteration: 76600 loss: 0.0030 lr: 0.005\n",
      "iteration: 76700 loss: 0.0029 lr: 0.005\n",
      "iteration: 76800 loss: 0.0030 lr: 0.005\n",
      "iteration: 76900 loss: 0.0029 lr: 0.005\n",
      "iteration: 77000 loss: 0.0026 lr: 0.005\n",
      "iteration: 77100 loss: 0.0030 lr: 0.005\n",
      "iteration: 77200 loss: 0.0027 lr: 0.005\n",
      "iteration: 77300 loss: 0.0035 lr: 0.005\n",
      "iteration: 77400 loss: 0.0031 lr: 0.005\n",
      "iteration: 77500 loss: 0.0028 lr: 0.005\n",
      "iteration: 77600 loss: 0.0029 lr: 0.005\n",
      "iteration: 77700 loss: 0.0026 lr: 0.005\n",
      "iteration: 77800 loss: 0.0023 lr: 0.005\n",
      "iteration: 77900 loss: 0.0032 lr: 0.005\n",
      "iteration: 78000 loss: 0.0028 lr: 0.005\n",
      "iteration: 78100 loss: 0.0032 lr: 0.005\n",
      "iteration: 78200 loss: 0.0031 lr: 0.005\n",
      "iteration: 78300 loss: 0.0027 lr: 0.005\n",
      "iteration: 78400 loss: 0.0032 lr: 0.005\n",
      "iteration: 78500 loss: 0.0034 lr: 0.005\n",
      "iteration: 78600 loss: 0.0028 lr: 0.005\n",
      "iteration: 78700 loss: 0.0030 lr: 0.005\n",
      "iteration: 78800 loss: 0.0034 lr: 0.005\n",
      "iteration: 78900 loss: 0.0035 lr: 0.005\n",
      "iteration: 79000 loss: 0.0027 lr: 0.005\n",
      "iteration: 79100 loss: 0.0026 lr: 0.005\n",
      "iteration: 79200 loss: 0.0027 lr: 0.005\n",
      "iteration: 79300 loss: 0.0026 lr: 0.005\n",
      "iteration: 79400 loss: 0.0025 lr: 0.005\n",
      "iteration: 79500 loss: 0.0028 lr: 0.005\n",
      "iteration: 79600 loss: 0.0028 lr: 0.005\n",
      "iteration: 79700 loss: 0.0030 lr: 0.005\n",
      "iteration: 79800 loss: 0.0030 lr: 0.005\n",
      "iteration: 79900 loss: 0.0028 lr: 0.005\n",
      "iteration: 80000 loss: 0.0029 lr: 0.005\n",
      "iteration: 80100 loss: 0.0024 lr: 0.005\n",
      "iteration: 80200 loss: 0.0027 lr: 0.005\n",
      "iteration: 80300 loss: 0.0029 lr: 0.005\n",
      "iteration: 80400 loss: 0.0032 lr: 0.005\n",
      "iteration: 80500 loss: 0.0030 lr: 0.005\n",
      "iteration: 80600 loss: 0.0030 lr: 0.005\n",
      "iteration: 80700 loss: 0.0028 lr: 0.005\n",
      "iteration: 80800 loss: 0.0029 lr: 0.005\n",
      "iteration: 80900 loss: 0.0026 lr: 0.005\n",
      "iteration: 81000 loss: 0.0027 lr: 0.005\n",
      "iteration: 81100 loss: 0.0029 lr: 0.005\n",
      "iteration: 81200 loss: 0.0028 lr: 0.005\n",
      "iteration: 81300 loss: 0.0029 lr: 0.005\n",
      "iteration: 81400 loss: 0.0035 lr: 0.005\n",
      "iteration: 81500 loss: 0.0034 lr: 0.005\n",
      "iteration: 81600 loss: 0.0028 lr: 0.005\n",
      "iteration: 81700 loss: 0.0028 lr: 0.005\n",
      "iteration: 81800 loss: 0.0025 lr: 0.005\n",
      "iteration: 81900 loss: 0.0030 lr: 0.005\n",
      "iteration: 82000 loss: 0.0032 lr: 0.005\n",
      "iteration: 82100 loss: 0.0026 lr: 0.005\n",
      "iteration: 82200 loss: 0.0024 lr: 0.005\n",
      "iteration: 82300 loss: 0.0028 lr: 0.005\n",
      "iteration: 82400 loss: 0.0026 lr: 0.005\n",
      "iteration: 82500 loss: 0.0027 lr: 0.005\n",
      "iteration: 82600 loss: 0.0028 lr: 0.005\n",
      "iteration: 82700 loss: 0.0026 lr: 0.005\n",
      "iteration: 82800 loss: 0.0026 lr: 0.005\n",
      "iteration: 82900 loss: 0.0032 lr: 0.005\n",
      "iteration: 83000 loss: 0.0034 lr: 0.005\n",
      "iteration: 83100 loss: 0.0028 lr: 0.005\n",
      "iteration: 83200 loss: 0.0035 lr: 0.005\n",
      "iteration: 83300 loss: 0.0029 lr: 0.005\n",
      "iteration: 83400 loss: 0.0028 lr: 0.005\n",
      "iteration: 83500 loss: 0.0031 lr: 0.005\n",
      "iteration: 83600 loss: 0.0025 lr: 0.005\n",
      "iteration: 83700 loss: 0.0031 lr: 0.005\n",
      "iteration: 83800 loss: 0.0029 lr: 0.005\n",
      "iteration: 83900 loss: 0.0032 lr: 0.005\n",
      "iteration: 84000 loss: 0.0027 lr: 0.005\n",
      "iteration: 84100 loss: 0.0034 lr: 0.005\n",
      "iteration: 84200 loss: 0.0036 lr: 0.005\n",
      "iteration: 84300 loss: 0.0027 lr: 0.005\n",
      "iteration: 84400 loss: 0.0032 lr: 0.005\n",
      "iteration: 84500 loss: 0.0025 lr: 0.005\n",
      "iteration: 84600 loss: 0.0027 lr: 0.005\n",
      "iteration: 84700 loss: 0.0034 lr: 0.005\n",
      "iteration: 84800 loss: 0.0028 lr: 0.005\n",
      "iteration: 84900 loss: 0.0028 lr: 0.005\n",
      "iteration: 85000 loss: 0.0029 lr: 0.005\n",
      "iteration: 85100 loss: 0.0025 lr: 0.005\n",
      "iteration: 85200 loss: 0.0027 lr: 0.005\n",
      "iteration: 85300 loss: 0.0026 lr: 0.005\n",
      "iteration: 85400 loss: 0.0028 lr: 0.005\n",
      "iteration: 85500 loss: 0.0028 lr: 0.005\n",
      "iteration: 85600 loss: 0.0025 lr: 0.005\n",
      "iteration: 85700 loss: 0.0025 lr: 0.005\n",
      "iteration: 85800 loss: 0.0029 lr: 0.005\n",
      "iteration: 85900 loss: 0.0030 lr: 0.005\n",
      "iteration: 86000 loss: 0.0034 lr: 0.005\n",
      "iteration: 86100 loss: 0.0028 lr: 0.005\n",
      "iteration: 86200 loss: 0.0029 lr: 0.005\n",
      "iteration: 86300 loss: 0.0027 lr: 0.005\n",
      "iteration: 86400 loss: 0.0025 lr: 0.005\n",
      "iteration: 86500 loss: 0.0030 lr: 0.005\n",
      "iteration: 86600 loss: 0.0030 lr: 0.005\n",
      "iteration: 86700 loss: 0.0025 lr: 0.005\n",
      "iteration: 86800 loss: 0.0024 lr: 0.005\n",
      "iteration: 86900 loss: 0.0024 lr: 0.005\n",
      "iteration: 87000 loss: 0.0024 lr: 0.005\n",
      "iteration: 87100 loss: 0.0030 lr: 0.005\n",
      "iteration: 87200 loss: 0.0027 lr: 0.005\n",
      "iteration: 87300 loss: 0.0027 lr: 0.005\n",
      "iteration: 87400 loss: 0.0028 lr: 0.005\n",
      "iteration: 87500 loss: 0.0029 lr: 0.005\n",
      "iteration: 87600 loss: 0.0037 lr: 0.005\n",
      "iteration: 87700 loss: 0.0026 lr: 0.005\n",
      "iteration: 87800 loss: 0.0026 lr: 0.005\n",
      "iteration: 87900 loss: 0.0027 lr: 0.005\n",
      "iteration: 88000 loss: 0.0027 lr: 0.005\n",
      "iteration: 88100 loss: 0.0026 lr: 0.005\n",
      "iteration: 88200 loss: 0.0030 lr: 0.005\n",
      "iteration: 88300 loss: 0.0030 lr: 0.005\n",
      "iteration: 88400 loss: 0.0027 lr: 0.005\n",
      "iteration: 88500 loss: 0.0029 lr: 0.005\n",
      "iteration: 88600 loss: 0.0029 lr: 0.005\n",
      "iteration: 88700 loss: 0.0030 lr: 0.005\n",
      "iteration: 88800 loss: 0.0024 lr: 0.005\n",
      "iteration: 88900 loss: 0.0031 lr: 0.005\n",
      "iteration: 89000 loss: 0.0026 lr: 0.005\n",
      "iteration: 89100 loss: 0.0029 lr: 0.005\n",
      "iteration: 89200 loss: 0.0026 lr: 0.005\n",
      "iteration: 89300 loss: 0.0028 lr: 0.005\n",
      "iteration: 89400 loss: 0.0028 lr: 0.005\n",
      "iteration: 89500 loss: 0.0027 lr: 0.005\n",
      "iteration: 89600 loss: 0.0027 lr: 0.005\n",
      "iteration: 89700 loss: 0.0028 lr: 0.005\n",
      "iteration: 89800 loss: 0.0026 lr: 0.005\n",
      "iteration: 89900 loss: 0.0031 lr: 0.005\n",
      "iteration: 90000 loss: 0.0036 lr: 0.005\n",
      "iteration: 90100 loss: 0.0030 lr: 0.005\n",
      "iteration: 90200 loss: 0.0029 lr: 0.005\n",
      "iteration: 90300 loss: 0.0029 lr: 0.005\n",
      "iteration: 90400 loss: 0.0027 lr: 0.005\n",
      "iteration: 90500 loss: 0.0024 lr: 0.005\n",
      "iteration: 90600 loss: 0.0025 lr: 0.005\n",
      "iteration: 90700 loss: 0.0034 lr: 0.005\n",
      "iteration: 90800 loss: 0.0036 lr: 0.005\n",
      "iteration: 90900 loss: 0.0029 lr: 0.005\n",
      "iteration: 91000 loss: 0.0028 lr: 0.005\n",
      "iteration: 91100 loss: 0.0031 lr: 0.005\n",
      "iteration: 91200 loss: 0.0028 lr: 0.005\n",
      "iteration: 91300 loss: 0.0027 lr: 0.005\n",
      "iteration: 91400 loss: 0.0024 lr: 0.005\n",
      "iteration: 91500 loss: 0.0025 lr: 0.005\n",
      "iteration: 91600 loss: 0.0027 lr: 0.005\n",
      "iteration: 91700 loss: 0.0029 lr: 0.005\n",
      "iteration: 91800 loss: 0.0028 lr: 0.005\n",
      "iteration: 91900 loss: 0.0026 lr: 0.005\n",
      "iteration: 92000 loss: 0.0029 lr: 0.005\n",
      "iteration: 92100 loss: 0.0026 lr: 0.005\n",
      "iteration: 92200 loss: 0.0029 lr: 0.005\n",
      "iteration: 92300 loss: 0.0030 lr: 0.005\n",
      "iteration: 92400 loss: 0.0030 lr: 0.005\n",
      "iteration: 92500 loss: 0.0037 lr: 0.005\n",
      "iteration: 92600 loss: 0.0028 lr: 0.005\n",
      "iteration: 92700 loss: 0.0031 lr: 0.005\n",
      "iteration: 92800 loss: 0.0028 lr: 0.005\n",
      "iteration: 92900 loss: 0.0029 lr: 0.005\n",
      "iteration: 93000 loss: 0.0029 lr: 0.005\n",
      "iteration: 93100 loss: 0.0032 lr: 0.005\n",
      "iteration: 93200 loss: 0.0024 lr: 0.005\n",
      "iteration: 93300 loss: 0.0031 lr: 0.005\n",
      "iteration: 93400 loss: 0.0027 lr: 0.005\n",
      "iteration: 93500 loss: 0.0026 lr: 0.005\n",
      "iteration: 93600 loss: 0.0027 lr: 0.005\n",
      "iteration: 93700 loss: 0.0027 lr: 0.005\n",
      "iteration: 93800 loss: 0.0027 lr: 0.005\n",
      "iteration: 93900 loss: 0.0031 lr: 0.005\n",
      "iteration: 94000 loss: 0.0029 lr: 0.005\n",
      "iteration: 94100 loss: 0.0024 lr: 0.005\n",
      "iteration: 94200 loss: 0.0023 lr: 0.005\n",
      "iteration: 94300 loss: 0.0028 lr: 0.005\n",
      "iteration: 94400 loss: 0.0027 lr: 0.005\n",
      "iteration: 94500 loss: 0.0031 lr: 0.005\n",
      "iteration: 94600 loss: 0.0030 lr: 0.005\n",
      "iteration: 94700 loss: 0.0028 lr: 0.005\n",
      "iteration: 94800 loss: 0.0023 lr: 0.005\n",
      "iteration: 94900 loss: 0.0030 lr: 0.005\n",
      "iteration: 95000 loss: 0.0023 lr: 0.005\n",
      "iteration: 95100 loss: 0.0027 lr: 0.005\n",
      "iteration: 95200 loss: 0.0030 lr: 0.005\n",
      "iteration: 95300 loss: 0.0028 lr: 0.005\n",
      "iteration: 95400 loss: 0.0030 lr: 0.005\n",
      "iteration: 95500 loss: 0.0027 lr: 0.005\n",
      "iteration: 95600 loss: 0.0028 lr: 0.005\n",
      "iteration: 95700 loss: 0.0027 lr: 0.005\n",
      "iteration: 95800 loss: 0.0026 lr: 0.005\n",
      "iteration: 95900 loss: 0.0023 lr: 0.005\n",
      "iteration: 96000 loss: 0.0036 lr: 0.005\n",
      "iteration: 96100 loss: 0.0027 lr: 0.005\n",
      "iteration: 96200 loss: 0.0028 lr: 0.005\n",
      "iteration: 96300 loss: 0.0030 lr: 0.005\n",
      "iteration: 96400 loss: 0.0025 lr: 0.005\n",
      "iteration: 96500 loss: 0.0027 lr: 0.005\n",
      "iteration: 96600 loss: 0.0027 lr: 0.005\n",
      "iteration: 96700 loss: 0.0026 lr: 0.005\n",
      "iteration: 96800 loss: 0.0027 lr: 0.005\n",
      "iteration: 96900 loss: 0.0027 lr: 0.005\n",
      "iteration: 97000 loss: 0.0029 lr: 0.005\n",
      "iteration: 97100 loss: 0.0026 lr: 0.005\n",
      "iteration: 97200 loss: 0.0028 lr: 0.005\n",
      "iteration: 97300 loss: 0.0023 lr: 0.005\n",
      "iteration: 97400 loss: 0.0032 lr: 0.005\n",
      "iteration: 97500 loss: 0.0024 lr: 0.005\n",
      "iteration: 97600 loss: 0.0034 lr: 0.005\n",
      "iteration: 97700 loss: 0.0024 lr: 0.005\n",
      "iteration: 97800 loss: 0.0030 lr: 0.005\n",
      "iteration: 97900 loss: 0.0027 lr: 0.005\n",
      "iteration: 98000 loss: 0.0030 lr: 0.005\n",
      "iteration: 98100 loss: 0.0027 lr: 0.005\n",
      "iteration: 98200 loss: 0.0025 lr: 0.005\n",
      "iteration: 98300 loss: 0.0025 lr: 0.005\n",
      "iteration: 98400 loss: 0.0031 lr: 0.005\n",
      "iteration: 98500 loss: 0.0028 lr: 0.005\n",
      "iteration: 98600 loss: 0.0035 lr: 0.005\n",
      "iteration: 98700 loss: 0.0033 lr: 0.005\n",
      "iteration: 98800 loss: 0.0029 lr: 0.005\n",
      "iteration: 98900 loss: 0.0033 lr: 0.005\n",
      "iteration: 99000 loss: 0.0026 lr: 0.005\n",
      "iteration: 99100 loss: 0.0026 lr: 0.005\n",
      "iteration: 99200 loss: 0.0026 lr: 0.005\n",
      "iteration: 99300 loss: 0.0025 lr: 0.005\n",
      "iteration: 99400 loss: 0.0026 lr: 0.005\n",
      "iteration: 99500 loss: 0.0029 lr: 0.005\n",
      "iteration: 99600 loss: 0.0027 lr: 0.005\n",
      "iteration: 99700 loss: 0.0032 lr: 0.005\n",
      "iteration: 99800 loss: 0.0020 lr: 0.005\n",
      "iteration: 99900 loss: 0.0027 lr: 0.005\n",
      "iteration: 100000 loss: 0.0030 lr: 0.005\n",
      "iteration: 100100 loss: 0.0029 lr: 0.005\n",
      "iteration: 100200 loss: 0.0030 lr: 0.005\n",
      "iteration: 100300 loss: 0.0025 lr: 0.005\n",
      "iteration: 100400 loss: 0.0028 lr: 0.005\n",
      "iteration: 100500 loss: 0.0027 lr: 0.005\n",
      "iteration: 100600 loss: 0.0030 lr: 0.005\n",
      "iteration: 100700 loss: 0.0026 lr: 0.005\n",
      "iteration: 100800 loss: 0.0028 lr: 0.005\n",
      "iteration: 100900 loss: 0.0029 lr: 0.005\n",
      "iteration: 101000 loss: 0.0028 lr: 0.005\n",
      "iteration: 101100 loss: 0.0026 lr: 0.005\n",
      "iteration: 101200 loss: 0.0028 lr: 0.005\n",
      "iteration: 101300 loss: 0.0030 lr: 0.005\n",
      "iteration: 101400 loss: 0.0029 lr: 0.005\n",
      "iteration: 101500 loss: 0.0028 lr: 0.005\n",
      "iteration: 101600 loss: 0.0026 lr: 0.005\n",
      "iteration: 101700 loss: 0.0027 lr: 0.005\n",
      "iteration: 101800 loss: 0.0026 lr: 0.005\n",
      "iteration: 101900 loss: 0.0028 lr: 0.005\n",
      "iteration: 102000 loss: 0.0030 lr: 0.005\n",
      "iteration: 102100 loss: 0.0028 lr: 0.005\n",
      "iteration: 102200 loss: 0.0026 lr: 0.005\n",
      "iteration: 102300 loss: 0.0033 lr: 0.005\n",
      "iteration: 102400 loss: 0.0030 lr: 0.005\n",
      "iteration: 102500 loss: 0.0030 lr: 0.005\n",
      "iteration: 102600 loss: 0.0025 lr: 0.005\n",
      "iteration: 102700 loss: 0.0028 lr: 0.005\n",
      "iteration: 102800 loss: 0.0029 lr: 0.005\n",
      "iteration: 102900 loss: 0.0031 lr: 0.005\n",
      "iteration: 103000 loss: 0.0028 lr: 0.005\n",
      "iteration: 103100 loss: 0.0029 lr: 0.005\n",
      "iteration: 103200 loss: 0.0028 lr: 0.005\n",
      "iteration: 103300 loss: 0.0025 lr: 0.005\n",
      "iteration: 103400 loss: 0.0027 lr: 0.005\n",
      "iteration: 103500 loss: 0.0026 lr: 0.005\n",
      "iteration: 103600 loss: 0.0026 lr: 0.005\n",
      "iteration: 103700 loss: 0.0027 lr: 0.005\n",
      "iteration: 103800 loss: 0.0027 lr: 0.005\n",
      "iteration: 103900 loss: 0.0029 lr: 0.005\n",
      "iteration: 104000 loss: 0.0025 lr: 0.005\n",
      "iteration: 104100 loss: 0.0026 lr: 0.005\n",
      "iteration: 104200 loss: 0.0026 lr: 0.005\n",
      "iteration: 104300 loss: 0.0027 lr: 0.005\n",
      "iteration: 104400 loss: 0.0026 lr: 0.005\n",
      "iteration: 104500 loss: 0.0029 lr: 0.005\n",
      "iteration: 104600 loss: 0.0031 lr: 0.005\n",
      "iteration: 104700 loss: 0.0027 lr: 0.005\n",
      "iteration: 104800 loss: 0.0023 lr: 0.005\n",
      "iteration: 104900 loss: 0.0028 lr: 0.005\n",
      "iteration: 105000 loss: 0.0029 lr: 0.005\n",
      "iteration: 105100 loss: 0.0022 lr: 0.005\n",
      "iteration: 105200 loss: 0.0030 lr: 0.005\n",
      "iteration: 105300 loss: 0.0022 lr: 0.005\n",
      "iteration: 105400 loss: 0.0025 lr: 0.005\n",
      "iteration: 105500 loss: 0.0026 lr: 0.005\n",
      "iteration: 105600 loss: 0.0028 lr: 0.005\n",
      "iteration: 105700 loss: 0.0028 lr: 0.005\n",
      "iteration: 105800 loss: 0.0028 lr: 0.005\n",
      "iteration: 105900 loss: 0.0025 lr: 0.005\n",
      "iteration: 106000 loss: 0.0031 lr: 0.005\n",
      "iteration: 106100 loss: 0.0024 lr: 0.005\n",
      "iteration: 106200 loss: 0.0027 lr: 0.005\n",
      "iteration: 106300 loss: 0.0029 lr: 0.005\n",
      "iteration: 106400 loss: 0.0027 lr: 0.005\n",
      "iteration: 106500 loss: 0.0029 lr: 0.005\n",
      "iteration: 106600 loss: 0.0032 lr: 0.005\n",
      "iteration: 106700 loss: 0.0026 lr: 0.005\n",
      "iteration: 106800 loss: 0.0022 lr: 0.005\n",
      "iteration: 106900 loss: 0.0025 lr: 0.005\n",
      "iteration: 107000 loss: 0.0023 lr: 0.005\n",
      "iteration: 107100 loss: 0.0030 lr: 0.005\n",
      "iteration: 107200 loss: 0.0024 lr: 0.005\n",
      "iteration: 107300 loss: 0.0031 lr: 0.005\n",
      "iteration: 107400 loss: 0.0025 lr: 0.005\n",
      "iteration: 107500 loss: 0.0024 lr: 0.005\n",
      "iteration: 107600 loss: 0.0024 lr: 0.005\n",
      "iteration: 107700 loss: 0.0028 lr: 0.005\n",
      "iteration: 107800 loss: 0.0025 lr: 0.005\n",
      "iteration: 107900 loss: 0.0021 lr: 0.005\n",
      "iteration: 108000 loss: 0.0024 lr: 0.005\n",
      "iteration: 108100 loss: 0.0028 lr: 0.005\n",
      "iteration: 108200 loss: 0.0029 lr: 0.005\n",
      "iteration: 108300 loss: 0.0024 lr: 0.005\n",
      "iteration: 108400 loss: 0.0024 lr: 0.005\n",
      "iteration: 108500 loss: 0.0028 lr: 0.005\n",
      "iteration: 108600 loss: 0.0031 lr: 0.005\n",
      "iteration: 108700 loss: 0.0023 lr: 0.005\n",
      "iteration: 108800 loss: 0.0024 lr: 0.005\n",
      "iteration: 108900 loss: 0.0024 lr: 0.005\n",
      "iteration: 109000 loss: 0.0028 lr: 0.005\n",
      "iteration: 109100 loss: 0.0023 lr: 0.005\n",
      "iteration: 109200 loss: 0.0031 lr: 0.005\n",
      "iteration: 109300 loss: 0.0032 lr: 0.005\n",
      "iteration: 109400 loss: 0.0033 lr: 0.005\n",
      "iteration: 109500 loss: 0.0031 lr: 0.005\n",
      "iteration: 109600 loss: 0.0028 lr: 0.005\n",
      "iteration: 109700 loss: 0.0030 lr: 0.005\n",
      "iteration: 109800 loss: 0.0028 lr: 0.005\n",
      "iteration: 109900 loss: 0.0028 lr: 0.005\n",
      "iteration: 110000 loss: 0.0029 lr: 0.005\n",
      "iteration: 110100 loss: 0.0030 lr: 0.005\n",
      "iteration: 110200 loss: 0.0025 lr: 0.005\n",
      "iteration: 110300 loss: 0.0026 lr: 0.005\n",
      "iteration: 110400 loss: 0.0025 lr: 0.005\n",
      "iteration: 110500 loss: 0.0025 lr: 0.005\n",
      "iteration: 110600 loss: 0.0027 lr: 0.005\n",
      "iteration: 110700 loss: 0.0026 lr: 0.005\n",
      "iteration: 110800 loss: 0.0025 lr: 0.005\n",
      "iteration: 110900 loss: 0.0028 lr: 0.005\n",
      "iteration: 111000 loss: 0.0027 lr: 0.005\n",
      "iteration: 111100 loss: 0.0031 lr: 0.005\n",
      "iteration: 111200 loss: 0.0023 lr: 0.005\n",
      "iteration: 111300 loss: 0.0025 lr: 0.005\n",
      "iteration: 111400 loss: 0.0027 lr: 0.005\n",
      "iteration: 111500 loss: 0.0029 lr: 0.005\n",
      "iteration: 111600 loss: 0.0021 lr: 0.005\n",
      "iteration: 111700 loss: 0.0025 lr: 0.005\n",
      "iteration: 111800 loss: 0.0025 lr: 0.005\n",
      "iteration: 111900 loss: 0.0029 lr: 0.005\n",
      "iteration: 112000 loss: 0.0025 lr: 0.005\n",
      "iteration: 112100 loss: 0.0027 lr: 0.005\n",
      "iteration: 112200 loss: 0.0024 lr: 0.005\n",
      "iteration: 112300 loss: 0.0027 lr: 0.005\n",
      "iteration: 112400 loss: 0.0025 lr: 0.005\n",
      "iteration: 112500 loss: 0.0026 lr: 0.005\n",
      "iteration: 112600 loss: 0.0024 lr: 0.005\n",
      "iteration: 112700 loss: 0.0027 lr: 0.005\n",
      "iteration: 112800 loss: 0.0029 lr: 0.005\n",
      "iteration: 112900 loss: 0.0022 lr: 0.005\n",
      "iteration: 113000 loss: 0.0032 lr: 0.005\n",
      "iteration: 113100 loss: 0.0023 lr: 0.005\n",
      "iteration: 113200 loss: 0.0027 lr: 0.005\n",
      "iteration: 113300 loss: 0.0029 lr: 0.005\n",
      "iteration: 113400 loss: 0.0033 lr: 0.005\n",
      "iteration: 113500 loss: 0.0024 lr: 0.005\n",
      "iteration: 113600 loss: 0.0027 lr: 0.005\n",
      "iteration: 113700 loss: 0.0025 lr: 0.005\n",
      "iteration: 113800 loss: 0.0026 lr: 0.005\n",
      "iteration: 113900 loss: 0.0027 lr: 0.005\n",
      "iteration: 114000 loss: 0.0024 lr: 0.005\n",
      "iteration: 114100 loss: 0.0026 lr: 0.005\n",
      "iteration: 114200 loss: 0.0027 lr: 0.005\n",
      "iteration: 114300 loss: 0.0029 lr: 0.005\n",
      "iteration: 114400 loss: 0.0029 lr: 0.005\n",
      "iteration: 114500 loss: 0.0029 lr: 0.005\n",
      "iteration: 114600 loss: 0.0021 lr: 0.005\n",
      "iteration: 114700 loss: 0.0024 lr: 0.005\n",
      "iteration: 114800 loss: 0.0023 lr: 0.005\n",
      "iteration: 114900 loss: 0.0023 lr: 0.005\n",
      "iteration: 115000 loss: 0.0031 lr: 0.005\n",
      "iteration: 115100 loss: 0.0025 lr: 0.005\n",
      "iteration: 115200 loss: 0.0025 lr: 0.005\n",
      "iteration: 115300 loss: 0.0028 lr: 0.005\n",
      "iteration: 115400 loss: 0.0025 lr: 0.005\n",
      "iteration: 115500 loss: 0.0027 lr: 0.005\n",
      "iteration: 115600 loss: 0.0030 lr: 0.005\n",
      "iteration: 115700 loss: 0.0023 lr: 0.005\n",
      "iteration: 115800 loss: 0.0028 lr: 0.005\n",
      "iteration: 115900 loss: 0.0026 lr: 0.005\n",
      "iteration: 116000 loss: 0.0025 lr: 0.005\n",
      "iteration: 116100 loss: 0.0027 lr: 0.005\n",
      "iteration: 116200 loss: 0.0031 lr: 0.005\n",
      "iteration: 116300 loss: 0.0027 lr: 0.005\n",
      "iteration: 116400 loss: 0.0024 lr: 0.005\n",
      "iteration: 116500 loss: 0.0028 lr: 0.005\n",
      "iteration: 116600 loss: 0.0028 lr: 0.005\n",
      "iteration: 116700 loss: 0.0027 lr: 0.005\n",
      "iteration: 116800 loss: 0.0024 lr: 0.005\n",
      "iteration: 116900 loss: 0.0031 lr: 0.005\n",
      "iteration: 117000 loss: 0.0027 lr: 0.005\n",
      "iteration: 117100 loss: 0.0026 lr: 0.005\n",
      "iteration: 117200 loss: 0.0026 lr: 0.005\n",
      "iteration: 117300 loss: 0.0024 lr: 0.005\n",
      "iteration: 117400 loss: 0.0026 lr: 0.005\n",
      "iteration: 117500 loss: 0.0028 lr: 0.005\n",
      "iteration: 117600 loss: 0.0026 lr: 0.005\n",
      "iteration: 117700 loss: 0.0026 lr: 0.005\n",
      "iteration: 117800 loss: 0.0030 lr: 0.005\n",
      "iteration: 117900 loss: 0.0027 lr: 0.005\n",
      "iteration: 118000 loss: 0.0029 lr: 0.005\n",
      "iteration: 118100 loss: 0.0027 lr: 0.005\n",
      "iteration: 118200 loss: 0.0025 lr: 0.005\n",
      "iteration: 118300 loss: 0.0032 lr: 0.005\n",
      "iteration: 118400 loss: 0.0027 lr: 0.005\n",
      "iteration: 118500 loss: 0.0026 lr: 0.005\n",
      "iteration: 118600 loss: 0.0029 lr: 0.005\n",
      "iteration: 118700 loss: 0.0028 lr: 0.005\n",
      "iteration: 118800 loss: 0.0022 lr: 0.005\n",
      "iteration: 118900 loss: 0.0024 lr: 0.005\n",
      "iteration: 119000 loss: 0.0024 lr: 0.005\n",
      "iteration: 119100 loss: 0.0025 lr: 0.005\n",
      "iteration: 119200 loss: 0.0026 lr: 0.005\n",
      "iteration: 119300 loss: 0.0023 lr: 0.005\n",
      "iteration: 119400 loss: 0.0024 lr: 0.005\n",
      "iteration: 119500 loss: 0.0030 lr: 0.005\n",
      "iteration: 119600 loss: 0.0027 lr: 0.005\n",
      "iteration: 119700 loss: 0.0026 lr: 0.005\n",
      "iteration: 119800 loss: 0.0026 lr: 0.005\n",
      "iteration: 119900 loss: 0.0026 lr: 0.005\n",
      "iteration: 120000 loss: 0.0025 lr: 0.005\n",
      "iteration: 120100 loss: 0.0031 lr: 0.005\n",
      "iteration: 120200 loss: 0.0023 lr: 0.005\n",
      "iteration: 120300 loss: 0.0030 lr: 0.005\n",
      "iteration: 120400 loss: 0.0028 lr: 0.005\n",
      "iteration: 120500 loss: 0.0027 lr: 0.005\n",
      "iteration: 120600 loss: 0.0026 lr: 0.005\n",
      "iteration: 120700 loss: 0.0025 lr: 0.005\n",
      "iteration: 120800 loss: 0.0025 lr: 0.005\n",
      "iteration: 120900 loss: 0.0023 lr: 0.005\n",
      "iteration: 121000 loss: 0.0029 lr: 0.005\n",
      "iteration: 121100 loss: 0.0029 lr: 0.005\n",
      "iteration: 121200 loss: 0.0026 lr: 0.005\n",
      "iteration: 121300 loss: 0.0029 lr: 0.005\n",
      "iteration: 121400 loss: 0.0030 lr: 0.005\n",
      "iteration: 121500 loss: 0.0024 lr: 0.005\n",
      "iteration: 121600 loss: 0.0024 lr: 0.005\n",
      "iteration: 121700 loss: 0.0027 lr: 0.005\n",
      "iteration: 121800 loss: 0.0028 lr: 0.005\n",
      "iteration: 121900 loss: 0.0032 lr: 0.005\n",
      "iteration: 122000 loss: 0.0025 lr: 0.005\n",
      "iteration: 122100 loss: 0.0022 lr: 0.005\n",
      "iteration: 122200 loss: 0.0025 lr: 0.005\n",
      "iteration: 122300 loss: 0.0032 lr: 0.005\n",
      "iteration: 122400 loss: 0.0025 lr: 0.005\n",
      "iteration: 122500 loss: 0.0026 lr: 0.005\n",
      "iteration: 122600 loss: 0.0025 lr: 0.005\n",
      "iteration: 122700 loss: 0.0028 lr: 0.005\n",
      "iteration: 122800 loss: 0.0028 lr: 0.005\n",
      "iteration: 122900 loss: 0.0022 lr: 0.005\n",
      "iteration: 123000 loss: 0.0027 lr: 0.005\n",
      "iteration: 123100 loss: 0.0025 lr: 0.005\n",
      "iteration: 123200 loss: 0.0023 lr: 0.005\n",
      "iteration: 123300 loss: 0.0025 lr: 0.005\n",
      "iteration: 123400 loss: 0.0022 lr: 0.005\n",
      "iteration: 123500 loss: 0.0028 lr: 0.005\n",
      "iteration: 123600 loss: 0.0027 lr: 0.005\n",
      "iteration: 123700 loss: 0.0025 lr: 0.005\n",
      "iteration: 123800 loss: 0.0025 lr: 0.005\n",
      "iteration: 123900 loss: 0.0027 lr: 0.005\n",
      "iteration: 124000 loss: 0.0024 lr: 0.005\n",
      "iteration: 124100 loss: 0.0020 lr: 0.005\n",
      "iteration: 124200 loss: 0.0023 lr: 0.005\n",
      "iteration: 124300 loss: 0.0027 lr: 0.005\n",
      "iteration: 124400 loss: 0.0027 lr: 0.005\n",
      "iteration: 124500 loss: 0.0028 lr: 0.005\n",
      "iteration: 124600 loss: 0.0029 lr: 0.005\n",
      "iteration: 124700 loss: 0.0023 lr: 0.005\n",
      "iteration: 124800 loss: 0.0026 lr: 0.005\n",
      "iteration: 124900 loss: 0.0026 lr: 0.005\n",
      "iteration: 125000 loss: 0.0024 lr: 0.005\n",
      "iteration: 125100 loss: 0.0024 lr: 0.005\n",
      "iteration: 125200 loss: 0.0025 lr: 0.005\n",
      "iteration: 125300 loss: 0.0025 lr: 0.005\n",
      "iteration: 125400 loss: 0.0022 lr: 0.005\n",
      "iteration: 125500 loss: 0.0027 lr: 0.005\n",
      "iteration: 125600 loss: 0.0023 lr: 0.005\n",
      "iteration: 125700 loss: 0.0027 lr: 0.005\n",
      "iteration: 125800 loss: 0.0022 lr: 0.005\n",
      "iteration: 125900 loss: 0.0025 lr: 0.005\n",
      "iteration: 126000 loss: 0.0031 lr: 0.005\n",
      "iteration: 126100 loss: 0.0026 lr: 0.005\n",
      "iteration: 126200 loss: 0.0026 lr: 0.005\n",
      "iteration: 126300 loss: 0.0023 lr: 0.005\n",
      "iteration: 126400 loss: 0.0023 lr: 0.005\n",
      "iteration: 126500 loss: 0.0027 lr: 0.005\n",
      "iteration: 126600 loss: 0.0025 lr: 0.005\n",
      "iteration: 126700 loss: 0.0028 lr: 0.005\n",
      "iteration: 126800 loss: 0.0027 lr: 0.005\n",
      "iteration: 126900 loss: 0.0021 lr: 0.005\n",
      "iteration: 127000 loss: 0.0025 lr: 0.005\n",
      "iteration: 127100 loss: 0.0027 lr: 0.005\n",
      "iteration: 127200 loss: 0.0027 lr: 0.005\n",
      "iteration: 127300 loss: 0.0026 lr: 0.005\n",
      "iteration: 127400 loss: 0.0028 lr: 0.005\n",
      "iteration: 127500 loss: 0.0026 lr: 0.005\n",
      "iteration: 127600 loss: 0.0023 lr: 0.005\n",
      "iteration: 127700 loss: 0.0024 lr: 0.005\n",
      "iteration: 127800 loss: 0.0026 lr: 0.005\n",
      "iteration: 127900 loss: 0.0022 lr: 0.005\n",
      "iteration: 128000 loss: 0.0034 lr: 0.005\n",
      "iteration: 128100 loss: 0.0023 lr: 0.005\n",
      "iteration: 128200 loss: 0.0024 lr: 0.005\n",
      "iteration: 128300 loss: 0.0024 lr: 0.005\n",
      "iteration: 128400 loss: 0.0022 lr: 0.005\n",
      "iteration: 128500 loss: 0.0026 lr: 0.005\n",
      "iteration: 128600 loss: 0.0026 lr: 0.005\n",
      "iteration: 128700 loss: 0.0027 lr: 0.005\n",
      "iteration: 128800 loss: 0.0024 lr: 0.005\n",
      "iteration: 128900 loss: 0.0023 lr: 0.005\n",
      "iteration: 129000 loss: 0.0022 lr: 0.005\n",
      "iteration: 129100 loss: 0.0026 lr: 0.005\n",
      "iteration: 129200 loss: 0.0024 lr: 0.005\n",
      "iteration: 129300 loss: 0.0022 lr: 0.005\n",
      "iteration: 129400 loss: 0.0022 lr: 0.005\n",
      "iteration: 129500 loss: 0.0024 lr: 0.005\n",
      "iteration: 129600 loss: 0.0026 lr: 0.005\n",
      "iteration: 129700 loss: 0.0024 lr: 0.005\n",
      "iteration: 129800 loss: 0.0028 lr: 0.005\n",
      "iteration: 129900 loss: 0.0025 lr: 0.005\n",
      "iteration: 130000 loss: 0.0026 lr: 0.005\n",
      "iteration: 130100 loss: 0.0027 lr: 0.005\n",
      "iteration: 130200 loss: 0.0026 lr: 0.005\n",
      "iteration: 130300 loss: 0.0024 lr: 0.005\n",
      "iteration: 130400 loss: 0.0026 lr: 0.005\n",
      "iteration: 130500 loss: 0.0023 lr: 0.005\n",
      "iteration: 130600 loss: 0.0025 lr: 0.005\n",
      "iteration: 130700 loss: 0.0025 lr: 0.005\n",
      "iteration: 130800 loss: 0.0023 lr: 0.005\n",
      "iteration: 130900 loss: 0.0026 lr: 0.005\n",
      "iteration: 131000 loss: 0.0025 lr: 0.005\n",
      "iteration: 131100 loss: 0.0026 lr: 0.005\n",
      "iteration: 131200 loss: 0.0028 lr: 0.005\n",
      "iteration: 131300 loss: 0.0025 lr: 0.005\n",
      "iteration: 131400 loss: 0.0025 lr: 0.005\n",
      "iteration: 131500 loss: 0.0022 lr: 0.005\n",
      "iteration: 131600 loss: 0.0023 lr: 0.005\n",
      "iteration: 131700 loss: 0.0026 lr: 0.005\n",
      "iteration: 131800 loss: 0.0025 lr: 0.005\n",
      "iteration: 131900 loss: 0.0024 lr: 0.005\n",
      "iteration: 132000 loss: 0.0026 lr: 0.005\n",
      "iteration: 132100 loss: 0.0024 lr: 0.005\n",
      "iteration: 132200 loss: 0.0027 lr: 0.005\n",
      "iteration: 132300 loss: 0.0027 lr: 0.005\n",
      "iteration: 132400 loss: 0.0025 lr: 0.005\n",
      "iteration: 132500 loss: 0.0025 lr: 0.005\n",
      "iteration: 132600 loss: 0.0022 lr: 0.005\n",
      "iteration: 132700 loss: 0.0027 lr: 0.005\n",
      "iteration: 132800 loss: 0.0025 lr: 0.005\n",
      "iteration: 132900 loss: 0.0023 lr: 0.005\n",
      "iteration: 133000 loss: 0.0030 lr: 0.005\n",
      "iteration: 133100 loss: 0.0024 lr: 0.005\n",
      "iteration: 133200 loss: 0.0022 lr: 0.005\n",
      "iteration: 133300 loss: 0.0024 lr: 0.005\n",
      "iteration: 133400 loss: 0.0025 lr: 0.005\n",
      "iteration: 133500 loss: 0.0020 lr: 0.005\n",
      "iteration: 133600 loss: 0.0025 lr: 0.005\n",
      "iteration: 133700 loss: 0.0024 lr: 0.005\n",
      "iteration: 133800 loss: 0.0022 lr: 0.005\n",
      "iteration: 133900 loss: 0.0027 lr: 0.005\n",
      "iteration: 134000 loss: 0.0022 lr: 0.005\n",
      "iteration: 134100 loss: 0.0024 lr: 0.005\n",
      "iteration: 134200 loss: 0.0023 lr: 0.005\n",
      "iteration: 134300 loss: 0.0026 lr: 0.005\n",
      "iteration: 134400 loss: 0.0023 lr: 0.005\n",
      "iteration: 134500 loss: 0.0028 lr: 0.005\n",
      "iteration: 134600 loss: 0.0025 lr: 0.005\n",
      "iteration: 134700 loss: 0.0030 lr: 0.005\n",
      "iteration: 134800 loss: 0.0027 lr: 0.005\n",
      "iteration: 134900 loss: 0.0025 lr: 0.005\n",
      "iteration: 135000 loss: 0.0025 lr: 0.005\n",
      "iteration: 135100 loss: 0.0026 lr: 0.005\n",
      "iteration: 135200 loss: 0.0024 lr: 0.005\n",
      "iteration: 135300 loss: 0.0026 lr: 0.005\n",
      "iteration: 135400 loss: 0.0027 lr: 0.005\n",
      "iteration: 135500 loss: 0.0025 lr: 0.005\n",
      "iteration: 135600 loss: 0.0023 lr: 0.005\n",
      "iteration: 135700 loss: 0.0024 lr: 0.005\n",
      "iteration: 135800 loss: 0.0027 lr: 0.005\n",
      "iteration: 135900 loss: 0.0024 lr: 0.005\n",
      "iteration: 136000 loss: 0.0022 lr: 0.005\n",
      "iteration: 136100 loss: 0.0027 lr: 0.005\n",
      "iteration: 136200 loss: 0.0025 lr: 0.005\n",
      "iteration: 136300 loss: 0.0025 lr: 0.005\n",
      "iteration: 136400 loss: 0.0026 lr: 0.005\n",
      "iteration: 136500 loss: 0.0033 lr: 0.005\n",
      "iteration: 136600 loss: 0.0027 lr: 0.005\n",
      "iteration: 136700 loss: 0.0024 lr: 0.005\n",
      "iteration: 136800 loss: 0.0023 lr: 0.005\n",
      "iteration: 136900 loss: 0.0025 lr: 0.005\n",
      "iteration: 137000 loss: 0.0026 lr: 0.005\n",
      "iteration: 137100 loss: 0.0022 lr: 0.005\n",
      "iteration: 137200 loss: 0.0024 lr: 0.005\n",
      "iteration: 137300 loss: 0.0025 lr: 0.005\n",
      "iteration: 137400 loss: 0.0027 lr: 0.005\n",
      "iteration: 137500 loss: 0.0026 lr: 0.005\n",
      "iteration: 137600 loss: 0.0025 lr: 0.005\n",
      "iteration: 137700 loss: 0.0029 lr: 0.005\n",
      "iteration: 137800 loss: 0.0026 lr: 0.005\n",
      "iteration: 137900 loss: 0.0027 lr: 0.005\n",
      "iteration: 138000 loss: 0.0030 lr: 0.005\n",
      "iteration: 138100 loss: 0.0024 lr: 0.005\n",
      "iteration: 138200 loss: 0.0024 lr: 0.005\n",
      "iteration: 138300 loss: 0.0028 lr: 0.005\n",
      "iteration: 138400 loss: 0.0026 lr: 0.005\n",
      "iteration: 138500 loss: 0.0024 lr: 0.005\n",
      "iteration: 138600 loss: 0.0026 lr: 0.005\n",
      "iteration: 138700 loss: 0.0023 lr: 0.005\n",
      "iteration: 138800 loss: 0.0027 lr: 0.005\n",
      "iteration: 138900 loss: 0.0025 lr: 0.005\n",
      "iteration: 139000 loss: 0.0024 lr: 0.005\n",
      "iteration: 139100 loss: 0.0025 lr: 0.005\n",
      "iteration: 139200 loss: 0.0026 lr: 0.005\n",
      "iteration: 139300 loss: 0.0023 lr: 0.005\n",
      "iteration: 139400 loss: 0.0023 lr: 0.005\n",
      "iteration: 139500 loss: 0.0025 lr: 0.005\n",
      "iteration: 139600 loss: 0.0026 lr: 0.005\n",
      "iteration: 139700 loss: 0.0020 lr: 0.005\n",
      "iteration: 139800 loss: 0.0023 lr: 0.005\n",
      "iteration: 139900 loss: 0.0023 lr: 0.005\n",
      "iteration: 140000 loss: 0.0024 lr: 0.005\n",
      "iteration: 140100 loss: 0.0024 lr: 0.005\n",
      "iteration: 140200 loss: 0.0024 lr: 0.005\n",
      "iteration: 140300 loss: 0.0025 lr: 0.005\n",
      "iteration: 140400 loss: 0.0025 lr: 0.005\n",
      "iteration: 140500 loss: 0.0025 lr: 0.005\n",
      "iteration: 140600 loss: 0.0022 lr: 0.005\n",
      "iteration: 140700 loss: 0.0022 lr: 0.005\n",
      "iteration: 140800 loss: 0.0025 lr: 0.005\n",
      "iteration: 140900 loss: 0.0024 lr: 0.005\n",
      "iteration: 141000 loss: 0.0027 lr: 0.005\n",
      "iteration: 141100 loss: 0.0022 lr: 0.005\n",
      "iteration: 141200 loss: 0.0021 lr: 0.005\n",
      "iteration: 141300 loss: 0.0027 lr: 0.005\n",
      "iteration: 141400 loss: 0.0024 lr: 0.005\n",
      "iteration: 141500 loss: 0.0027 lr: 0.005\n",
      "iteration: 141600 loss: 0.0023 lr: 0.005\n",
      "iteration: 141700 loss: 0.0026 lr: 0.005\n",
      "iteration: 141800 loss: 0.0023 lr: 0.005\n",
      "iteration: 141900 loss: 0.0025 lr: 0.005\n",
      "iteration: 142000 loss: 0.0027 lr: 0.005\n",
      "iteration: 142100 loss: 0.0026 lr: 0.005\n",
      "iteration: 142200 loss: 0.0026 lr: 0.005\n",
      "iteration: 142300 loss: 0.0023 lr: 0.005\n",
      "iteration: 142400 loss: 0.0025 lr: 0.005\n",
      "iteration: 142500 loss: 0.0024 lr: 0.005\n",
      "iteration: 142600 loss: 0.0031 lr: 0.005\n",
      "iteration: 142700 loss: 0.0026 lr: 0.005\n",
      "iteration: 142800 loss: 0.0024 lr: 0.005\n",
      "iteration: 142900 loss: 0.0025 lr: 0.005\n",
      "iteration: 143000 loss: 0.0030 lr: 0.005\n",
      "iteration: 143100 loss: 0.0021 lr: 0.005\n",
      "iteration: 143200 loss: 0.0024 lr: 0.005\n",
      "iteration: 143300 loss: 0.0029 lr: 0.005\n",
      "iteration: 143400 loss: 0.0025 lr: 0.005\n",
      "iteration: 143500 loss: 0.0025 lr: 0.005\n",
      "iteration: 143600 loss: 0.0026 lr: 0.005\n",
      "iteration: 143700 loss: 0.0022 lr: 0.005\n",
      "iteration: 143800 loss: 0.0024 lr: 0.005\n",
      "iteration: 143900 loss: 0.0028 lr: 0.005\n",
      "iteration: 144000 loss: 0.0027 lr: 0.005\n",
      "iteration: 144100 loss: 0.0025 lr: 0.005\n",
      "iteration: 144200 loss: 0.0023 lr: 0.005\n",
      "iteration: 144300 loss: 0.0026 lr: 0.005\n",
      "iteration: 144400 loss: 0.0028 lr: 0.005\n",
      "iteration: 144500 loss: 0.0026 lr: 0.005\n",
      "iteration: 144600 loss: 0.0026 lr: 0.005\n",
      "iteration: 144700 loss: 0.0022 lr: 0.005\n",
      "iteration: 144800 loss: 0.0025 lr: 0.005\n",
      "iteration: 144900 loss: 0.0026 lr: 0.005\n",
      "iteration: 145000 loss: 0.0029 lr: 0.005\n",
      "iteration: 145100 loss: 0.0025 lr: 0.005\n",
      "iteration: 145200 loss: 0.0023 lr: 0.005\n",
      "iteration: 145300 loss: 0.0020 lr: 0.005\n",
      "iteration: 145400 loss: 0.0028 lr: 0.005\n",
      "iteration: 145500 loss: 0.0022 lr: 0.005\n",
      "iteration: 145600 loss: 0.0024 lr: 0.005\n",
      "iteration: 145700 loss: 0.0021 lr: 0.005\n",
      "iteration: 145800 loss: 0.0024 lr: 0.005\n",
      "iteration: 145900 loss: 0.0022 lr: 0.005\n",
      "iteration: 146000 loss: 0.0024 lr: 0.005\n",
      "iteration: 146100 loss: 0.0024 lr: 0.005\n",
      "iteration: 146200 loss: 0.0022 lr: 0.005\n",
      "iteration: 146300 loss: 0.0021 lr: 0.005\n",
      "iteration: 146400 loss: 0.0021 lr: 0.005\n",
      "iteration: 146500 loss: 0.0020 lr: 0.005\n",
      "iteration: 146600 loss: 0.0022 lr: 0.005\n",
      "iteration: 146700 loss: 0.0026 lr: 0.005\n",
      "iteration: 146800 loss: 0.0021 lr: 0.005\n",
      "iteration: 146900 loss: 0.0024 lr: 0.005\n",
      "iteration: 147000 loss: 0.0022 lr: 0.005\n",
      "iteration: 147100 loss: 0.0028 lr: 0.005\n",
      "iteration: 147200 loss: 0.0022 lr: 0.005\n",
      "iteration: 147300 loss: 0.0026 lr: 0.005\n",
      "iteration: 147400 loss: 0.0024 lr: 0.005\n",
      "iteration: 147500 loss: 0.0024 lr: 0.005\n",
      "iteration: 147600 loss: 0.0022 lr: 0.005\n",
      "iteration: 147700 loss: 0.0024 lr: 0.005\n",
      "iteration: 147800 loss: 0.0023 lr: 0.005\n",
      "iteration: 147900 loss: 0.0024 lr: 0.005\n",
      "iteration: 148000 loss: 0.0021 lr: 0.005\n",
      "iteration: 148100 loss: 0.0022 lr: 0.005\n",
      "iteration: 148200 loss: 0.0020 lr: 0.005\n",
      "iteration: 148300 loss: 0.0026 lr: 0.005\n",
      "iteration: 148400 loss: 0.0024 lr: 0.005\n",
      "iteration: 148500 loss: 0.0024 lr: 0.005\n",
      "iteration: 148600 loss: 0.0025 lr: 0.005\n",
      "iteration: 148700 loss: 0.0025 lr: 0.005\n",
      "iteration: 148800 loss: 0.0021 lr: 0.005\n",
      "iteration: 148900 loss: 0.0024 lr: 0.005\n",
      "iteration: 149000 loss: 0.0023 lr: 0.005\n",
      "iteration: 149100 loss: 0.0027 lr: 0.005\n",
      "iteration: 149200 loss: 0.0028 lr: 0.005\n",
      "iteration: 149300 loss: 0.0027 lr: 0.005\n",
      "iteration: 149400 loss: 0.0028 lr: 0.005\n",
      "iteration: 149500 loss: 0.0023 lr: 0.005\n",
      "iteration: 149600 loss: 0.0022 lr: 0.005\n",
      "iteration: 149700 loss: 0.0026 lr: 0.005\n",
      "iteration: 149800 loss: 0.0022 lr: 0.005\n",
      "iteration: 149900 loss: 0.0026 lr: 0.005\n",
      "iteration: 150000 loss: 0.0025 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1377, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1455, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 83, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1396, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:\n",
      "\n",
      "Detected at node 'fifo_queue_enqueue' defined at (most recent call last):\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "      \"__main__\", mod_spec)\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "      app.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "      handle._run()\n",
      "    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "      ret = callback()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "      self.run()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "      yielded = self.gen.send(value)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "      yield gen.maybe_future(dispatch(*args))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "      yield gen.maybe_future(handler(stream, idents, msg))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "      user_expressions, allow_stdin,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "      res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "      raw_cell, store_history, silent, shell_futures)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "      return runner(coro)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "      interactivity=interactivity, compiler=compiler, result=result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "      if (await self.run_code(code, result,  async_=asy)):\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"<ipython-input-11-a0990424d681>\", line 4, in <module>\n",
      "      deeplabcut.train_network(path_config_file, shuffle=7, displayiters=100, saveiters=500)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "      allow_growth=allow_growth,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "      batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "      enqueue_op = q.enqueue(placeholders_list)\n",
      "Node: 'fifo_queue_enqueue'\n",
      "Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_enqueue':\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-a0990424d681>\", line 4, in <module>\n",
      "    deeplabcut.train_network(path_config_file, shuffle=7, displayiters=100, saveiters=500)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "    allow_growth=allow_growth,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 348, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4065, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3784, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2175, in __init__\n",
      "    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
    "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000+ iterations.\n",
    "\n",
    "deeplabcut.train_network(path_config_file, shuffle=7, displayiters=100, saveiters=500)\n",
    "# https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\n",
    "\n",
    "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
    "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIoLditzU3w4",
    "outputId": "a4e847fe-4f06-4443-98df-12dabbbd5123"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle8.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle8.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 150000]],\n",
      " 'net_type': 'resnet_101',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/content/drive/My '\n",
      "                 'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/content/drive/My '\n",
      "                    'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle8/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading ImageNet-pretrained resnet_101\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 500\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle8/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle8.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_101.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle8.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 150000]], 'net_type': 'resnet_101', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0515 lr: 0.005\n",
      "iteration: 200 loss: 0.0201 lr: 0.005\n",
      "iteration: 300 loss: 0.0194 lr: 0.005\n",
      "iteration: 400 loss: 0.0192 lr: 0.005\n",
      "iteration: 500 loss: 0.0204 lr: 0.005\n",
      "iteration: 600 loss: 0.0196 lr: 0.005\n",
      "iteration: 700 loss: 0.0174 lr: 0.005\n",
      "iteration: 800 loss: 0.0175 lr: 0.005\n",
      "iteration: 900 loss: 0.0167 lr: 0.005\n",
      "iteration: 1000 loss: 0.0156 lr: 0.005\n",
      "iteration: 1100 loss: 0.0151 lr: 0.005\n",
      "iteration: 1200 loss: 0.0146 lr: 0.005\n",
      "iteration: 1300 loss: 0.0142 lr: 0.005\n",
      "iteration: 1400 loss: 0.0136 lr: 0.005\n",
      "iteration: 1500 loss: 0.0141 lr: 0.005\n",
      "iteration: 1600 loss: 0.0118 lr: 0.005\n",
      "iteration: 1700 loss: 0.0109 lr: 0.005\n",
      "iteration: 1800 loss: 0.0115 lr: 0.005\n",
      "iteration: 1900 loss: 0.0109 lr: 0.005\n",
      "iteration: 2000 loss: 0.0094 lr: 0.005\n",
      "iteration: 2100 loss: 0.0091 lr: 0.005\n",
      "iteration: 2200 loss: 0.0095 lr: 0.005\n",
      "iteration: 2300 loss: 0.0100 lr: 0.005\n",
      "iteration: 2400 loss: 0.0088 lr: 0.005\n",
      "iteration: 2500 loss: 0.0085 lr: 0.005\n",
      "iteration: 2600 loss: 0.0092 lr: 0.005\n",
      "iteration: 2700 loss: 0.0089 lr: 0.005\n",
      "iteration: 2800 loss: 0.0085 lr: 0.005\n",
      "iteration: 2900 loss: 0.0090 lr: 0.005\n",
      "iteration: 3000 loss: 0.0086 lr: 0.005\n",
      "iteration: 3100 loss: 0.0085 lr: 0.005\n",
      "iteration: 3200 loss: 0.0085 lr: 0.005\n",
      "iteration: 3300 loss: 0.0081 lr: 0.005\n",
      "iteration: 3400 loss: 0.0079 lr: 0.005\n",
      "iteration: 3500 loss: 0.0074 lr: 0.005\n",
      "iteration: 3600 loss: 0.0078 lr: 0.005\n",
      "iteration: 3700 loss: 0.0079 lr: 0.005\n",
      "iteration: 3800 loss: 0.0082 lr: 0.005\n",
      "iteration: 3900 loss: 0.0075 lr: 0.005\n",
      "iteration: 4000 loss: 0.0065 lr: 0.005\n",
      "iteration: 4100 loss: 0.0075 lr: 0.005\n",
      "iteration: 4200 loss: 0.0069 lr: 0.005\n",
      "iteration: 4300 loss: 0.0081 lr: 0.005\n",
      "iteration: 4400 loss: 0.0078 lr: 0.005\n",
      "iteration: 4500 loss: 0.0083 lr: 0.005\n",
      "iteration: 4600 loss: 0.0078 lr: 0.005\n",
      "iteration: 4700 loss: 0.0072 lr: 0.005\n",
      "iteration: 4800 loss: 0.0072 lr: 0.005\n",
      "iteration: 4900 loss: 0.0072 lr: 0.005\n",
      "iteration: 5000 loss: 0.0071 lr: 0.005\n",
      "iteration: 5100 loss: 0.0064 lr: 0.005\n",
      "iteration: 5200 loss: 0.0077 lr: 0.005\n",
      "iteration: 5300 loss: 0.0063 lr: 0.005\n",
      "iteration: 5400 loss: 0.0066 lr: 0.005\n",
      "iteration: 5500 loss: 0.0071 lr: 0.005\n",
      "iteration: 5600 loss: 0.0065 lr: 0.005\n",
      "iteration: 5700 loss: 0.0062 lr: 0.005\n",
      "iteration: 5800 loss: 0.0061 lr: 0.005\n",
      "iteration: 5900 loss: 0.0072 lr: 0.005\n",
      "iteration: 6000 loss: 0.0067 lr: 0.005\n",
      "iteration: 6100 loss: 0.0059 lr: 0.005\n",
      "iteration: 6200 loss: 0.0059 lr: 0.005\n",
      "iteration: 6300 loss: 0.0060 lr: 0.005\n",
      "iteration: 6400 loss: 0.0058 lr: 0.005\n",
      "iteration: 6500 loss: 0.0065 lr: 0.005\n",
      "iteration: 6600 loss: 0.0060 lr: 0.005\n",
      "iteration: 6700 loss: 0.0065 lr: 0.005\n",
      "iteration: 6800 loss: 0.0060 lr: 0.005\n",
      "iteration: 6900 loss: 0.0063 lr: 0.005\n",
      "iteration: 7000 loss: 0.0067 lr: 0.005\n",
      "iteration: 7100 loss: 0.0053 lr: 0.005\n",
      "iteration: 7200 loss: 0.0056 lr: 0.005\n",
      "iteration: 7300 loss: 0.0063 lr: 0.005\n",
      "iteration: 7400 loss: 0.0053 lr: 0.005\n",
      "iteration: 7500 loss: 0.0057 lr: 0.005\n",
      "iteration: 7600 loss: 0.0062 lr: 0.005\n",
      "iteration: 7700 loss: 0.0054 lr: 0.005\n",
      "iteration: 7800 loss: 0.0063 lr: 0.005\n",
      "iteration: 7900 loss: 0.0062 lr: 0.005\n",
      "iteration: 8000 loss: 0.0069 lr: 0.005\n",
      "iteration: 8100 loss: 0.0061 lr: 0.005\n",
      "iteration: 8200 loss: 0.0052 lr: 0.005\n",
      "iteration: 8300 loss: 0.0058 lr: 0.005\n",
      "iteration: 8400 loss: 0.0055 lr: 0.005\n",
      "iteration: 8500 loss: 0.0060 lr: 0.005\n",
      "iteration: 8600 loss: 0.0053 lr: 0.005\n",
      "iteration: 8700 loss: 0.0061 lr: 0.005\n",
      "iteration: 8800 loss: 0.0067 lr: 0.005\n",
      "iteration: 8900 loss: 0.0053 lr: 0.005\n",
      "iteration: 9000 loss: 0.0057 lr: 0.005\n",
      "iteration: 9100 loss: 0.0057 lr: 0.005\n",
      "iteration: 9200 loss: 0.0060 lr: 0.005\n",
      "iteration: 9300 loss: 0.0059 lr: 0.005\n",
      "iteration: 9400 loss: 0.0061 lr: 0.005\n",
      "iteration: 9500 loss: 0.0055 lr: 0.005\n",
      "iteration: 9600 loss: 0.0053 lr: 0.005\n",
      "iteration: 9700 loss: 0.0052 lr: 0.005\n",
      "iteration: 9800 loss: 0.0050 lr: 0.005\n",
      "iteration: 9900 loss: 0.0065 lr: 0.005\n",
      "iteration: 10000 loss: 0.0057 lr: 0.005\n",
      "iteration: 10100 loss: 0.0057 lr: 0.005\n",
      "iteration: 10200 loss: 0.0057 lr: 0.005\n",
      "iteration: 10300 loss: 0.0057 lr: 0.005\n",
      "iteration: 10400 loss: 0.0050 lr: 0.005\n",
      "iteration: 10500 loss: 0.0054 lr: 0.005\n",
      "iteration: 10600 loss: 0.0049 lr: 0.005\n",
      "iteration: 10700 loss: 0.0047 lr: 0.005\n",
      "iteration: 10800 loss: 0.0054 lr: 0.005\n",
      "iteration: 10900 loss: 0.0051 lr: 0.005\n",
      "iteration: 11000 loss: 0.0050 lr: 0.005\n",
      "iteration: 11100 loss: 0.0061 lr: 0.005\n",
      "iteration: 11200 loss: 0.0048 lr: 0.005\n",
      "iteration: 11300 loss: 0.0053 lr: 0.005\n",
      "iteration: 11400 loss: 0.0049 lr: 0.005\n",
      "iteration: 11500 loss: 0.0057 lr: 0.005\n",
      "iteration: 11600 loss: 0.0049 lr: 0.005\n",
      "iteration: 11700 loss: 0.0058 lr: 0.005\n",
      "iteration: 11800 loss: 0.0048 lr: 0.005\n",
      "iteration: 11900 loss: 0.0057 lr: 0.005\n",
      "iteration: 12000 loss: 0.0053 lr: 0.005\n",
      "iteration: 12100 loss: 0.0056 lr: 0.005\n",
      "iteration: 12200 loss: 0.0053 lr: 0.005\n",
      "iteration: 12300 loss: 0.0051 lr: 0.005\n",
      "iteration: 12400 loss: 0.0046 lr: 0.005\n",
      "iteration: 12500 loss: 0.0059 lr: 0.005\n",
      "iteration: 12600 loss: 0.0046 lr: 0.005\n",
      "iteration: 12700 loss: 0.0053 lr: 0.005\n",
      "iteration: 12800 loss: 0.0048 lr: 0.005\n",
      "iteration: 12900 loss: 0.0054 lr: 0.005\n",
      "iteration: 13000 loss: 0.0045 lr: 0.005\n",
      "iteration: 13100 loss: 0.0047 lr: 0.005\n",
      "iteration: 13200 loss: 0.0045 lr: 0.005\n",
      "iteration: 13300 loss: 0.0054 lr: 0.005\n",
      "iteration: 13400 loss: 0.0050 lr: 0.005\n",
      "iteration: 13500 loss: 0.0043 lr: 0.005\n",
      "iteration: 13600 loss: 0.0046 lr: 0.005\n",
      "iteration: 13700 loss: 0.0053 lr: 0.005\n",
      "iteration: 13800 loss: 0.0042 lr: 0.005\n",
      "iteration: 13900 loss: 0.0048 lr: 0.005\n",
      "iteration: 14000 loss: 0.0053 lr: 0.005\n",
      "iteration: 14100 loss: 0.0057 lr: 0.005\n",
      "iteration: 14200 loss: 0.0045 lr: 0.005\n",
      "iteration: 14300 loss: 0.0047 lr: 0.005\n",
      "iteration: 14400 loss: 0.0047 lr: 0.005\n",
      "iteration: 14500 loss: 0.0057 lr: 0.005\n",
      "iteration: 14600 loss: 0.0036 lr: 0.005\n",
      "iteration: 14700 loss: 0.0046 lr: 0.005\n",
      "iteration: 14800 loss: 0.0050 lr: 0.005\n",
      "iteration: 14900 loss: 0.0047 lr: 0.005\n",
      "iteration: 15000 loss: 0.0049 lr: 0.005\n",
      "iteration: 15100 loss: 0.0048 lr: 0.005\n",
      "iteration: 15200 loss: 0.0049 lr: 0.005\n",
      "iteration: 15300 loss: 0.0048 lr: 0.005\n",
      "iteration: 15400 loss: 0.0039 lr: 0.005\n",
      "iteration: 15500 loss: 0.0047 lr: 0.005\n",
      "iteration: 15600 loss: 0.0055 lr: 0.005\n",
      "iteration: 15700 loss: 0.0040 lr: 0.005\n",
      "iteration: 15800 loss: 0.0051 lr: 0.005\n",
      "iteration: 15900 loss: 0.0042 lr: 0.005\n",
      "iteration: 16000 loss: 0.0045 lr: 0.005\n",
      "iteration: 16100 loss: 0.0043 lr: 0.005\n",
      "iteration: 16200 loss: 0.0048 lr: 0.005\n",
      "iteration: 16300 loss: 0.0045 lr: 0.005\n",
      "iteration: 16400 loss: 0.0055 lr: 0.005\n",
      "iteration: 16500 loss: 0.0048 lr: 0.005\n",
      "iteration: 16600 loss: 0.0048 lr: 0.005\n",
      "iteration: 16700 loss: 0.0051 lr: 0.005\n",
      "iteration: 16800 loss: 0.0047 lr: 0.005\n",
      "iteration: 16900 loss: 0.0040 lr: 0.005\n",
      "iteration: 17000 loss: 0.0048 lr: 0.005\n",
      "iteration: 17100 loss: 0.0053 lr: 0.005\n",
      "iteration: 17200 loss: 0.0048 lr: 0.005\n",
      "iteration: 17300 loss: 0.0050 lr: 0.005\n",
      "iteration: 17400 loss: 0.0046 lr: 0.005\n",
      "iteration: 17500 loss: 0.0052 lr: 0.005\n",
      "iteration: 17600 loss: 0.0043 lr: 0.005\n",
      "iteration: 17700 loss: 0.0045 lr: 0.005\n",
      "iteration: 17800 loss: 0.0051 lr: 0.005\n",
      "iteration: 17900 loss: 0.0046 lr: 0.005\n",
      "iteration: 18000 loss: 0.0044 lr: 0.005\n",
      "iteration: 18100 loss: 0.0048 lr: 0.005\n",
      "iteration: 18200 loss: 0.0053 lr: 0.005\n",
      "iteration: 18300 loss: 0.0045 lr: 0.005\n",
      "iteration: 18400 loss: 0.0043 lr: 0.005\n",
      "iteration: 18500 loss: 0.0046 lr: 0.005\n",
      "iteration: 18600 loss: 0.0042 lr: 0.005\n",
      "iteration: 18700 loss: 0.0044 lr: 0.005\n",
      "iteration: 18800 loss: 0.0039 lr: 0.005\n",
      "iteration: 18900 loss: 0.0040 lr: 0.005\n",
      "iteration: 19000 loss: 0.0048 lr: 0.005\n",
      "iteration: 19100 loss: 0.0043 lr: 0.005\n",
      "iteration: 19200 loss: 0.0047 lr: 0.005\n",
      "iteration: 19300 loss: 0.0049 lr: 0.005\n",
      "iteration: 19400 loss: 0.0045 lr: 0.005\n",
      "iteration: 19500 loss: 0.0052 lr: 0.005\n",
      "iteration: 19600 loss: 0.0039 lr: 0.005\n",
      "iteration: 19700 loss: 0.0043 lr: 0.005\n",
      "iteration: 19800 loss: 0.0038 lr: 0.005\n",
      "iteration: 19900 loss: 0.0042 lr: 0.005\n",
      "iteration: 20000 loss: 0.0042 lr: 0.005\n",
      "iteration: 20100 loss: 0.0042 lr: 0.005\n",
      "iteration: 20200 loss: 0.0043 lr: 0.005\n",
      "iteration: 20300 loss: 0.0040 lr: 0.005\n",
      "iteration: 20400 loss: 0.0047 lr: 0.005\n",
      "iteration: 20500 loss: 0.0042 lr: 0.005\n",
      "iteration: 20600 loss: 0.0041 lr: 0.005\n",
      "iteration: 20700 loss: 0.0045 lr: 0.005\n",
      "iteration: 20800 loss: 0.0042 lr: 0.005\n",
      "iteration: 20900 loss: 0.0045 lr: 0.005\n",
      "iteration: 21000 loss: 0.0043 lr: 0.005\n",
      "iteration: 21100 loss: 0.0042 lr: 0.005\n",
      "iteration: 21200 loss: 0.0040 lr: 0.005\n",
      "iteration: 21300 loss: 0.0047 lr: 0.005\n",
      "iteration: 21400 loss: 0.0046 lr: 0.005\n",
      "iteration: 21500 loss: 0.0037 lr: 0.005\n",
      "iteration: 21600 loss: 0.0042 lr: 0.005\n",
      "iteration: 21700 loss: 0.0043 lr: 0.005\n",
      "iteration: 21800 loss: 0.0052 lr: 0.005\n",
      "iteration: 21900 loss: 0.0045 lr: 0.005\n",
      "iteration: 22000 loss: 0.0039 lr: 0.005\n",
      "iteration: 22100 loss: 0.0048 lr: 0.005\n",
      "iteration: 22200 loss: 0.0043 lr: 0.005\n",
      "iteration: 22300 loss: 0.0040 lr: 0.005\n",
      "iteration: 22400 loss: 0.0048 lr: 0.005\n",
      "iteration: 22500 loss: 0.0044 lr: 0.005\n",
      "iteration: 22600 loss: 0.0042 lr: 0.005\n",
      "iteration: 22700 loss: 0.0042 lr: 0.005\n",
      "iteration: 22800 loss: 0.0045 lr: 0.005\n",
      "iteration: 22900 loss: 0.0043 lr: 0.005\n",
      "iteration: 23000 loss: 0.0043 lr: 0.005\n",
      "iteration: 23100 loss: 0.0046 lr: 0.005\n",
      "iteration: 23200 loss: 0.0041 lr: 0.005\n",
      "iteration: 23300 loss: 0.0051 lr: 0.005\n",
      "iteration: 23400 loss: 0.0038 lr: 0.005\n",
      "iteration: 23500 loss: 0.0046 lr: 0.005\n",
      "iteration: 23600 loss: 0.0036 lr: 0.005\n",
      "iteration: 23700 loss: 0.0046 lr: 0.005\n",
      "iteration: 23800 loss: 0.0040 lr: 0.005\n",
      "iteration: 23900 loss: 0.0039 lr: 0.005\n",
      "iteration: 24000 loss: 0.0043 lr: 0.005\n",
      "iteration: 24100 loss: 0.0043 lr: 0.005\n",
      "iteration: 24200 loss: 0.0042 lr: 0.005\n",
      "iteration: 24300 loss: 0.0039 lr: 0.005\n",
      "iteration: 24400 loss: 0.0046 lr: 0.005\n",
      "iteration: 24500 loss: 0.0037 lr: 0.005\n",
      "iteration: 24600 loss: 0.0045 lr: 0.005\n",
      "iteration: 24700 loss: 0.0043 lr: 0.005\n",
      "iteration: 24800 loss: 0.0035 lr: 0.005\n",
      "iteration: 24900 loss: 0.0043 lr: 0.005\n",
      "iteration: 25000 loss: 0.0043 lr: 0.005\n",
      "iteration: 25100 loss: 0.0041 lr: 0.005\n",
      "iteration: 25200 loss: 0.0034 lr: 0.005\n",
      "iteration: 25300 loss: 0.0034 lr: 0.005\n",
      "iteration: 25400 loss: 0.0047 lr: 0.005\n",
      "iteration: 25500 loss: 0.0038 lr: 0.005\n",
      "iteration: 25600 loss: 0.0042 lr: 0.005\n",
      "iteration: 25700 loss: 0.0038 lr: 0.005\n",
      "iteration: 25800 loss: 0.0043 lr: 0.005\n",
      "iteration: 25900 loss: 0.0046 lr: 0.005\n",
      "iteration: 26000 loss: 0.0036 lr: 0.005\n",
      "iteration: 26100 loss: 0.0039 lr: 0.005\n",
      "iteration: 26200 loss: 0.0037 lr: 0.005\n",
      "iteration: 26300 loss: 0.0041 lr: 0.005\n",
      "iteration: 26400 loss: 0.0042 lr: 0.005\n",
      "iteration: 26500 loss: 0.0036 lr: 0.005\n",
      "iteration: 26600 loss: 0.0033 lr: 0.005\n",
      "iteration: 26700 loss: 0.0041 lr: 0.005\n",
      "iteration: 26800 loss: 0.0039 lr: 0.005\n",
      "iteration: 26900 loss: 0.0034 lr: 0.005\n",
      "iteration: 27000 loss: 0.0040 lr: 0.005\n",
      "iteration: 27100 loss: 0.0036 lr: 0.005\n",
      "iteration: 27200 loss: 0.0040 lr: 0.005\n",
      "iteration: 27300 loss: 0.0033 lr: 0.005\n",
      "iteration: 27400 loss: 0.0038 lr: 0.005\n",
      "iteration: 27500 loss: 0.0035 lr: 0.005\n",
      "iteration: 27600 loss: 0.0043 lr: 0.005\n",
      "iteration: 27700 loss: 0.0042 lr: 0.005\n",
      "iteration: 27800 loss: 0.0036 lr: 0.005\n",
      "iteration: 27900 loss: 0.0037 lr: 0.005\n",
      "iteration: 28000 loss: 0.0041 lr: 0.005\n",
      "iteration: 28100 loss: 0.0041 lr: 0.005\n",
      "iteration: 28200 loss: 0.0037 lr: 0.005\n",
      "iteration: 28300 loss: 0.0041 lr: 0.005\n",
      "iteration: 28400 loss: 0.0037 lr: 0.005\n",
      "iteration: 28500 loss: 0.0033 lr: 0.005\n",
      "iteration: 28600 loss: 0.0036 lr: 0.005\n",
      "iteration: 28700 loss: 0.0039 lr: 0.005\n",
      "iteration: 28800 loss: 0.0038 lr: 0.005\n",
      "iteration: 28900 loss: 0.0041 lr: 0.005\n",
      "iteration: 29000 loss: 0.0038 lr: 0.005\n",
      "iteration: 29100 loss: 0.0038 lr: 0.005\n",
      "iteration: 29200 loss: 0.0033 lr: 0.005\n",
      "iteration: 29300 loss: 0.0032 lr: 0.005\n",
      "iteration: 29400 loss: 0.0034 lr: 0.005\n",
      "iteration: 29500 loss: 0.0040 lr: 0.005\n",
      "iteration: 29600 loss: 0.0036 lr: 0.005\n",
      "iteration: 29700 loss: 0.0033 lr: 0.005\n",
      "iteration: 29800 loss: 0.0043 lr: 0.005\n",
      "iteration: 29900 loss: 0.0044 lr: 0.005\n",
      "iteration: 30000 loss: 0.0043 lr: 0.005\n",
      "iteration: 30100 loss: 0.0040 lr: 0.005\n",
      "iteration: 30200 loss: 0.0034 lr: 0.005\n",
      "iteration: 30300 loss: 0.0039 lr: 0.005\n",
      "iteration: 30400 loss: 0.0048 lr: 0.005\n",
      "iteration: 30500 loss: 0.0040 lr: 0.005\n",
      "iteration: 30600 loss: 0.0043 lr: 0.005\n",
      "iteration: 30700 loss: 0.0035 lr: 0.005\n",
      "iteration: 30800 loss: 0.0040 lr: 0.005\n",
      "iteration: 30900 loss: 0.0037 lr: 0.005\n",
      "iteration: 31000 loss: 0.0037 lr: 0.005\n",
      "iteration: 31100 loss: 0.0037 lr: 0.005\n",
      "iteration: 31200 loss: 0.0040 lr: 0.005\n",
      "iteration: 31300 loss: 0.0043 lr: 0.005\n",
      "iteration: 31400 loss: 0.0037 lr: 0.005\n",
      "iteration: 31500 loss: 0.0038 lr: 0.005\n",
      "iteration: 31600 loss: 0.0034 lr: 0.005\n",
      "iteration: 31700 loss: 0.0036 lr: 0.005\n",
      "iteration: 31800 loss: 0.0036 lr: 0.005\n",
      "iteration: 31900 loss: 0.0032 lr: 0.005\n",
      "iteration: 32000 loss: 0.0036 lr: 0.005\n",
      "iteration: 32100 loss: 0.0037 lr: 0.005\n",
      "iteration: 32200 loss: 0.0039 lr: 0.005\n",
      "iteration: 32300 loss: 0.0040 lr: 0.005\n",
      "iteration: 32400 loss: 0.0039 lr: 0.005\n",
      "iteration: 32500 loss: 0.0034 lr: 0.005\n",
      "iteration: 32600 loss: 0.0041 lr: 0.005\n",
      "iteration: 32700 loss: 0.0039 lr: 0.005\n",
      "iteration: 32800 loss: 0.0037 lr: 0.005\n",
      "iteration: 32900 loss: 0.0039 lr: 0.005\n",
      "iteration: 33000 loss: 0.0037 lr: 0.005\n",
      "iteration: 33100 loss: 0.0036 lr: 0.005\n",
      "iteration: 33200 loss: 0.0036 lr: 0.005\n",
      "iteration: 33300 loss: 0.0040 lr: 0.005\n",
      "iteration: 33400 loss: 0.0035 lr: 0.005\n",
      "iteration: 33500 loss: 0.0036 lr: 0.005\n",
      "iteration: 33600 loss: 0.0034 lr: 0.005\n",
      "iteration: 33700 loss: 0.0039 lr: 0.005\n",
      "iteration: 33800 loss: 0.0038 lr: 0.005\n",
      "iteration: 33900 loss: 0.0035 lr: 0.005\n",
      "iteration: 34000 loss: 0.0036 lr: 0.005\n",
      "iteration: 34100 loss: 0.0034 lr: 0.005\n",
      "iteration: 34200 loss: 0.0041 lr: 0.005\n",
      "iteration: 34300 loss: 0.0037 lr: 0.005\n",
      "iteration: 34400 loss: 0.0036 lr: 0.005\n",
      "iteration: 34500 loss: 0.0042 lr: 0.005\n",
      "iteration: 34600 loss: 0.0032 lr: 0.005\n",
      "iteration: 34700 loss: 0.0033 lr: 0.005\n",
      "iteration: 34800 loss: 0.0037 lr: 0.005\n",
      "iteration: 34900 loss: 0.0040 lr: 0.005\n",
      "iteration: 35000 loss: 0.0037 lr: 0.005\n",
      "iteration: 35100 loss: 0.0033 lr: 0.005\n",
      "iteration: 35200 loss: 0.0037 lr: 0.005\n",
      "iteration: 35300 loss: 0.0035 lr: 0.005\n",
      "iteration: 35400 loss: 0.0044 lr: 0.005\n",
      "iteration: 35500 loss: 0.0041 lr: 0.005\n",
      "iteration: 35600 loss: 0.0034 lr: 0.005\n",
      "iteration: 35700 loss: 0.0033 lr: 0.005\n",
      "iteration: 35800 loss: 0.0042 lr: 0.005\n",
      "iteration: 35900 loss: 0.0034 lr: 0.005\n",
      "iteration: 36000 loss: 0.0036 lr: 0.005\n",
      "iteration: 36100 loss: 0.0039 lr: 0.005\n",
      "iteration: 36200 loss: 0.0039 lr: 0.005\n",
      "iteration: 36300 loss: 0.0039 lr: 0.005\n",
      "iteration: 36400 loss: 0.0039 lr: 0.005\n",
      "iteration: 36500 loss: 0.0035 lr: 0.005\n",
      "iteration: 36600 loss: 0.0037 lr: 0.005\n",
      "iteration: 36700 loss: 0.0034 lr: 0.005\n",
      "iteration: 36800 loss: 0.0043 lr: 0.005\n",
      "iteration: 36900 loss: 0.0030 lr: 0.005\n",
      "iteration: 37000 loss: 0.0031 lr: 0.005\n",
      "iteration: 37100 loss: 0.0038 lr: 0.005\n",
      "iteration: 37200 loss: 0.0038 lr: 0.005\n",
      "iteration: 37300 loss: 0.0031 lr: 0.005\n",
      "iteration: 37400 loss: 0.0032 lr: 0.005\n",
      "iteration: 37500 loss: 0.0035 lr: 0.005\n",
      "iteration: 37600 loss: 0.0031 lr: 0.005\n",
      "iteration: 37700 loss: 0.0041 lr: 0.005\n",
      "iteration: 37800 loss: 0.0033 lr: 0.005\n",
      "iteration: 37900 loss: 0.0038 lr: 0.005\n",
      "iteration: 38000 loss: 0.0035 lr: 0.005\n",
      "iteration: 38100 loss: 0.0034 lr: 0.005\n",
      "iteration: 38200 loss: 0.0033 lr: 0.005\n",
      "iteration: 38300 loss: 0.0037 lr: 0.005\n",
      "iteration: 38400 loss: 0.0031 lr: 0.005\n",
      "iteration: 38500 loss: 0.0042 lr: 0.005\n",
      "iteration: 38600 loss: 0.0030 lr: 0.005\n",
      "iteration: 38700 loss: 0.0034 lr: 0.005\n",
      "iteration: 38800 loss: 0.0034 lr: 0.005\n",
      "iteration: 38900 loss: 0.0028 lr: 0.005\n",
      "iteration: 39000 loss: 0.0033 lr: 0.005\n",
      "iteration: 39100 loss: 0.0036 lr: 0.005\n",
      "iteration: 39200 loss: 0.0030 lr: 0.005\n",
      "iteration: 39300 loss: 0.0035 lr: 0.005\n",
      "iteration: 39400 loss: 0.0031 lr: 0.005\n",
      "iteration: 39500 loss: 0.0040 lr: 0.005\n",
      "iteration: 39600 loss: 0.0031 lr: 0.005\n",
      "iteration: 39700 loss: 0.0036 lr: 0.005\n",
      "iteration: 39800 loss: 0.0039 lr: 0.005\n",
      "iteration: 39900 loss: 0.0032 lr: 0.005\n",
      "iteration: 40000 loss: 0.0036 lr: 0.005\n",
      "iteration: 40100 loss: 0.0038 lr: 0.005\n",
      "iteration: 40200 loss: 0.0037 lr: 0.005\n",
      "iteration: 40300 loss: 0.0037 lr: 0.005\n",
      "iteration: 40400 loss: 0.0039 lr: 0.005\n",
      "iteration: 40500 loss: 0.0035 lr: 0.005\n",
      "iteration: 40600 loss: 0.0032 lr: 0.005\n",
      "iteration: 40700 loss: 0.0031 lr: 0.005\n",
      "iteration: 40800 loss: 0.0035 lr: 0.005\n",
      "iteration: 40900 loss: 0.0043 lr: 0.005\n",
      "iteration: 41000 loss: 0.0043 lr: 0.005\n",
      "iteration: 41100 loss: 0.0036 lr: 0.005\n",
      "iteration: 41200 loss: 0.0048 lr: 0.005\n",
      "iteration: 41300 loss: 0.0040 lr: 0.005\n",
      "iteration: 41400 loss: 0.0030 lr: 0.005\n",
      "iteration: 41500 loss: 0.0033 lr: 0.005\n",
      "iteration: 41600 loss: 0.0042 lr: 0.005\n",
      "iteration: 41700 loss: 0.0032 lr: 0.005\n",
      "iteration: 41800 loss: 0.0038 lr: 0.005\n",
      "iteration: 41900 loss: 0.0034 lr: 0.005\n",
      "iteration: 42000 loss: 0.0033 lr: 0.005\n",
      "iteration: 42100 loss: 0.0030 lr: 0.005\n",
      "iteration: 42200 loss: 0.0039 lr: 0.005\n",
      "iteration: 42300 loss: 0.0029 lr: 0.005\n",
      "iteration: 42400 loss: 0.0033 lr: 0.005\n",
      "iteration: 42500 loss: 0.0036 lr: 0.005\n",
      "iteration: 42600 loss: 0.0038 lr: 0.005\n",
      "iteration: 42700 loss: 0.0031 lr: 0.005\n",
      "iteration: 42800 loss: 0.0032 lr: 0.005\n",
      "iteration: 42900 loss: 0.0044 lr: 0.005\n",
      "iteration: 43000 loss: 0.0033 lr: 0.005\n",
      "iteration: 43100 loss: 0.0032 lr: 0.005\n",
      "iteration: 43200 loss: 0.0030 lr: 0.005\n",
      "iteration: 43300 loss: 0.0030 lr: 0.005\n",
      "iteration: 43400 loss: 0.0034 lr: 0.005\n",
      "iteration: 43500 loss: 0.0034 lr: 0.005\n",
      "iteration: 43600 loss: 0.0030 lr: 0.005\n",
      "iteration: 43700 loss: 0.0035 lr: 0.005\n",
      "iteration: 43800 loss: 0.0035 lr: 0.005\n",
      "iteration: 43900 loss: 0.0032 lr: 0.005\n",
      "iteration: 44000 loss: 0.0036 lr: 0.005\n",
      "iteration: 44100 loss: 0.0031 lr: 0.005\n",
      "iteration: 44200 loss: 0.0035 lr: 0.005\n",
      "iteration: 44300 loss: 0.0033 lr: 0.005\n",
      "iteration: 44400 loss: 0.0032 lr: 0.005\n",
      "iteration: 44500 loss: 0.0032 lr: 0.005\n",
      "iteration: 44600 loss: 0.0039 lr: 0.005\n",
      "iteration: 44700 loss: 0.0035 lr: 0.005\n",
      "iteration: 44800 loss: 0.0036 lr: 0.005\n",
      "iteration: 44900 loss: 0.0033 lr: 0.005\n",
      "iteration: 45000 loss: 0.0036 lr: 0.005\n",
      "iteration: 45100 loss: 0.0033 lr: 0.005\n",
      "iteration: 45200 loss: 0.0030 lr: 0.005\n",
      "iteration: 45300 loss: 0.0034 lr: 0.005\n",
      "iteration: 45400 loss: 0.0038 lr: 0.005\n",
      "iteration: 45500 loss: 0.0033 lr: 0.005\n",
      "iteration: 45600 loss: 0.0030 lr: 0.005\n",
      "iteration: 45700 loss: 0.0035 lr: 0.005\n",
      "iteration: 45800 loss: 0.0037 lr: 0.005\n",
      "iteration: 45900 loss: 0.0032 lr: 0.005\n",
      "iteration: 46000 loss: 0.0030 lr: 0.005\n",
      "iteration: 46100 loss: 0.0031 lr: 0.005\n",
      "iteration: 46200 loss: 0.0036 lr: 0.005\n",
      "iteration: 46300 loss: 0.0036 lr: 0.005\n",
      "iteration: 46400 loss: 0.0037 lr: 0.005\n",
      "iteration: 46500 loss: 0.0034 lr: 0.005\n",
      "iteration: 46600 loss: 0.0034 lr: 0.005\n",
      "iteration: 46700 loss: 0.0033 lr: 0.005\n",
      "iteration: 46800 loss: 0.0036 lr: 0.005\n",
      "iteration: 46900 loss: 0.0030 lr: 0.005\n",
      "iteration: 47000 loss: 0.0034 lr: 0.005\n",
      "iteration: 47100 loss: 0.0034 lr: 0.005\n",
      "iteration: 47200 loss: 0.0031 lr: 0.005\n",
      "iteration: 47300 loss: 0.0031 lr: 0.005\n",
      "iteration: 47400 loss: 0.0028 lr: 0.005\n",
      "iteration: 47500 loss: 0.0035 lr: 0.005\n",
      "iteration: 47600 loss: 0.0035 lr: 0.005\n",
      "iteration: 47700 loss: 0.0031 lr: 0.005\n",
      "iteration: 47800 loss: 0.0034 lr: 0.005\n",
      "iteration: 47900 loss: 0.0031 lr: 0.005\n",
      "iteration: 48000 loss: 0.0030 lr: 0.005\n",
      "iteration: 48100 loss: 0.0032 lr: 0.005\n",
      "iteration: 48200 loss: 0.0042 lr: 0.005\n",
      "iteration: 48300 loss: 0.0033 lr: 0.005\n",
      "iteration: 48400 loss: 0.0029 lr: 0.005\n",
      "iteration: 48500 loss: 0.0032 lr: 0.005\n",
      "iteration: 48600 loss: 0.0026 lr: 0.005\n",
      "iteration: 48700 loss: 0.0030 lr: 0.005\n",
      "iteration: 48800 loss: 0.0033 lr: 0.005\n",
      "iteration: 48900 loss: 0.0038 lr: 0.005\n",
      "iteration: 49000 loss: 0.0032 lr: 0.005\n",
      "iteration: 49100 loss: 0.0035 lr: 0.005\n",
      "iteration: 49200 loss: 0.0035 lr: 0.005\n",
      "iteration: 49300 loss: 0.0038 lr: 0.005\n",
      "iteration: 49400 loss: 0.0025 lr: 0.005\n",
      "iteration: 49500 loss: 0.0034 lr: 0.005\n",
      "iteration: 49600 loss: 0.0030 lr: 0.005\n",
      "iteration: 49700 loss: 0.0036 lr: 0.005\n",
      "iteration: 49800 loss: 0.0032 lr: 0.005\n",
      "iteration: 49900 loss: 0.0031 lr: 0.005\n",
      "iteration: 50000 loss: 0.0033 lr: 0.005\n",
      "iteration: 50100 loss: 0.0031 lr: 0.005\n",
      "iteration: 50200 loss: 0.0031 lr: 0.005\n",
      "iteration: 50300 loss: 0.0030 lr: 0.005\n",
      "iteration: 50400 loss: 0.0031 lr: 0.005\n",
      "iteration: 50500 loss: 0.0036 lr: 0.005\n",
      "iteration: 50600 loss: 0.0030 lr: 0.005\n",
      "iteration: 50700 loss: 0.0037 lr: 0.005\n",
      "iteration: 50800 loss: 0.0034 lr: 0.005\n",
      "iteration: 50900 loss: 0.0026 lr: 0.005\n",
      "iteration: 51000 loss: 0.0032 lr: 0.005\n",
      "iteration: 51100 loss: 0.0036 lr: 0.005\n",
      "iteration: 51200 loss: 0.0031 lr: 0.005\n",
      "iteration: 51300 loss: 0.0039 lr: 0.005\n",
      "iteration: 51400 loss: 0.0034 lr: 0.005\n",
      "iteration: 51500 loss: 0.0032 lr: 0.005\n",
      "iteration: 51600 loss: 0.0034 lr: 0.005\n",
      "iteration: 51700 loss: 0.0030 lr: 0.005\n",
      "iteration: 51800 loss: 0.0033 lr: 0.005\n",
      "iteration: 51900 loss: 0.0028 lr: 0.005\n",
      "iteration: 52000 loss: 0.0032 lr: 0.005\n",
      "iteration: 52100 loss: 0.0033 lr: 0.005\n",
      "iteration: 52200 loss: 0.0028 lr: 0.005\n",
      "iteration: 52300 loss: 0.0031 lr: 0.005\n",
      "iteration: 52400 loss: 0.0029 lr: 0.005\n",
      "iteration: 52500 loss: 0.0035 lr: 0.005\n",
      "iteration: 52600 loss: 0.0038 lr: 0.005\n",
      "iteration: 52700 loss: 0.0034 lr: 0.005\n",
      "iteration: 52800 loss: 0.0031 lr: 0.005\n",
      "iteration: 52900 loss: 0.0031 lr: 0.005\n",
      "iteration: 53000 loss: 0.0030 lr: 0.005\n",
      "iteration: 53100 loss: 0.0026 lr: 0.005\n",
      "iteration: 53200 loss: 0.0031 lr: 0.005\n",
      "iteration: 53300 loss: 0.0032 lr: 0.005\n",
      "iteration: 53400 loss: 0.0026 lr: 0.005\n",
      "iteration: 53500 loss: 0.0032 lr: 0.005\n",
      "iteration: 53600 loss: 0.0035 lr: 0.005\n",
      "iteration: 53700 loss: 0.0032 lr: 0.005\n",
      "iteration: 53800 loss: 0.0028 lr: 0.005\n",
      "iteration: 53900 loss: 0.0035 lr: 0.005\n",
      "iteration: 54000 loss: 0.0034 lr: 0.005\n",
      "iteration: 54100 loss: 0.0029 lr: 0.005\n",
      "iteration: 54200 loss: 0.0029 lr: 0.005\n",
      "iteration: 54300 loss: 0.0033 lr: 0.005\n",
      "iteration: 54400 loss: 0.0033 lr: 0.005\n",
      "iteration: 54500 loss: 0.0034 lr: 0.005\n",
      "iteration: 54600 loss: 0.0034 lr: 0.005\n",
      "iteration: 54700 loss: 0.0030 lr: 0.005\n",
      "iteration: 54800 loss: 0.0031 lr: 0.005\n",
      "iteration: 54900 loss: 0.0034 lr: 0.005\n",
      "iteration: 55000 loss: 0.0029 lr: 0.005\n",
      "iteration: 55100 loss: 0.0031 lr: 0.005\n",
      "iteration: 55200 loss: 0.0034 lr: 0.005\n",
      "iteration: 55300 loss: 0.0033 lr: 0.005\n",
      "iteration: 55400 loss: 0.0029 lr: 0.005\n",
      "iteration: 55500 loss: 0.0029 lr: 0.005\n",
      "iteration: 55600 loss: 0.0031 lr: 0.005\n",
      "iteration: 55700 loss: 0.0030 lr: 0.005\n",
      "iteration: 55800 loss: 0.0031 lr: 0.005\n",
      "iteration: 55900 loss: 0.0028 lr: 0.005\n",
      "iteration: 56000 loss: 0.0037 lr: 0.005\n",
      "iteration: 56100 loss: 0.0035 lr: 0.005\n",
      "iteration: 56200 loss: 0.0030 lr: 0.005\n",
      "iteration: 56300 loss: 0.0033 lr: 0.005\n",
      "iteration: 56400 loss: 0.0029 lr: 0.005\n",
      "iteration: 56500 loss: 0.0032 lr: 0.005\n",
      "iteration: 56600 loss: 0.0030 lr: 0.005\n",
      "iteration: 56700 loss: 0.0034 lr: 0.005\n",
      "iteration: 56800 loss: 0.0035 lr: 0.005\n",
      "iteration: 56900 loss: 0.0033 lr: 0.005\n",
      "iteration: 57000 loss: 0.0042 lr: 0.005\n",
      "iteration: 57100 loss: 0.0032 lr: 0.005\n",
      "iteration: 57200 loss: 0.0031 lr: 0.005\n",
      "iteration: 57300 loss: 0.0037 lr: 0.005\n",
      "iteration: 57400 loss: 0.0036 lr: 0.005\n",
      "iteration: 57500 loss: 0.0034 lr: 0.005\n",
      "iteration: 57600 loss: 0.0030 lr: 0.005\n",
      "iteration: 57700 loss: 0.0028 lr: 0.005\n",
      "iteration: 57800 loss: 0.0030 lr: 0.005\n",
      "iteration: 57900 loss: 0.0026 lr: 0.005\n",
      "iteration: 58000 loss: 0.0030 lr: 0.005\n",
      "iteration: 58100 loss: 0.0034 lr: 0.005\n",
      "iteration: 58200 loss: 0.0032 lr: 0.005\n",
      "iteration: 58300 loss: 0.0030 lr: 0.005\n",
      "iteration: 58400 loss: 0.0031 lr: 0.005\n",
      "iteration: 58500 loss: 0.0033 lr: 0.005\n",
      "iteration: 58600 loss: 0.0028 lr: 0.005\n",
      "iteration: 58700 loss: 0.0029 lr: 0.005\n",
      "iteration: 58800 loss: 0.0027 lr: 0.005\n",
      "iteration: 58900 loss: 0.0032 lr: 0.005\n",
      "iteration: 59000 loss: 0.0033 lr: 0.005\n",
      "iteration: 59100 loss: 0.0034 lr: 0.005\n",
      "iteration: 59200 loss: 0.0028 lr: 0.005\n",
      "iteration: 59300 loss: 0.0035 lr: 0.005\n",
      "iteration: 59400 loss: 0.0033 lr: 0.005\n",
      "iteration: 59500 loss: 0.0033 lr: 0.005\n",
      "iteration: 59600 loss: 0.0030 lr: 0.005\n",
      "iteration: 59700 loss: 0.0031 lr: 0.005\n",
      "iteration: 59800 loss: 0.0027 lr: 0.005\n",
      "iteration: 59900 loss: 0.0028 lr: 0.005\n",
      "iteration: 60000 loss: 0.0030 lr: 0.005\n",
      "iteration: 60100 loss: 0.0029 lr: 0.005\n",
      "iteration: 60200 loss: 0.0028 lr: 0.005\n",
      "iteration: 60300 loss: 0.0030 lr: 0.005\n",
      "iteration: 60400 loss: 0.0029 lr: 0.005\n",
      "iteration: 60500 loss: 0.0027 lr: 0.005\n",
      "iteration: 60600 loss: 0.0030 lr: 0.005\n",
      "iteration: 60700 loss: 0.0031 lr: 0.005\n",
      "iteration: 60800 loss: 0.0037 lr: 0.005\n",
      "iteration: 60900 loss: 0.0030 lr: 0.005\n",
      "iteration: 61000 loss: 0.0027 lr: 0.005\n",
      "iteration: 61100 loss: 0.0032 lr: 0.005\n",
      "iteration: 61200 loss: 0.0037 lr: 0.005\n",
      "iteration: 61300 loss: 0.0031 lr: 0.005\n",
      "iteration: 61400 loss: 0.0028 lr: 0.005\n",
      "iteration: 61500 loss: 0.0028 lr: 0.005\n",
      "iteration: 61600 loss: 0.0029 lr: 0.005\n",
      "iteration: 61700 loss: 0.0030 lr: 0.005\n",
      "iteration: 61800 loss: 0.0031 lr: 0.005\n",
      "iteration: 61900 loss: 0.0028 lr: 0.005\n",
      "iteration: 62000 loss: 0.0030 lr: 0.005\n",
      "iteration: 62100 loss: 0.0028 lr: 0.005\n",
      "iteration: 62200 loss: 0.0030 lr: 0.005\n",
      "iteration: 62300 loss: 0.0028 lr: 0.005\n",
      "iteration: 62400 loss: 0.0038 lr: 0.005\n",
      "iteration: 62500 loss: 0.0032 lr: 0.005\n",
      "iteration: 62600 loss: 0.0039 lr: 0.005\n",
      "iteration: 62700 loss: 0.0028 lr: 0.005\n",
      "iteration: 62800 loss: 0.0030 lr: 0.005\n",
      "iteration: 62900 loss: 0.0028 lr: 0.005\n",
      "iteration: 63000 loss: 0.0031 lr: 0.005\n",
      "iteration: 63100 loss: 0.0032 lr: 0.005\n",
      "iteration: 63200 loss: 0.0033 lr: 0.005\n",
      "iteration: 63300 loss: 0.0030 lr: 0.005\n",
      "iteration: 63400 loss: 0.0031 lr: 0.005\n",
      "iteration: 63500 loss: 0.0033 lr: 0.005\n",
      "iteration: 63600 loss: 0.0029 lr: 0.005\n",
      "iteration: 63700 loss: 0.0032 lr: 0.005\n",
      "iteration: 63800 loss: 0.0024 lr: 0.005\n",
      "iteration: 63900 loss: 0.0028 lr: 0.005\n",
      "iteration: 64000 loss: 0.0030 lr: 0.005\n",
      "iteration: 64100 loss: 0.0030 lr: 0.005\n",
      "iteration: 64200 loss: 0.0036 lr: 0.005\n",
      "iteration: 64300 loss: 0.0033 lr: 0.005\n",
      "iteration: 64400 loss: 0.0027 lr: 0.005\n",
      "iteration: 64500 loss: 0.0033 lr: 0.005\n",
      "iteration: 64600 loss: 0.0033 lr: 0.005\n",
      "iteration: 64700 loss: 0.0025 lr: 0.005\n",
      "iteration: 64800 loss: 0.0031 lr: 0.005\n",
      "iteration: 64900 loss: 0.0031 lr: 0.005\n",
      "iteration: 65000 loss: 0.0027 lr: 0.005\n",
      "iteration: 65100 loss: 0.0034 lr: 0.005\n",
      "iteration: 65200 loss: 0.0028 lr: 0.005\n",
      "iteration: 65300 loss: 0.0031 lr: 0.005\n",
      "iteration: 65400 loss: 0.0026 lr: 0.005\n",
      "iteration: 65500 loss: 0.0032 lr: 0.005\n",
      "iteration: 65600 loss: 0.0033 lr: 0.005\n",
      "iteration: 65700 loss: 0.0025 lr: 0.005\n",
      "iteration: 65800 loss: 0.0028 lr: 0.005\n",
      "iteration: 65900 loss: 0.0034 lr: 0.005\n",
      "iteration: 66000 loss: 0.0032 lr: 0.005\n",
      "iteration: 66100 loss: 0.0029 lr: 0.005\n",
      "iteration: 66200 loss: 0.0030 lr: 0.005\n",
      "iteration: 66300 loss: 0.0028 lr: 0.005\n",
      "iteration: 66400 loss: 0.0032 lr: 0.005\n",
      "iteration: 66500 loss: 0.0033 lr: 0.005\n",
      "iteration: 66600 loss: 0.0036 lr: 0.005\n",
      "iteration: 66700 loss: 0.0032 lr: 0.005\n",
      "iteration: 66800 loss: 0.0028 lr: 0.005\n",
      "iteration: 66900 loss: 0.0033 lr: 0.005\n",
      "iteration: 67000 loss: 0.0029 lr: 0.005\n",
      "iteration: 67100 loss: 0.0029 lr: 0.005\n",
      "iteration: 67200 loss: 0.0028 lr: 0.005\n",
      "iteration: 67300 loss: 0.0027 lr: 0.005\n",
      "iteration: 67400 loss: 0.0036 lr: 0.005\n",
      "iteration: 67500 loss: 0.0032 lr: 0.005\n",
      "iteration: 67600 loss: 0.0031 lr: 0.005\n",
      "iteration: 67700 loss: 0.0032 lr: 0.005\n",
      "iteration: 67800 loss: 0.0026 lr: 0.005\n",
      "iteration: 67900 loss: 0.0028 lr: 0.005\n",
      "iteration: 68000 loss: 0.0028 lr: 0.005\n",
      "iteration: 68100 loss: 0.0029 lr: 0.005\n",
      "iteration: 68200 loss: 0.0029 lr: 0.005\n",
      "iteration: 68300 loss: 0.0029 lr: 0.005\n",
      "iteration: 68400 loss: 0.0028 lr: 0.005\n",
      "iteration: 68500 loss: 0.0025 lr: 0.005\n",
      "iteration: 68600 loss: 0.0030 lr: 0.005\n",
      "iteration: 68700 loss: 0.0030 lr: 0.005\n",
      "iteration: 68800 loss: 0.0026 lr: 0.005\n",
      "iteration: 68900 loss: 0.0030 lr: 0.005\n",
      "iteration: 69000 loss: 0.0026 lr: 0.005\n",
      "iteration: 69100 loss: 0.0031 lr: 0.005\n",
      "iteration: 69200 loss: 0.0029 lr: 0.005\n",
      "iteration: 69300 loss: 0.0029 lr: 0.005\n",
      "iteration: 69400 loss: 0.0029 lr: 0.005\n",
      "iteration: 69500 loss: 0.0027 lr: 0.005\n",
      "iteration: 69600 loss: 0.0031 lr: 0.005\n",
      "iteration: 69700 loss: 0.0028 lr: 0.005\n",
      "iteration: 69800 loss: 0.0031 lr: 0.005\n",
      "iteration: 69900 loss: 0.0027 lr: 0.005\n",
      "iteration: 70000 loss: 0.0023 lr: 0.005\n",
      "iteration: 70100 loss: 0.0035 lr: 0.005\n",
      "iteration: 70200 loss: 0.0026 lr: 0.005\n",
      "iteration: 70300 loss: 0.0031 lr: 0.005\n",
      "iteration: 70400 loss: 0.0031 lr: 0.005\n",
      "iteration: 70500 loss: 0.0032 lr: 0.005\n",
      "iteration: 70600 loss: 0.0025 lr: 0.005\n",
      "iteration: 70700 loss: 0.0030 lr: 0.005\n",
      "iteration: 70800 loss: 0.0029 lr: 0.005\n",
      "iteration: 70900 loss: 0.0029 lr: 0.005\n",
      "iteration: 71000 loss: 0.0034 lr: 0.005\n",
      "iteration: 71100 loss: 0.0030 lr: 0.005\n",
      "iteration: 71200 loss: 0.0030 lr: 0.005\n",
      "iteration: 71300 loss: 0.0027 lr: 0.005\n",
      "iteration: 71400 loss: 0.0025 lr: 0.005\n",
      "iteration: 71500 loss: 0.0033 lr: 0.005\n",
      "iteration: 71600 loss: 0.0028 lr: 0.005\n",
      "iteration: 71700 loss: 0.0027 lr: 0.005\n",
      "iteration: 71800 loss: 0.0030 lr: 0.005\n",
      "iteration: 71900 loss: 0.0034 lr: 0.005\n",
      "iteration: 72000 loss: 0.0027 lr: 0.005\n",
      "iteration: 72100 loss: 0.0027 lr: 0.005\n",
      "iteration: 72200 loss: 0.0026 lr: 0.005\n",
      "iteration: 72300 loss: 0.0023 lr: 0.005\n",
      "iteration: 72400 loss: 0.0025 lr: 0.005\n",
      "iteration: 72500 loss: 0.0025 lr: 0.005\n",
      "iteration: 72600 loss: 0.0032 lr: 0.005\n",
      "iteration: 72700 loss: 0.0029 lr: 0.005\n",
      "iteration: 72800 loss: 0.0026 lr: 0.005\n",
      "iteration: 72900 loss: 0.0035 lr: 0.005\n",
      "iteration: 73000 loss: 0.0035 lr: 0.005\n",
      "iteration: 73100 loss: 0.0031 lr: 0.005\n",
      "iteration: 73200 loss: 0.0030 lr: 0.005\n",
      "iteration: 73300 loss: 0.0029 lr: 0.005\n",
      "iteration: 73400 loss: 0.0025 lr: 0.005\n",
      "iteration: 73500 loss: 0.0028 lr: 0.005\n",
      "iteration: 73600 loss: 0.0030 lr: 0.005\n",
      "iteration: 73700 loss: 0.0027 lr: 0.005\n",
      "iteration: 73800 loss: 0.0034 lr: 0.005\n",
      "iteration: 73900 loss: 0.0025 lr: 0.005\n",
      "iteration: 74000 loss: 0.0028 lr: 0.005\n",
      "iteration: 74100 loss: 0.0024 lr: 0.005\n",
      "iteration: 74200 loss: 0.0029 lr: 0.005\n",
      "iteration: 74300 loss: 0.0026 lr: 0.005\n",
      "iteration: 74400 loss: 0.0029 lr: 0.005\n",
      "iteration: 74500 loss: 0.0026 lr: 0.005\n",
      "iteration: 74600 loss: 0.0032 lr: 0.005\n",
      "iteration: 74700 loss: 0.0025 lr: 0.005\n",
      "iteration: 74800 loss: 0.0029 lr: 0.005\n",
      "iteration: 74900 loss: 0.0033 lr: 0.005\n",
      "iteration: 75000 loss: 0.0029 lr: 0.005\n",
      "iteration: 75100 loss: 0.0028 lr: 0.005\n",
      "iteration: 75200 loss: 0.0026 lr: 0.005\n",
      "iteration: 75300 loss: 0.0028 lr: 0.005\n",
      "iteration: 75400 loss: 0.0029 lr: 0.005\n",
      "iteration: 75500 loss: 0.0028 lr: 0.005\n",
      "iteration: 75600 loss: 0.0030 lr: 0.005\n",
      "iteration: 75700 loss: 0.0029 lr: 0.005\n",
      "iteration: 75800 loss: 0.0031 lr: 0.005\n",
      "iteration: 75900 loss: 0.0027 lr: 0.005\n",
      "iteration: 76000 loss: 0.0027 lr: 0.005\n",
      "iteration: 76100 loss: 0.0025 lr: 0.005\n",
      "iteration: 76200 loss: 0.0033 lr: 0.005\n",
      "iteration: 76300 loss: 0.0031 lr: 0.005\n",
      "iteration: 76400 loss: 0.0026 lr: 0.005\n",
      "iteration: 76500 loss: 0.0033 lr: 0.005\n",
      "iteration: 76600 loss: 0.0030 lr: 0.005\n",
      "iteration: 76700 loss: 0.0033 lr: 0.005\n",
      "iteration: 76800 loss: 0.0029 lr: 0.005\n",
      "iteration: 76900 loss: 0.0030 lr: 0.005\n",
      "iteration: 77000 loss: 0.0031 lr: 0.005\n",
      "iteration: 77100 loss: 0.0033 lr: 0.005\n",
      "iteration: 77200 loss: 0.0031 lr: 0.005\n",
      "iteration: 77300 loss: 0.0027 lr: 0.005\n",
      "iteration: 77400 loss: 0.0030 lr: 0.005\n",
      "iteration: 77500 loss: 0.0030 lr: 0.005\n",
      "iteration: 77600 loss: 0.0026 lr: 0.005\n",
      "iteration: 77700 loss: 0.0029 lr: 0.005\n",
      "iteration: 77800 loss: 0.0030 lr: 0.005\n",
      "iteration: 77900 loss: 0.0029 lr: 0.005\n",
      "iteration: 78000 loss: 0.0026 lr: 0.005\n",
      "iteration: 78100 loss: 0.0031 lr: 0.005\n",
      "iteration: 78200 loss: 0.0025 lr: 0.005\n",
      "iteration: 78300 loss: 0.0031 lr: 0.005\n",
      "iteration: 78400 loss: 0.0032 lr: 0.005\n",
      "iteration: 78500 loss: 0.0031 lr: 0.005\n",
      "iteration: 78600 loss: 0.0027 lr: 0.005\n",
      "iteration: 78700 loss: 0.0029 lr: 0.005\n",
      "iteration: 78800 loss: 0.0024 lr: 0.005\n",
      "iteration: 78900 loss: 0.0029 lr: 0.005\n",
      "iteration: 79000 loss: 0.0026 lr: 0.005\n",
      "iteration: 79100 loss: 0.0025 lr: 0.005\n",
      "iteration: 79200 loss: 0.0029 lr: 0.005\n",
      "iteration: 79300 loss: 0.0026 lr: 0.005\n",
      "iteration: 79400 loss: 0.0031 lr: 0.005\n",
      "iteration: 79500 loss: 0.0025 lr: 0.005\n",
      "iteration: 79600 loss: 0.0033 lr: 0.005\n",
      "iteration: 79700 loss: 0.0030 lr: 0.005\n",
      "iteration: 79800 loss: 0.0031 lr: 0.005\n",
      "iteration: 79900 loss: 0.0030 lr: 0.005\n",
      "iteration: 80000 loss: 0.0029 lr: 0.005\n",
      "iteration: 80100 loss: 0.0031 lr: 0.005\n",
      "iteration: 80200 loss: 0.0029 lr: 0.005\n",
      "iteration: 80300 loss: 0.0032 lr: 0.005\n",
      "iteration: 80400 loss: 0.0026 lr: 0.005\n",
      "iteration: 80500 loss: 0.0027 lr: 0.005\n",
      "iteration: 80600 loss: 0.0025 lr: 0.005\n",
      "iteration: 80700 loss: 0.0029 lr: 0.005\n",
      "iteration: 80800 loss: 0.0031 lr: 0.005\n",
      "iteration: 80900 loss: 0.0028 lr: 0.005\n",
      "iteration: 81000 loss: 0.0028 lr: 0.005\n",
      "iteration: 81100 loss: 0.0028 lr: 0.005\n",
      "iteration: 81200 loss: 0.0026 lr: 0.005\n",
      "iteration: 81300 loss: 0.0030 lr: 0.005\n",
      "iteration: 81400 loss: 0.0027 lr: 0.005\n",
      "iteration: 81500 loss: 0.0029 lr: 0.005\n",
      "iteration: 81600 loss: 0.0030 lr: 0.005\n",
      "iteration: 81700 loss: 0.0024 lr: 0.005\n",
      "iteration: 81800 loss: 0.0029 lr: 0.005\n",
      "iteration: 81900 loss: 0.0030 lr: 0.005\n",
      "iteration: 82000 loss: 0.0027 lr: 0.005\n",
      "iteration: 82100 loss: 0.0031 lr: 0.005\n",
      "iteration: 82200 loss: 0.0031 lr: 0.005\n",
      "iteration: 82300 loss: 0.0031 lr: 0.005\n",
      "iteration: 82400 loss: 0.0027 lr: 0.005\n",
      "iteration: 82500 loss: 0.0033 lr: 0.005\n",
      "iteration: 82600 loss: 0.0027 lr: 0.005\n",
      "iteration: 82700 loss: 0.0033 lr: 0.005\n",
      "iteration: 82800 loss: 0.0027 lr: 0.005\n",
      "iteration: 82900 loss: 0.0027 lr: 0.005\n",
      "iteration: 83000 loss: 0.0028 lr: 0.005\n",
      "iteration: 83100 loss: 0.0028 lr: 0.005\n",
      "iteration: 83200 loss: 0.0030 lr: 0.005\n",
      "iteration: 83300 loss: 0.0028 lr: 0.005\n",
      "iteration: 83400 loss: 0.0029 lr: 0.005\n",
      "iteration: 83500 loss: 0.0029 lr: 0.005\n",
      "iteration: 83600 loss: 0.0029 lr: 0.005\n",
      "iteration: 83700 loss: 0.0025 lr: 0.005\n",
      "iteration: 83800 loss: 0.0027 lr: 0.005\n",
      "iteration: 83900 loss: 0.0025 lr: 0.005\n",
      "iteration: 84000 loss: 0.0024 lr: 0.005\n",
      "iteration: 84100 loss: 0.0030 lr: 0.005\n",
      "iteration: 84200 loss: 0.0028 lr: 0.005\n",
      "iteration: 84300 loss: 0.0026 lr: 0.005\n",
      "iteration: 84400 loss: 0.0033 lr: 0.005\n",
      "iteration: 84500 loss: 0.0027 lr: 0.005\n",
      "iteration: 84600 loss: 0.0023 lr: 0.005\n",
      "iteration: 84700 loss: 0.0029 lr: 0.005\n",
      "iteration: 84800 loss: 0.0029 lr: 0.005\n",
      "iteration: 84900 loss: 0.0028 lr: 0.005\n",
      "iteration: 85000 loss: 0.0029 lr: 0.005\n",
      "iteration: 85100 loss: 0.0026 lr: 0.005\n",
      "iteration: 85200 loss: 0.0030 lr: 0.005\n",
      "iteration: 85300 loss: 0.0026 lr: 0.005\n",
      "iteration: 85400 loss: 0.0027 lr: 0.005\n",
      "iteration: 85500 loss: 0.0029 lr: 0.005\n",
      "iteration: 85600 loss: 0.0028 lr: 0.005\n",
      "iteration: 85700 loss: 0.0025 lr: 0.005\n",
      "iteration: 85800 loss: 0.0024 lr: 0.005\n",
      "iteration: 85900 loss: 0.0025 lr: 0.005\n",
      "iteration: 86000 loss: 0.0035 lr: 0.005\n",
      "iteration: 86100 loss: 0.0025 lr: 0.005\n",
      "iteration: 86200 loss: 0.0025 lr: 0.005\n",
      "iteration: 86300 loss: 0.0027 lr: 0.005\n",
      "iteration: 86400 loss: 0.0026 lr: 0.005\n",
      "iteration: 86500 loss: 0.0024 lr: 0.005\n",
      "iteration: 86600 loss: 0.0030 lr: 0.005\n",
      "iteration: 86700 loss: 0.0027 lr: 0.005\n",
      "iteration: 86800 loss: 0.0025 lr: 0.005\n",
      "iteration: 86900 loss: 0.0025 lr: 0.005\n",
      "iteration: 87000 loss: 0.0031 lr: 0.005\n",
      "iteration: 87100 loss: 0.0026 lr: 0.005\n",
      "iteration: 87200 loss: 0.0025 lr: 0.005\n",
      "iteration: 87300 loss: 0.0027 lr: 0.005\n",
      "iteration: 87400 loss: 0.0028 lr: 0.005\n",
      "iteration: 87500 loss: 0.0026 lr: 0.005\n",
      "iteration: 87600 loss: 0.0023 lr: 0.005\n",
      "iteration: 87700 loss: 0.0026 lr: 0.005\n",
      "iteration: 87800 loss: 0.0026 lr: 0.005\n",
      "iteration: 87900 loss: 0.0026 lr: 0.005\n",
      "iteration: 88000 loss: 0.0028 lr: 0.005\n",
      "iteration: 88100 loss: 0.0028 lr: 0.005\n",
      "iteration: 88200 loss: 0.0026 lr: 0.005\n",
      "iteration: 88300 loss: 0.0029 lr: 0.005\n",
      "iteration: 88400 loss: 0.0027 lr: 0.005\n",
      "iteration: 88500 loss: 0.0031 lr: 0.005\n",
      "iteration: 88600 loss: 0.0024 lr: 0.005\n",
      "iteration: 88700 loss: 0.0027 lr: 0.005\n",
      "iteration: 88800 loss: 0.0030 lr: 0.005\n",
      "iteration: 88900 loss: 0.0026 lr: 0.005\n",
      "iteration: 89000 loss: 0.0026 lr: 0.005\n",
      "iteration: 89100 loss: 0.0028 lr: 0.005\n",
      "iteration: 89200 loss: 0.0025 lr: 0.005\n",
      "iteration: 89300 loss: 0.0027 lr: 0.005\n",
      "iteration: 89400 loss: 0.0027 lr: 0.005\n",
      "iteration: 89500 loss: 0.0027 lr: 0.005\n",
      "iteration: 89600 loss: 0.0025 lr: 0.005\n",
      "iteration: 89700 loss: 0.0029 lr: 0.005\n",
      "iteration: 89800 loss: 0.0031 lr: 0.005\n",
      "iteration: 89900 loss: 0.0031 lr: 0.005\n",
      "iteration: 90000 loss: 0.0024 lr: 0.005\n",
      "iteration: 90100 loss: 0.0026 lr: 0.005\n",
      "iteration: 90200 loss: 0.0028 lr: 0.005\n",
      "iteration: 90300 loss: 0.0028 lr: 0.005\n",
      "iteration: 90400 loss: 0.0025 lr: 0.005\n",
      "iteration: 90500 loss: 0.0028 lr: 0.005\n",
      "iteration: 90600 loss: 0.0028 lr: 0.005\n",
      "iteration: 90700 loss: 0.0026 lr: 0.005\n",
      "iteration: 90800 loss: 0.0032 lr: 0.005\n",
      "iteration: 90900 loss: 0.0026 lr: 0.005\n",
      "iteration: 91000 loss: 0.0028 lr: 0.005\n",
      "iteration: 91100 loss: 0.0026 lr: 0.005\n",
      "iteration: 91200 loss: 0.0030 lr: 0.005\n",
      "iteration: 91300 loss: 0.0026 lr: 0.005\n",
      "iteration: 91400 loss: 0.0026 lr: 0.005\n",
      "iteration: 91500 loss: 0.0023 lr: 0.005\n",
      "iteration: 91600 loss: 0.0027 lr: 0.005\n",
      "iteration: 91700 loss: 0.0029 lr: 0.005\n",
      "iteration: 91800 loss: 0.0025 lr: 0.005\n",
      "iteration: 91900 loss: 0.0032 lr: 0.005\n",
      "iteration: 92000 loss: 0.0026 lr: 0.005\n",
      "iteration: 92100 loss: 0.0027 lr: 0.005\n",
      "iteration: 92200 loss: 0.0026 lr: 0.005\n",
      "iteration: 92300 loss: 0.0027 lr: 0.005\n",
      "iteration: 92400 loss: 0.0024 lr: 0.005\n",
      "iteration: 92500 loss: 0.0024 lr: 0.005\n",
      "iteration: 92600 loss: 0.0026 lr: 0.005\n",
      "iteration: 92700 loss: 0.0026 lr: 0.005\n",
      "iteration: 92800 loss: 0.0029 lr: 0.005\n",
      "iteration: 92900 loss: 0.0024 lr: 0.005\n",
      "iteration: 93000 loss: 0.0024 lr: 0.005\n",
      "iteration: 93100 loss: 0.0024 lr: 0.005\n",
      "iteration: 93200 loss: 0.0024 lr: 0.005\n",
      "iteration: 93300 loss: 0.0023 lr: 0.005\n",
      "iteration: 93400 loss: 0.0031 lr: 0.005\n",
      "iteration: 93500 loss: 0.0023 lr: 0.005\n",
      "iteration: 93600 loss: 0.0025 lr: 0.005\n",
      "iteration: 93700 loss: 0.0027 lr: 0.005\n",
      "iteration: 93800 loss: 0.0036 lr: 0.005\n",
      "iteration: 93900 loss: 0.0024 lr: 0.005\n",
      "iteration: 94000 loss: 0.0024 lr: 0.005\n",
      "iteration: 94100 loss: 0.0028 lr: 0.005\n",
      "iteration: 94200 loss: 0.0023 lr: 0.005\n",
      "iteration: 94300 loss: 0.0028 lr: 0.005\n",
      "iteration: 94400 loss: 0.0027 lr: 0.005\n",
      "iteration: 94500 loss: 0.0027 lr: 0.005\n",
      "iteration: 94600 loss: 0.0026 lr: 0.005\n",
      "iteration: 94700 loss: 0.0027 lr: 0.005\n",
      "iteration: 94800 loss: 0.0028 lr: 0.005\n",
      "iteration: 94900 loss: 0.0028 lr: 0.005\n",
      "iteration: 95000 loss: 0.0031 lr: 0.005\n",
      "iteration: 95100 loss: 0.0026 lr: 0.005\n",
      "iteration: 95200 loss: 0.0028 lr: 0.005\n",
      "iteration: 95300 loss: 0.0030 lr: 0.005\n",
      "iteration: 95400 loss: 0.0023 lr: 0.005\n",
      "iteration: 95500 loss: 0.0025 lr: 0.005\n",
      "iteration: 95600 loss: 0.0028 lr: 0.005\n",
      "iteration: 95700 loss: 0.0030 lr: 0.005\n",
      "iteration: 95800 loss: 0.0023 lr: 0.005\n",
      "iteration: 95900 loss: 0.0024 lr: 0.005\n",
      "iteration: 96000 loss: 0.0024 lr: 0.005\n",
      "iteration: 96100 loss: 0.0023 lr: 0.005\n",
      "iteration: 96200 loss: 0.0025 lr: 0.005\n",
      "iteration: 96300 loss: 0.0026 lr: 0.005\n",
      "iteration: 96400 loss: 0.0025 lr: 0.005\n",
      "iteration: 96500 loss: 0.0026 lr: 0.005\n",
      "iteration: 96600 loss: 0.0031 lr: 0.005\n",
      "iteration: 96700 loss: 0.0027 lr: 0.005\n",
      "iteration: 96800 loss: 0.0029 lr: 0.005\n",
      "iteration: 96900 loss: 0.0028 lr: 0.005\n",
      "iteration: 97000 loss: 0.0023 lr: 0.005\n",
      "iteration: 97100 loss: 0.0027 lr: 0.005\n",
      "iteration: 97200 loss: 0.0027 lr: 0.005\n",
      "iteration: 97300 loss: 0.0023 lr: 0.005\n",
      "iteration: 97400 loss: 0.0025 lr: 0.005\n",
      "iteration: 97500 loss: 0.0030 lr: 0.005\n",
      "iteration: 97600 loss: 0.0025 lr: 0.005\n",
      "iteration: 97700 loss: 0.0025 lr: 0.005\n",
      "iteration: 97800 loss: 0.0026 lr: 0.005\n",
      "iteration: 97900 loss: 0.0029 lr: 0.005\n",
      "iteration: 98000 loss: 0.0023 lr: 0.005\n",
      "iteration: 98100 loss: 0.0026 lr: 0.005\n",
      "iteration: 98200 loss: 0.0027 lr: 0.005\n",
      "iteration: 98300 loss: 0.0030 lr: 0.005\n",
      "iteration: 98400 loss: 0.0026 lr: 0.005\n",
      "iteration: 98500 loss: 0.0025 lr: 0.005\n",
      "iteration: 98600 loss: 0.0028 lr: 0.005\n",
      "iteration: 98700 loss: 0.0029 lr: 0.005\n",
      "iteration: 98800 loss: 0.0024 lr: 0.005\n",
      "iteration: 98900 loss: 0.0024 lr: 0.005\n",
      "iteration: 99000 loss: 0.0028 lr: 0.005\n",
      "iteration: 99100 loss: 0.0025 lr: 0.005\n",
      "iteration: 99200 loss: 0.0025 lr: 0.005\n",
      "iteration: 99300 loss: 0.0024 lr: 0.005\n",
      "iteration: 99400 loss: 0.0023 lr: 0.005\n",
      "iteration: 99500 loss: 0.0024 lr: 0.005\n",
      "iteration: 99600 loss: 0.0030 lr: 0.005\n",
      "iteration: 99700 loss: 0.0027 lr: 0.005\n",
      "iteration: 99800 loss: 0.0024 lr: 0.005\n",
      "iteration: 99900 loss: 0.0025 lr: 0.005\n",
      "iteration: 100000 loss: 0.0023 lr: 0.005\n",
      "iteration: 100100 loss: 0.0023 lr: 0.005\n",
      "iteration: 100200 loss: 0.0026 lr: 0.005\n",
      "iteration: 100300 loss: 0.0024 lr: 0.005\n",
      "iteration: 100400 loss: 0.0026 lr: 0.005\n",
      "iteration: 100500 loss: 0.0024 lr: 0.005\n",
      "iteration: 100600 loss: 0.0029 lr: 0.005\n",
      "iteration: 100700 loss: 0.0023 lr: 0.005\n",
      "iteration: 100800 loss: 0.0028 lr: 0.005\n",
      "iteration: 100900 loss: 0.0028 lr: 0.005\n",
      "iteration: 101000 loss: 0.0030 lr: 0.005\n",
      "iteration: 101100 loss: 0.0028 lr: 0.005\n",
      "iteration: 101200 loss: 0.0028 lr: 0.005\n",
      "iteration: 101300 loss: 0.0025 lr: 0.005\n",
      "iteration: 101400 loss: 0.0028 lr: 0.005\n",
      "iteration: 101500 loss: 0.0026 lr: 0.005\n",
      "iteration: 101600 loss: 0.0026 lr: 0.005\n",
      "iteration: 101700 loss: 0.0027 lr: 0.005\n",
      "iteration: 101800 loss: 0.0023 lr: 0.005\n",
      "iteration: 101900 loss: 0.0029 lr: 0.005\n",
      "iteration: 102000 loss: 0.0025 lr: 0.005\n",
      "iteration: 102100 loss: 0.0023 lr: 0.005\n",
      "iteration: 102200 loss: 0.0020 lr: 0.005\n",
      "iteration: 102300 loss: 0.0025 lr: 0.005\n",
      "iteration: 102400 loss: 0.0029 lr: 0.005\n",
      "iteration: 102500 loss: 0.0027 lr: 0.005\n",
      "iteration: 102600 loss: 0.0025 lr: 0.005\n",
      "iteration: 102700 loss: 0.0028 lr: 0.005\n",
      "iteration: 102800 loss: 0.0022 lr: 0.005\n",
      "iteration: 102900 loss: 0.0022 lr: 0.005\n",
      "iteration: 103000 loss: 0.0026 lr: 0.005\n",
      "iteration: 103100 loss: 0.0030 lr: 0.005\n",
      "iteration: 103200 loss: 0.0026 lr: 0.005\n",
      "iteration: 103300 loss: 0.0025 lr: 0.005\n",
      "iteration: 103400 loss: 0.0028 lr: 0.005\n",
      "iteration: 103500 loss: 0.0019 lr: 0.005\n",
      "iteration: 103600 loss: 0.0024 lr: 0.005\n",
      "iteration: 103700 loss: 0.0029 lr: 0.005\n",
      "iteration: 103800 loss: 0.0024 lr: 0.005\n",
      "iteration: 103900 loss: 0.0029 lr: 0.005\n",
      "iteration: 104000 loss: 0.0025 lr: 0.005\n",
      "iteration: 104100 loss: 0.0025 lr: 0.005\n",
      "iteration: 104200 loss: 0.0022 lr: 0.005\n",
      "iteration: 104300 loss: 0.0025 lr: 0.005\n",
      "iteration: 104400 loss: 0.0024 lr: 0.005\n",
      "iteration: 104500 loss: 0.0026 lr: 0.005\n",
      "iteration: 104600 loss: 0.0025 lr: 0.005\n",
      "iteration: 104700 loss: 0.0025 lr: 0.005\n",
      "iteration: 104800 loss: 0.0022 lr: 0.005\n",
      "iteration: 104900 loss: 0.0024 lr: 0.005\n",
      "iteration: 105000 loss: 0.0023 lr: 0.005\n",
      "iteration: 105100 loss: 0.0024 lr: 0.005\n",
      "iteration: 105200 loss: 0.0024 lr: 0.005\n",
      "iteration: 105300 loss: 0.0028 lr: 0.005\n",
      "iteration: 105400 loss: 0.0026 lr: 0.005\n",
      "iteration: 105500 loss: 0.0028 lr: 0.005\n",
      "iteration: 105600 loss: 0.0023 lr: 0.005\n",
      "iteration: 105700 loss: 0.0033 lr: 0.005\n",
      "iteration: 105800 loss: 0.0027 lr: 0.005\n",
      "iteration: 105900 loss: 0.0022 lr: 0.005\n",
      "iteration: 106000 loss: 0.0029 lr: 0.005\n",
      "iteration: 106100 loss: 0.0026 lr: 0.005\n",
      "iteration: 106200 loss: 0.0026 lr: 0.005\n",
      "iteration: 106300 loss: 0.0023 lr: 0.005\n",
      "iteration: 106400 loss: 0.0026 lr: 0.005\n",
      "iteration: 106500 loss: 0.0026 lr: 0.005\n",
      "iteration: 106600 loss: 0.0024 lr: 0.005\n",
      "iteration: 106700 loss: 0.0026 lr: 0.005\n",
      "iteration: 106800 loss: 0.0025 lr: 0.005\n",
      "iteration: 106900 loss: 0.0027 lr: 0.005\n",
      "iteration: 107000 loss: 0.0024 lr: 0.005\n",
      "iteration: 107100 loss: 0.0024 lr: 0.005\n",
      "iteration: 107200 loss: 0.0026 lr: 0.005\n",
      "iteration: 107300 loss: 0.0025 lr: 0.005\n",
      "iteration: 107400 loss: 0.0027 lr: 0.005\n",
      "iteration: 107500 loss: 0.0026 lr: 0.005\n",
      "iteration: 107600 loss: 0.0026 lr: 0.005\n",
      "iteration: 107700 loss: 0.0022 lr: 0.005\n",
      "iteration: 107800 loss: 0.0023 lr: 0.005\n",
      "iteration: 107900 loss: 0.0026 lr: 0.005\n",
      "iteration: 108000 loss: 0.0030 lr: 0.005\n",
      "iteration: 108100 loss: 0.0029 lr: 0.005\n",
      "iteration: 108200 loss: 0.0026 lr: 0.005\n",
      "iteration: 108300 loss: 0.0025 lr: 0.005\n",
      "iteration: 108400 loss: 0.0025 lr: 0.005\n",
      "iteration: 108500 loss: 0.0026 lr: 0.005\n",
      "iteration: 108600 loss: 0.0025 lr: 0.005\n",
      "iteration: 108700 loss: 0.0027 lr: 0.005\n",
      "iteration: 108800 loss: 0.0027 lr: 0.005\n",
      "iteration: 108900 loss: 0.0025 lr: 0.005\n",
      "iteration: 109000 loss: 0.0026 lr: 0.005\n",
      "iteration: 109100 loss: 0.0032 lr: 0.005\n",
      "iteration: 109200 loss: 0.0021 lr: 0.005\n",
      "iteration: 109300 loss: 0.0023 lr: 0.005\n",
      "iteration: 109400 loss: 0.0026 lr: 0.005\n",
      "iteration: 109500 loss: 0.0024 lr: 0.005\n",
      "iteration: 109600 loss: 0.0020 lr: 0.005\n",
      "iteration: 109700 loss: 0.0029 lr: 0.005\n",
      "iteration: 109800 loss: 0.0029 lr: 0.005\n",
      "iteration: 109900 loss: 0.0024 lr: 0.005\n",
      "iteration: 110000 loss: 0.0024 lr: 0.005\n",
      "iteration: 110100 loss: 0.0026 lr: 0.005\n",
      "iteration: 110200 loss: 0.0025 lr: 0.005\n",
      "iteration: 110300 loss: 0.0029 lr: 0.005\n",
      "iteration: 110400 loss: 0.0023 lr: 0.005\n",
      "iteration: 110500 loss: 0.0026 lr: 0.005\n",
      "iteration: 110600 loss: 0.0025 lr: 0.005\n",
      "iteration: 110700 loss: 0.0029 lr: 0.005\n",
      "iteration: 110800 loss: 0.0026 lr: 0.005\n",
      "iteration: 110900 loss: 0.0027 lr: 0.005\n",
      "iteration: 111000 loss: 0.0029 lr: 0.005\n",
      "iteration: 111100 loss: 0.0024 lr: 0.005\n",
      "iteration: 111200 loss: 0.0024 lr: 0.005\n",
      "iteration: 111300 loss: 0.0024 lr: 0.005\n",
      "iteration: 111400 loss: 0.0024 lr: 0.005\n",
      "iteration: 111500 loss: 0.0025 lr: 0.005\n",
      "iteration: 111600 loss: 0.0027 lr: 0.005\n",
      "iteration: 111700 loss: 0.0023 lr: 0.005\n",
      "iteration: 111800 loss: 0.0026 lr: 0.005\n",
      "iteration: 111900 loss: 0.0022 lr: 0.005\n",
      "iteration: 112000 loss: 0.0027 lr: 0.005\n",
      "iteration: 112100 loss: 0.0023 lr: 0.005\n",
      "iteration: 112200 loss: 0.0026 lr: 0.005\n",
      "iteration: 112300 loss: 0.0027 lr: 0.005\n",
      "iteration: 112400 loss: 0.0026 lr: 0.005\n",
      "iteration: 112500 loss: 0.0023 lr: 0.005\n",
      "iteration: 112600 loss: 0.0023 lr: 0.005\n",
      "iteration: 112700 loss: 0.0022 lr: 0.005\n",
      "iteration: 112800 loss: 0.0022 lr: 0.005\n",
      "iteration: 112900 loss: 0.0023 lr: 0.005\n",
      "iteration: 113000 loss: 0.0023 lr: 0.005\n",
      "iteration: 113100 loss: 0.0024 lr: 0.005\n",
      "iteration: 113200 loss: 0.0028 lr: 0.005\n",
      "iteration: 113300 loss: 0.0023 lr: 0.005\n",
      "iteration: 113400 loss: 0.0025 lr: 0.005\n",
      "iteration: 113500 loss: 0.0027 lr: 0.005\n",
      "iteration: 113600 loss: 0.0026 lr: 0.005\n",
      "iteration: 113700 loss: 0.0032 lr: 0.005\n",
      "iteration: 113800 loss: 0.0024 lr: 0.005\n",
      "iteration: 113900 loss: 0.0025 lr: 0.005\n",
      "iteration: 114000 loss: 0.0021 lr: 0.005\n",
      "iteration: 114100 loss: 0.0025 lr: 0.005\n",
      "iteration: 114200 loss: 0.0022 lr: 0.005\n",
      "iteration: 114300 loss: 0.0028 lr: 0.005\n",
      "iteration: 114400 loss: 0.0023 lr: 0.005\n",
      "iteration: 114500 loss: 0.0023 lr: 0.005\n",
      "iteration: 114600 loss: 0.0025 lr: 0.005\n",
      "iteration: 114700 loss: 0.0023 lr: 0.005\n",
      "iteration: 114800 loss: 0.0024 lr: 0.005\n",
      "iteration: 114900 loss: 0.0026 lr: 0.005\n",
      "iteration: 115000 loss: 0.0031 lr: 0.005\n",
      "iteration: 115100 loss: 0.0025 lr: 0.005\n",
      "iteration: 115200 loss: 0.0025 lr: 0.005\n",
      "iteration: 115300 loss: 0.0021 lr: 0.005\n",
      "iteration: 115400 loss: 0.0024 lr: 0.005\n",
      "iteration: 115500 loss: 0.0024 lr: 0.005\n",
      "iteration: 115600 loss: 0.0026 lr: 0.005\n",
      "iteration: 115700 loss: 0.0026 lr: 0.005\n",
      "iteration: 115800 loss: 0.0021 lr: 0.005\n",
      "iteration: 115900 loss: 0.0024 lr: 0.005\n",
      "iteration: 116000 loss: 0.0025 lr: 0.005\n",
      "iteration: 116100 loss: 0.0025 lr: 0.005\n",
      "iteration: 116200 loss: 0.0030 lr: 0.005\n",
      "iteration: 116300 loss: 0.0024 lr: 0.005\n",
      "iteration: 116400 loss: 0.0028 lr: 0.005\n",
      "iteration: 116500 loss: 0.0026 lr: 0.005\n",
      "iteration: 116600 loss: 0.0024 lr: 0.005\n",
      "iteration: 116700 loss: 0.0024 lr: 0.005\n",
      "iteration: 116800 loss: 0.0027 lr: 0.005\n",
      "iteration: 116900 loss: 0.0024 lr: 0.005\n",
      "iteration: 117000 loss: 0.0026 lr: 0.005\n",
      "iteration: 117100 loss: 0.0031 lr: 0.005\n",
      "iteration: 117200 loss: 0.0025 lr: 0.005\n",
      "iteration: 117300 loss: 0.0022 lr: 0.005\n",
      "iteration: 117400 loss: 0.0024 lr: 0.005\n",
      "iteration: 117500 loss: 0.0022 lr: 0.005\n",
      "iteration: 117600 loss: 0.0023 lr: 0.005\n",
      "iteration: 117700 loss: 0.0025 lr: 0.005\n",
      "iteration: 117800 loss: 0.0024 lr: 0.005\n",
      "iteration: 117900 loss: 0.0023 lr: 0.005\n",
      "iteration: 118000 loss: 0.0027 lr: 0.005\n",
      "iteration: 118100 loss: 0.0027 lr: 0.005\n",
      "iteration: 118200 loss: 0.0024 lr: 0.005\n",
      "iteration: 118300 loss: 0.0030 lr: 0.005\n",
      "iteration: 118400 loss: 0.0023 lr: 0.005\n",
      "iteration: 118500 loss: 0.0023 lr: 0.005\n",
      "iteration: 118600 loss: 0.0028 lr: 0.005\n",
      "iteration: 118700 loss: 0.0023 lr: 0.005\n",
      "iteration: 118800 loss: 0.0022 lr: 0.005\n",
      "iteration: 118900 loss: 0.0021 lr: 0.005\n",
      "iteration: 119000 loss: 0.0023 lr: 0.005\n",
      "iteration: 119100 loss: 0.0021 lr: 0.005\n",
      "iteration: 119200 loss: 0.0021 lr: 0.005\n",
      "iteration: 119300 loss: 0.0022 lr: 0.005\n",
      "iteration: 119400 loss: 0.0029 lr: 0.005\n",
      "iteration: 119500 loss: 0.0024 lr: 0.005\n",
      "iteration: 119600 loss: 0.0028 lr: 0.005\n",
      "iteration: 119700 loss: 0.0022 lr: 0.005\n",
      "iteration: 119800 loss: 0.0027 lr: 0.005\n",
      "iteration: 119900 loss: 0.0023 lr: 0.005\n",
      "iteration: 120000 loss: 0.0026 lr: 0.005\n",
      "iteration: 120100 loss: 0.0027 lr: 0.005\n",
      "iteration: 120200 loss: 0.0028 lr: 0.005\n",
      "iteration: 120300 loss: 0.0031 lr: 0.005\n",
      "iteration: 120400 loss: 0.0023 lr: 0.005\n",
      "iteration: 120500 loss: 0.0021 lr: 0.005\n",
      "iteration: 120600 loss: 0.0028 lr: 0.005\n",
      "iteration: 120700 loss: 0.0030 lr: 0.005\n",
      "iteration: 120800 loss: 0.0022 lr: 0.005\n",
      "iteration: 120900 loss: 0.0025 lr: 0.005\n",
      "iteration: 121000 loss: 0.0028 lr: 0.005\n",
      "iteration: 121100 loss: 0.0029 lr: 0.005\n",
      "iteration: 121200 loss: 0.0028 lr: 0.005\n",
      "iteration: 121300 loss: 0.0023 lr: 0.005\n",
      "iteration: 121400 loss: 0.0027 lr: 0.005\n",
      "iteration: 121500 loss: 0.0025 lr: 0.005\n",
      "iteration: 121600 loss: 0.0022 lr: 0.005\n",
      "iteration: 121700 loss: 0.0024 lr: 0.005\n",
      "iteration: 121800 loss: 0.0024 lr: 0.005\n",
      "iteration: 121900 loss: 0.0023 lr: 0.005\n",
      "iteration: 122000 loss: 0.0027 lr: 0.005\n",
      "iteration: 122100 loss: 0.0022 lr: 0.005\n",
      "iteration: 122200 loss: 0.0022 lr: 0.005\n",
      "iteration: 122300 loss: 0.0023 lr: 0.005\n",
      "iteration: 122400 loss: 0.0026 lr: 0.005\n",
      "iteration: 122500 loss: 0.0026 lr: 0.005\n",
      "iteration: 122600 loss: 0.0025 lr: 0.005\n",
      "iteration: 122700 loss: 0.0027 lr: 0.005\n",
      "iteration: 122800 loss: 0.0024 lr: 0.005\n",
      "iteration: 122900 loss: 0.0026 lr: 0.005\n",
      "iteration: 123000 loss: 0.0022 lr: 0.005\n",
      "iteration: 123100 loss: 0.0030 lr: 0.005\n",
      "iteration: 123200 loss: 0.0020 lr: 0.005\n",
      "iteration: 123300 loss: 0.0022 lr: 0.005\n",
      "iteration: 123400 loss: 0.0025 lr: 0.005\n",
      "iteration: 123500 loss: 0.0023 lr: 0.005\n",
      "iteration: 123600 loss: 0.0027 lr: 0.005\n",
      "iteration: 123700 loss: 0.0026 lr: 0.005\n",
      "iteration: 123800 loss: 0.0021 lr: 0.005\n",
      "iteration: 123900 loss: 0.0022 lr: 0.005\n",
      "iteration: 124000 loss: 0.0023 lr: 0.005\n",
      "iteration: 124100 loss: 0.0027 lr: 0.005\n",
      "iteration: 124200 loss: 0.0021 lr: 0.005\n",
      "iteration: 124300 loss: 0.0021 lr: 0.005\n",
      "iteration: 124400 loss: 0.0023 lr: 0.005\n",
      "iteration: 124500 loss: 0.0023 lr: 0.005\n",
      "iteration: 124600 loss: 0.0023 lr: 0.005\n",
      "iteration: 124700 loss: 0.0022 lr: 0.005\n",
      "iteration: 124800 loss: 0.0023 lr: 0.005\n",
      "iteration: 124900 loss: 0.0025 lr: 0.005\n",
      "iteration: 125000 loss: 0.0023 lr: 0.005\n",
      "iteration: 125100 loss: 0.0026 lr: 0.005\n",
      "iteration: 125200 loss: 0.0024 lr: 0.005\n",
      "iteration: 125300 loss: 0.0029 lr: 0.005\n",
      "iteration: 125400 loss: 0.0023 lr: 0.005\n",
      "iteration: 125500 loss: 0.0024 lr: 0.005\n",
      "iteration: 125600 loss: 0.0024 lr: 0.005\n",
      "iteration: 125700 loss: 0.0023 lr: 0.005\n",
      "iteration: 125800 loss: 0.0025 lr: 0.005\n",
      "iteration: 125900 loss: 0.0024 lr: 0.005\n",
      "iteration: 126000 loss: 0.0025 lr: 0.005\n",
      "iteration: 126100 loss: 0.0021 lr: 0.005\n",
      "iteration: 126200 loss: 0.0024 lr: 0.005\n",
      "iteration: 126300 loss: 0.0030 lr: 0.005\n",
      "iteration: 126400 loss: 0.0024 lr: 0.005\n",
      "iteration: 126500 loss: 0.0021 lr: 0.005\n",
      "iteration: 126600 loss: 0.0023 lr: 0.005\n",
      "iteration: 126700 loss: 0.0025 lr: 0.005\n",
      "iteration: 126800 loss: 0.0026 lr: 0.005\n",
      "iteration: 126900 loss: 0.0033 lr: 0.005\n",
      "iteration: 127000 loss: 0.0022 lr: 0.005\n",
      "iteration: 127100 loss: 0.0022 lr: 0.005\n",
      "iteration: 127200 loss: 0.0024 lr: 0.005\n",
      "iteration: 127300 loss: 0.0029 lr: 0.005\n",
      "iteration: 127400 loss: 0.0027 lr: 0.005\n",
      "iteration: 127500 loss: 0.0026 lr: 0.005\n",
      "iteration: 127600 loss: 0.0024 lr: 0.005\n",
      "iteration: 127700 loss: 0.0024 lr: 0.005\n",
      "iteration: 127800 loss: 0.0022 lr: 0.005\n",
      "iteration: 127900 loss: 0.0028 lr: 0.005\n",
      "iteration: 128000 loss: 0.0023 lr: 0.005\n",
      "iteration: 128100 loss: 0.0031 lr: 0.005\n",
      "iteration: 128200 loss: 0.0030 lr: 0.005\n",
      "iteration: 128300 loss: 0.0025 lr: 0.005\n",
      "iteration: 128400 loss: 0.0026 lr: 0.005\n",
      "iteration: 128500 loss: 0.0024 lr: 0.005\n",
      "iteration: 128600 loss: 0.0022 lr: 0.005\n",
      "iteration: 128700 loss: 0.0026 lr: 0.005\n",
      "iteration: 128800 loss: 0.0023 lr: 0.005\n",
      "iteration: 128900 loss: 0.0026 lr: 0.005\n",
      "iteration: 129000 loss: 0.0031 lr: 0.005\n",
      "iteration: 129100 loss: 0.0025 lr: 0.005\n",
      "iteration: 129200 loss: 0.0024 lr: 0.005\n",
      "iteration: 129300 loss: 0.0024 lr: 0.005\n",
      "iteration: 129400 loss: 0.0021 lr: 0.005\n",
      "iteration: 129500 loss: 0.0025 lr: 0.005\n",
      "iteration: 129600 loss: 0.0022 lr: 0.005\n",
      "iteration: 129700 loss: 0.0023 lr: 0.005\n",
      "iteration: 129800 loss: 0.0024 lr: 0.005\n",
      "iteration: 129900 loss: 0.0032 lr: 0.005\n",
      "iteration: 130000 loss: 0.0024 lr: 0.005\n",
      "iteration: 130100 loss: 0.0026 lr: 0.005\n",
      "iteration: 130200 loss: 0.0024 lr: 0.005\n",
      "iteration: 130300 loss: 0.0023 lr: 0.005\n",
      "iteration: 130400 loss: 0.0026 lr: 0.005\n",
      "iteration: 130500 loss: 0.0030 lr: 0.005\n",
      "iteration: 130600 loss: 0.0026 lr: 0.005\n",
      "iteration: 130700 loss: 0.0026 lr: 0.005\n",
      "iteration: 130800 loss: 0.0025 lr: 0.005\n",
      "iteration: 130900 loss: 0.0026 lr: 0.005\n",
      "iteration: 131000 loss: 0.0023 lr: 0.005\n",
      "iteration: 131100 loss: 0.0025 lr: 0.005\n",
      "iteration: 131200 loss: 0.0022 lr: 0.005\n",
      "iteration: 131300 loss: 0.0022 lr: 0.005\n",
      "iteration: 131400 loss: 0.0023 lr: 0.005\n",
      "iteration: 131500 loss: 0.0027 lr: 0.005\n",
      "iteration: 131600 loss: 0.0023 lr: 0.005\n",
      "iteration: 131700 loss: 0.0022 lr: 0.005\n",
      "iteration: 131800 loss: 0.0021 lr: 0.005\n",
      "iteration: 131900 loss: 0.0026 lr: 0.005\n",
      "iteration: 132000 loss: 0.0023 lr: 0.005\n",
      "iteration: 132100 loss: 0.0025 lr: 0.005\n",
      "iteration: 132200 loss: 0.0026 lr: 0.005\n",
      "iteration: 132300 loss: 0.0024 lr: 0.005\n",
      "iteration: 132400 loss: 0.0023 lr: 0.005\n",
      "iteration: 132500 loss: 0.0027 lr: 0.005\n",
      "iteration: 132600 loss: 0.0023 lr: 0.005\n",
      "iteration: 132700 loss: 0.0024 lr: 0.005\n",
      "iteration: 132800 loss: 0.0024 lr: 0.005\n",
      "iteration: 132900 loss: 0.0024 lr: 0.005\n",
      "iteration: 133000 loss: 0.0024 lr: 0.005\n",
      "iteration: 133100 loss: 0.0023 lr: 0.005\n",
      "iteration: 133200 loss: 0.0020 lr: 0.005\n",
      "iteration: 133300 loss: 0.0026 lr: 0.005\n",
      "iteration: 133400 loss: 0.0023 lr: 0.005\n",
      "iteration: 133500 loss: 0.0025 lr: 0.005\n",
      "iteration: 133600 loss: 0.0026 lr: 0.005\n",
      "iteration: 133700 loss: 0.0024 lr: 0.005\n",
      "iteration: 133800 loss: 0.0024 lr: 0.005\n",
      "iteration: 133900 loss: 0.0023 lr: 0.005\n",
      "iteration: 134000 loss: 0.0027 lr: 0.005\n",
      "iteration: 134100 loss: 0.0023 lr: 0.005\n",
      "iteration: 134200 loss: 0.0020 lr: 0.005\n",
      "iteration: 134300 loss: 0.0029 lr: 0.005\n",
      "iteration: 134400 loss: 0.0025 lr: 0.005\n",
      "iteration: 134500 loss: 0.0027 lr: 0.005\n",
      "iteration: 134600 loss: 0.0026 lr: 0.005\n",
      "iteration: 134700 loss: 0.0023 lr: 0.005\n",
      "iteration: 134800 loss: 0.0020 lr: 0.005\n",
      "iteration: 134900 loss: 0.0028 lr: 0.005\n",
      "iteration: 135000 loss: 0.0024 lr: 0.005\n",
      "iteration: 135100 loss: 0.0025 lr: 0.005\n",
      "iteration: 135200 loss: 0.0022 lr: 0.005\n",
      "iteration: 135300 loss: 0.0023 lr: 0.005\n",
      "iteration: 135400 loss: 0.0026 lr: 0.005\n",
      "iteration: 135500 loss: 0.0025 lr: 0.005\n",
      "iteration: 135600 loss: 0.0027 lr: 0.005\n",
      "iteration: 135700 loss: 0.0022 lr: 0.005\n",
      "iteration: 135800 loss: 0.0025 lr: 0.005\n",
      "iteration: 135900 loss: 0.0022 lr: 0.005\n",
      "iteration: 136000 loss: 0.0026 lr: 0.005\n",
      "iteration: 136100 loss: 0.0021 lr: 0.005\n",
      "iteration: 136200 loss: 0.0022 lr: 0.005\n",
      "iteration: 136300 loss: 0.0027 lr: 0.005\n",
      "iteration: 136400 loss: 0.0021 lr: 0.005\n",
      "iteration: 136500 loss: 0.0021 lr: 0.005\n",
      "iteration: 136600 loss: 0.0024 lr: 0.005\n",
      "iteration: 136700 loss: 0.0019 lr: 0.005\n",
      "iteration: 136800 loss: 0.0025 lr: 0.005\n",
      "iteration: 136900 loss: 0.0022 lr: 0.005\n",
      "iteration: 137000 loss: 0.0023 lr: 0.005\n",
      "iteration: 137100 loss: 0.0024 lr: 0.005\n",
      "iteration: 137200 loss: 0.0021 lr: 0.005\n",
      "iteration: 137300 loss: 0.0023 lr: 0.005\n",
      "iteration: 137400 loss: 0.0024 lr: 0.005\n",
      "iteration: 137500 loss: 0.0023 lr: 0.005\n",
      "iteration: 137600 loss: 0.0021 lr: 0.005\n",
      "iteration: 137700 loss: 0.0024 lr: 0.005\n",
      "iteration: 137800 loss: 0.0021 lr: 0.005\n",
      "iteration: 137900 loss: 0.0021 lr: 0.005\n",
      "iteration: 138000 loss: 0.0022 lr: 0.005\n",
      "iteration: 138100 loss: 0.0024 lr: 0.005\n",
      "iteration: 138200 loss: 0.0025 lr: 0.005\n",
      "iteration: 138300 loss: 0.0027 lr: 0.005\n",
      "iteration: 138400 loss: 0.0025 lr: 0.005\n",
      "iteration: 138500 loss: 0.0019 lr: 0.005\n",
      "iteration: 138600 loss: 0.0024 lr: 0.005\n",
      "iteration: 138700 loss: 0.0022 lr: 0.005\n",
      "iteration: 138800 loss: 0.0026 lr: 0.005\n",
      "iteration: 138900 loss: 0.0025 lr: 0.005\n",
      "iteration: 139000 loss: 0.0021 lr: 0.005\n",
      "iteration: 139100 loss: 0.0021 lr: 0.005\n",
      "iteration: 139200 loss: 0.0022 lr: 0.005\n",
      "iteration: 139300 loss: 0.0024 lr: 0.005\n",
      "iteration: 139400 loss: 0.0023 lr: 0.005\n",
      "iteration: 139500 loss: 0.0024 lr: 0.005\n",
      "iteration: 139600 loss: 0.0021 lr: 0.005\n",
      "iteration: 139700 loss: 0.0024 lr: 0.005\n",
      "iteration: 139800 loss: 0.0020 lr: 0.005\n",
      "iteration: 139900 loss: 0.0022 lr: 0.005\n",
      "iteration: 140000 loss: 0.0026 lr: 0.005\n",
      "iteration: 140100 loss: 0.0020 lr: 0.005\n",
      "iteration: 140200 loss: 0.0023 lr: 0.005\n",
      "iteration: 140300 loss: 0.0023 lr: 0.005\n",
      "iteration: 140400 loss: 0.0022 lr: 0.005\n",
      "iteration: 140500 loss: 0.0023 lr: 0.005\n",
      "iteration: 140600 loss: 0.0022 lr: 0.005\n",
      "iteration: 140700 loss: 0.0026 lr: 0.005\n",
      "iteration: 140800 loss: 0.0022 lr: 0.005\n",
      "iteration: 140900 loss: 0.0023 lr: 0.005\n",
      "iteration: 141000 loss: 0.0024 lr: 0.005\n",
      "iteration: 141100 loss: 0.0024 lr: 0.005\n",
      "iteration: 141200 loss: 0.0020 lr: 0.005\n",
      "iteration: 141300 loss: 0.0027 lr: 0.005\n",
      "iteration: 141400 loss: 0.0023 lr: 0.005\n",
      "iteration: 141500 loss: 0.0028 lr: 0.005\n",
      "iteration: 141600 loss: 0.0024 lr: 0.005\n",
      "iteration: 141700 loss: 0.0025 lr: 0.005\n",
      "iteration: 141800 loss: 0.0027 lr: 0.005\n",
      "iteration: 141900 loss: 0.0023 lr: 0.005\n",
      "iteration: 142000 loss: 0.0027 lr: 0.005\n",
      "iteration: 142100 loss: 0.0025 lr: 0.005\n",
      "iteration: 142200 loss: 0.0024 lr: 0.005\n",
      "iteration: 142300 loss: 0.0024 lr: 0.005\n",
      "iteration: 142400 loss: 0.0023 lr: 0.005\n",
      "iteration: 142500 loss: 0.0027 lr: 0.005\n",
      "iteration: 142600 loss: 0.0022 lr: 0.005\n",
      "iteration: 142700 loss: 0.0023 lr: 0.005\n",
      "iteration: 142800 loss: 0.0027 lr: 0.005\n",
      "iteration: 142900 loss: 0.0022 lr: 0.005\n",
      "iteration: 143000 loss: 0.0021 lr: 0.005\n",
      "iteration: 143100 loss: 0.0023 lr: 0.005\n",
      "iteration: 143200 loss: 0.0020 lr: 0.005\n",
      "iteration: 143300 loss: 0.0022 lr: 0.005\n",
      "iteration: 143400 loss: 0.0025 lr: 0.005\n",
      "iteration: 143500 loss: 0.0020 lr: 0.005\n",
      "iteration: 143600 loss: 0.0024 lr: 0.005\n",
      "iteration: 143700 loss: 0.0023 lr: 0.005\n",
      "iteration: 143800 loss: 0.0027 lr: 0.005\n",
      "iteration: 143900 loss: 0.0024 lr: 0.005\n",
      "iteration: 144000 loss: 0.0021 lr: 0.005\n",
      "iteration: 144100 loss: 0.0023 lr: 0.005\n",
      "iteration: 144200 loss: 0.0025 lr: 0.005\n",
      "iteration: 144300 loss: 0.0024 lr: 0.005\n",
      "iteration: 144400 loss: 0.0021 lr: 0.005\n",
      "iteration: 144500 loss: 0.0031 lr: 0.005\n",
      "iteration: 144600 loss: 0.0022 lr: 0.005\n",
      "iteration: 144700 loss: 0.0019 lr: 0.005\n",
      "iteration: 144800 loss: 0.0023 lr: 0.005\n",
      "iteration: 144900 loss: 0.0028 lr: 0.005\n",
      "iteration: 145000 loss: 0.0025 lr: 0.005\n",
      "iteration: 145100 loss: 0.0028 lr: 0.005\n",
      "iteration: 145200 loss: 0.0026 lr: 0.005\n",
      "iteration: 145300 loss: 0.0022 lr: 0.005\n",
      "iteration: 145400 loss: 0.0023 lr: 0.005\n",
      "iteration: 145500 loss: 0.0024 lr: 0.005\n",
      "iteration: 145600 loss: 0.0023 lr: 0.005\n",
      "iteration: 145700 loss: 0.0028 lr: 0.005\n",
      "iteration: 145800 loss: 0.0021 lr: 0.005\n",
      "iteration: 145900 loss: 0.0022 lr: 0.005\n",
      "iteration: 146000 loss: 0.0023 lr: 0.005\n",
      "iteration: 146100 loss: 0.0023 lr: 0.005\n",
      "iteration: 146200 loss: 0.0027 lr: 0.005\n",
      "iteration: 146300 loss: 0.0025 lr: 0.005\n",
      "iteration: 146400 loss: 0.0022 lr: 0.005\n",
      "iteration: 146500 loss: 0.0024 lr: 0.005\n",
      "iteration: 146600 loss: 0.0028 lr: 0.005\n",
      "iteration: 146700 loss: 0.0027 lr: 0.005\n",
      "iteration: 146800 loss: 0.0024 lr: 0.005\n",
      "iteration: 146900 loss: 0.0020 lr: 0.005\n",
      "iteration: 147000 loss: 0.0023 lr: 0.005\n",
      "iteration: 147100 loss: 0.0023 lr: 0.005\n",
      "iteration: 147200 loss: 0.0028 lr: 0.005\n",
      "iteration: 147300 loss: 0.0025 lr: 0.005\n",
      "iteration: 147400 loss: 0.0023 lr: 0.005\n",
      "iteration: 147500 loss: 0.0022 lr: 0.005\n",
      "iteration: 147600 loss: 0.0024 lr: 0.005\n",
      "iteration: 147700 loss: 0.0024 lr: 0.005\n",
      "iteration: 147800 loss: 0.0023 lr: 0.005\n",
      "iteration: 147900 loss: 0.0022 lr: 0.005\n",
      "iteration: 148000 loss: 0.0020 lr: 0.005\n",
      "iteration: 148100 loss: 0.0018 lr: 0.005\n",
      "iteration: 148200 loss: 0.0021 lr: 0.005\n",
      "iteration: 148300 loss: 0.0023 lr: 0.005\n",
      "iteration: 148400 loss: 0.0023 lr: 0.005\n",
      "iteration: 148500 loss: 0.0024 lr: 0.005\n",
      "iteration: 148600 loss: 0.0026 lr: 0.005\n",
      "iteration: 148700 loss: 0.0025 lr: 0.005\n",
      "iteration: 148800 loss: 0.0020 lr: 0.005\n",
      "iteration: 148900 loss: 0.0021 lr: 0.005\n",
      "iteration: 149000 loss: 0.0021 lr: 0.005\n",
      "iteration: 149100 loss: 0.0022 lr: 0.005\n",
      "iteration: 149200 loss: 0.0022 lr: 0.005\n",
      "iteration: 149300 loss: 0.0022 lr: 0.005\n",
      "iteration: 149400 loss: 0.0022 lr: 0.005\n",
      "iteration: 149500 loss: 0.0027 lr: 0.005\n",
      "iteration: 149600 loss: 0.0022 lr: 0.005\n",
      "iteration: 149700 loss: 0.0021 lr: 0.005\n",
      "iteration: 149800 loss: 0.0023 lr: 0.005\n",
      "iteration: 149900 loss: 0.0023 lr: 0.005\n",
      "iteration: 150000 loss: 0.0023 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1377, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1455, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 83, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1396, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:\n",
      "\n",
      "Detected at node 'fifo_queue_enqueue' defined at (most recent call last):\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "      \"__main__\", mod_spec)\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "      app.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "      handle._run()\n",
      "    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "      ret = callback()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "      self.run()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "      yielded = self.gen.send(value)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "      yield self.process_one()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 346, in wrapper\n",
      "      runner = Runner(result, future, yielded)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1080, in __init__\n",
      "      self.run()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "      yielded = self.gen.send(value)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "      yield gen.maybe_future(dispatch(*args))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "      yield gen.maybe_future(handler(stream, idents, msg))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "      user_expressions, allow_stdin,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "      res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "      raw_cell, store_history, silent, shell_futures)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "      return runner(coro)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "      interactivity=interactivity, compiler=compiler, result=result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "      if (await self.run_code(code, result,  async_=asy)):\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"<ipython-input-12-8818d4b8f049>\", line 2, in <module>\n",
      "      deeplabcut.train_network(path_config_file, shuffle=8, displayiters=100, saveiters=500)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "      allow_growth=allow_growth,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "      batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "      enqueue_op = q.enqueue(placeholders_list)\n",
      "Node: 'fifo_queue_enqueue'\n",
      "Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_enqueue':\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 346, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1080, in __init__\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-8818d4b8f049>\", line 2, in <module>\n",
      "    deeplabcut.train_network(path_config_file, shuffle=8, displayiters=100, saveiters=500)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "    allow_growth=allow_growth,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 348, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4065, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3784, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2175, in __init__\n",
      "    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "deeplabcut.train_network(path_config_file, shuffle=8, displayiters=100, saveiters=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kcwMTOpXU4kB",
    "outputId": "43f83d9a-a2c2-48fb-ddff-0648612d8fde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle10.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle10.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 150000]],\n",
      " 'net_type': 'mobilenet_v2_1.0',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/content/drive/My '\n",
      "                 'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/content/drive/My '\n",
      "                    'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle10/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading ImageNet-pretrained mobilenet_v2_1.0\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 500\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle10/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle10.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle10.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 150000]], 'net_type': 'mobilenet_v2_1.0', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.1454 lr: 0.005\n",
      "iteration: 200 loss: 0.0386 lr: 0.005\n",
      "iteration: 300 loss: 0.0324 lr: 0.005\n",
      "iteration: 400 loss: 0.0293 lr: 0.005\n",
      "iteration: 500 loss: 0.0268 lr: 0.005\n",
      "iteration: 600 loss: 0.0251 lr: 0.005\n",
      "iteration: 700 loss: 0.0224 lr: 0.005\n",
      "iteration: 800 loss: 0.0228 lr: 0.005\n",
      "iteration: 900 loss: 0.0240 lr: 0.005\n",
      "iteration: 1000 loss: 0.0204 lr: 0.005\n",
      "iteration: 1100 loss: 0.0222 lr: 0.005\n",
      "iteration: 1200 loss: 0.0214 lr: 0.005\n",
      "iteration: 1300 loss: 0.0213 lr: 0.005\n",
      "iteration: 1400 loss: 0.0221 lr: 0.005\n",
      "iteration: 1500 loss: 0.0211 lr: 0.005\n",
      "iteration: 1600 loss: 0.0204 lr: 0.005\n",
      "iteration: 1700 loss: 0.0205 lr: 0.005\n",
      "iteration: 1800 loss: 0.0191 lr: 0.005\n",
      "iteration: 1900 loss: 0.0204 lr: 0.005\n",
      "iteration: 2000 loss: 0.0208 lr: 0.005\n",
      "iteration: 2100 loss: 0.0199 lr: 0.005\n",
      "iteration: 2200 loss: 0.0186 lr: 0.005\n",
      "iteration: 2300 loss: 0.0187 lr: 0.005\n",
      "iteration: 2400 loss: 0.0181 lr: 0.005\n",
      "iteration: 2500 loss: 0.0180 lr: 0.005\n",
      "iteration: 2600 loss: 0.0190 lr: 0.005\n",
      "iteration: 2700 loss: 0.0190 lr: 0.005\n",
      "iteration: 2800 loss: 0.0181 lr: 0.005\n",
      "iteration: 2900 loss: 0.0171 lr: 0.005\n",
      "iteration: 3000 loss: 0.0184 lr: 0.005\n",
      "iteration: 3100 loss: 0.0164 lr: 0.005\n",
      "iteration: 3200 loss: 0.0169 lr: 0.005\n",
      "iteration: 3300 loss: 0.0158 lr: 0.005\n",
      "iteration: 3400 loss: 0.0176 lr: 0.005\n",
      "iteration: 3500 loss: 0.0171 lr: 0.005\n",
      "iteration: 3600 loss: 0.0174 lr: 0.005\n",
      "iteration: 3700 loss: 0.0163 lr: 0.005\n",
      "iteration: 3800 loss: 0.0163 lr: 0.005\n",
      "iteration: 3900 loss: 0.0170 lr: 0.005\n",
      "iteration: 4000 loss: 0.0155 lr: 0.005\n",
      "iteration: 4100 loss: 0.0162 lr: 0.005\n",
      "iteration: 4200 loss: 0.0159 lr: 0.005\n",
      "iteration: 4300 loss: 0.0165 lr: 0.005\n",
      "iteration: 4400 loss: 0.0169 lr: 0.005\n",
      "iteration: 4500 loss: 0.0158 lr: 0.005\n",
      "iteration: 4600 loss: 0.0160 lr: 0.005\n",
      "iteration: 4700 loss: 0.0137 lr: 0.005\n",
      "iteration: 4800 loss: 0.0151 lr: 0.005\n",
      "iteration: 4900 loss: 0.0148 lr: 0.005\n",
      "iteration: 5000 loss: 0.0167 lr: 0.005\n",
      "iteration: 5100 loss: 0.0146 lr: 0.005\n",
      "iteration: 5200 loss: 0.0149 lr: 0.005\n",
      "iteration: 5300 loss: 0.0151 lr: 0.005\n",
      "iteration: 5400 loss: 0.0139 lr: 0.005\n",
      "iteration: 5500 loss: 0.0147 lr: 0.005\n",
      "iteration: 5600 loss: 0.0136 lr: 0.005\n",
      "iteration: 5700 loss: 0.0144 lr: 0.005\n",
      "iteration: 5800 loss: 0.0143 lr: 0.005\n",
      "iteration: 5900 loss: 0.0136 lr: 0.005\n",
      "iteration: 6000 loss: 0.0157 lr: 0.005\n",
      "iteration: 6100 loss: 0.0160 lr: 0.005\n",
      "iteration: 6200 loss: 0.0149 lr: 0.005\n",
      "iteration: 6300 loss: 0.0145 lr: 0.005\n",
      "iteration: 6400 loss: 0.0140 lr: 0.005\n",
      "iteration: 6500 loss: 0.0150 lr: 0.005\n",
      "iteration: 6600 loss: 0.0146 lr: 0.005\n",
      "iteration: 6700 loss: 0.0137 lr: 0.005\n",
      "iteration: 6800 loss: 0.0134 lr: 0.005\n",
      "iteration: 6900 loss: 0.0138 lr: 0.005\n",
      "iteration: 7000 loss: 0.0135 lr: 0.005\n",
      "iteration: 7100 loss: 0.0137 lr: 0.005\n",
      "iteration: 7200 loss: 0.0143 lr: 0.005\n",
      "iteration: 7300 loss: 0.0144 lr: 0.005\n",
      "iteration: 7400 loss: 0.0138 lr: 0.005\n",
      "iteration: 7500 loss: 0.0137 lr: 0.005\n",
      "iteration: 7600 loss: 0.0134 lr: 0.005\n",
      "iteration: 7700 loss: 0.0137 lr: 0.005\n",
      "iteration: 7800 loss: 0.0129 lr: 0.005\n",
      "iteration: 7900 loss: 0.0128 lr: 0.005\n",
      "iteration: 8000 loss: 0.0130 lr: 0.005\n",
      "iteration: 8100 loss: 0.0134 lr: 0.005\n",
      "iteration: 8200 loss: 0.0125 lr: 0.005\n",
      "iteration: 8300 loss: 0.0131 lr: 0.005\n",
      "iteration: 8400 loss: 0.0127 lr: 0.005\n",
      "iteration: 8500 loss: 0.0138 lr: 0.005\n",
      "iteration: 8600 loss: 0.0126 lr: 0.005\n",
      "iteration: 8700 loss: 0.0119 lr: 0.005\n",
      "iteration: 8800 loss: 0.0123 lr: 0.005\n",
      "iteration: 8900 loss: 0.0116 lr: 0.005\n",
      "iteration: 9000 loss: 0.0127 lr: 0.005\n",
      "iteration: 9100 loss: 0.0131 lr: 0.005\n",
      "iteration: 9200 loss: 0.0129 lr: 0.005\n",
      "iteration: 9300 loss: 0.0126 lr: 0.005\n",
      "iteration: 9400 loss: 0.0121 lr: 0.005\n",
      "iteration: 9500 loss: 0.0135 lr: 0.005\n",
      "iteration: 9600 loss: 0.0120 lr: 0.005\n",
      "iteration: 9700 loss: 0.0107 lr: 0.005\n",
      "iteration: 9800 loss: 0.0122 lr: 0.005\n",
      "iteration: 9900 loss: 0.0118 lr: 0.005\n",
      "iteration: 10000 loss: 0.0124 lr: 0.005\n",
      "iteration: 10100 loss: 0.0119 lr: 0.005\n",
      "iteration: 10200 loss: 0.0113 lr: 0.005\n",
      "iteration: 10300 loss: 0.0120 lr: 0.005\n",
      "iteration: 10400 loss: 0.0115 lr: 0.005\n",
      "iteration: 10500 loss: 0.0119 lr: 0.005\n",
      "iteration: 10600 loss: 0.0115 lr: 0.005\n",
      "iteration: 10700 loss: 0.0113 lr: 0.005\n",
      "iteration: 10800 loss: 0.0116 lr: 0.005\n",
      "iteration: 10900 loss: 0.0117 lr: 0.005\n",
      "iteration: 11000 loss: 0.0115 lr: 0.005\n",
      "iteration: 11100 loss: 0.0115 lr: 0.005\n",
      "iteration: 11200 loss: 0.0106 lr: 0.005\n",
      "iteration: 11300 loss: 0.0117 lr: 0.005\n",
      "iteration: 11400 loss: 0.0108 lr: 0.005\n",
      "iteration: 11500 loss: 0.0126 lr: 0.005\n",
      "iteration: 11600 loss: 0.0103 lr: 0.005\n",
      "iteration: 11700 loss: 0.0096 lr: 0.005\n",
      "iteration: 11800 loss: 0.0108 lr: 0.005\n",
      "iteration: 11900 loss: 0.0125 lr: 0.005\n",
      "iteration: 12000 loss: 0.0109 lr: 0.005\n",
      "iteration: 12100 loss: 0.0112 lr: 0.005\n",
      "iteration: 12200 loss: 0.0113 lr: 0.005\n",
      "iteration: 12300 loss: 0.0108 lr: 0.005\n",
      "iteration: 12400 loss: 0.0110 lr: 0.005\n",
      "iteration: 12500 loss: 0.0104 lr: 0.005\n",
      "iteration: 12600 loss: 0.0105 lr: 0.005\n",
      "iteration: 12700 loss: 0.0104 lr: 0.005\n",
      "iteration: 12800 loss: 0.0095 lr: 0.005\n",
      "iteration: 12900 loss: 0.0104 lr: 0.005\n",
      "iteration: 13000 loss: 0.0102 lr: 0.005\n",
      "iteration: 13100 loss: 0.0094 lr: 0.005\n",
      "iteration: 13200 loss: 0.0102 lr: 0.005\n",
      "iteration: 13300 loss: 0.0100 lr: 0.005\n",
      "iteration: 13400 loss: 0.0110 lr: 0.005\n",
      "iteration: 13500 loss: 0.0098 lr: 0.005\n",
      "iteration: 13600 loss: 0.0103 lr: 0.005\n",
      "iteration: 13700 loss: 0.0112 lr: 0.005\n",
      "iteration: 13800 loss: 0.0096 lr: 0.005\n",
      "iteration: 13900 loss: 0.0099 lr: 0.005\n",
      "iteration: 14000 loss: 0.0097 lr: 0.005\n",
      "iteration: 14100 loss: 0.0094 lr: 0.005\n",
      "iteration: 14200 loss: 0.0103 lr: 0.005\n",
      "iteration: 14300 loss: 0.0087 lr: 0.005\n",
      "iteration: 14400 loss: 0.0095 lr: 0.005\n",
      "iteration: 14500 loss: 0.0097 lr: 0.005\n",
      "iteration: 14600 loss: 0.0079 lr: 0.005\n",
      "iteration: 14700 loss: 0.0095 lr: 0.005\n",
      "iteration: 14800 loss: 0.0084 lr: 0.005\n",
      "iteration: 14900 loss: 0.0095 lr: 0.005\n",
      "iteration: 15000 loss: 0.0095 lr: 0.005\n",
      "iteration: 15100 loss: 0.0085 lr: 0.005\n",
      "iteration: 15200 loss: 0.0084 lr: 0.005\n",
      "iteration: 15300 loss: 0.0098 lr: 0.005\n",
      "iteration: 15400 loss: 0.0085 lr: 0.005\n",
      "iteration: 15500 loss: 0.0093 lr: 0.005\n",
      "iteration: 15600 loss: 0.0082 lr: 0.005\n",
      "iteration: 15700 loss: 0.0087 lr: 0.005\n",
      "iteration: 15800 loss: 0.0093 lr: 0.005\n",
      "iteration: 15900 loss: 0.0083 lr: 0.005\n",
      "iteration: 16000 loss: 0.0081 lr: 0.005\n",
      "iteration: 16100 loss: 0.0084 lr: 0.005\n",
      "iteration: 16200 loss: 0.0081 lr: 0.005\n",
      "iteration: 16300 loss: 0.0084 lr: 0.005\n",
      "iteration: 16400 loss: 0.0085 lr: 0.005\n",
      "iteration: 16500 loss: 0.0085 lr: 0.005\n",
      "iteration: 16600 loss: 0.0080 lr: 0.005\n",
      "iteration: 16700 loss: 0.0080 lr: 0.005\n",
      "iteration: 16800 loss: 0.0084 lr: 0.005\n",
      "iteration: 16900 loss: 0.0081 lr: 0.005\n",
      "iteration: 17000 loss: 0.0086 lr: 0.005\n",
      "iteration: 17100 loss: 0.0088 lr: 0.005\n",
      "iteration: 17200 loss: 0.0075 lr: 0.005\n",
      "iteration: 17300 loss: 0.0079 lr: 0.005\n",
      "iteration: 17400 loss: 0.0084 lr: 0.005\n",
      "iteration: 17500 loss: 0.0088 lr: 0.005\n",
      "iteration: 17600 loss: 0.0077 lr: 0.005\n",
      "iteration: 17700 loss: 0.0075 lr: 0.005\n",
      "iteration: 17800 loss: 0.0071 lr: 0.005\n",
      "iteration: 17900 loss: 0.0073 lr: 0.005\n",
      "iteration: 18000 loss: 0.0073 lr: 0.005\n",
      "iteration: 18100 loss: 0.0084 lr: 0.005\n",
      "iteration: 18200 loss: 0.0079 lr: 0.005\n",
      "iteration: 18300 loss: 0.0079 lr: 0.005\n",
      "iteration: 18400 loss: 0.0088 lr: 0.005\n",
      "iteration: 18500 loss: 0.0084 lr: 0.005\n",
      "iteration: 18600 loss: 0.0080 lr: 0.005\n",
      "iteration: 18700 loss: 0.0092 lr: 0.005\n",
      "iteration: 18800 loss: 0.0076 lr: 0.005\n",
      "iteration: 18900 loss: 0.0073 lr: 0.005\n",
      "iteration: 19000 loss: 0.0089 lr: 0.005\n",
      "iteration: 19100 loss: 0.0077 lr: 0.005\n",
      "iteration: 19200 loss: 0.0082 lr: 0.005\n",
      "iteration: 19300 loss: 0.0074 lr: 0.005\n",
      "iteration: 19400 loss: 0.0079 lr: 0.005\n",
      "iteration: 19500 loss: 0.0080 lr: 0.005\n",
      "iteration: 19600 loss: 0.0069 lr: 0.005\n",
      "iteration: 19700 loss: 0.0071 lr: 0.005\n",
      "iteration: 19800 loss: 0.0080 lr: 0.005\n",
      "iteration: 19900 loss: 0.0073 lr: 0.005\n",
      "iteration: 20000 loss: 0.0070 lr: 0.005\n",
      "iteration: 20100 loss: 0.0067 lr: 0.005\n",
      "iteration: 20200 loss: 0.0077 lr: 0.005\n",
      "iteration: 20300 loss: 0.0079 lr: 0.005\n",
      "iteration: 20400 loss: 0.0084 lr: 0.005\n",
      "iteration: 20500 loss: 0.0075 lr: 0.005\n",
      "iteration: 20600 loss: 0.0064 lr: 0.005\n",
      "iteration: 20700 loss: 0.0073 lr: 0.005\n",
      "iteration: 20800 loss: 0.0083 lr: 0.005\n",
      "iteration: 20900 loss: 0.0077 lr: 0.005\n",
      "iteration: 21000 loss: 0.0068 lr: 0.005\n",
      "iteration: 21100 loss: 0.0083 lr: 0.005\n",
      "iteration: 21200 loss: 0.0074 lr: 0.005\n",
      "iteration: 21300 loss: 0.0065 lr: 0.005\n",
      "iteration: 21400 loss: 0.0069 lr: 0.005\n",
      "iteration: 21500 loss: 0.0063 lr: 0.005\n",
      "iteration: 21600 loss: 0.0079 lr: 0.005\n",
      "iteration: 21700 loss: 0.0073 lr: 0.005\n",
      "iteration: 21800 loss: 0.0068 lr: 0.005\n",
      "iteration: 21900 loss: 0.0068 lr: 0.005\n",
      "iteration: 22000 loss: 0.0078 lr: 0.005\n",
      "iteration: 22100 loss: 0.0071 lr: 0.005\n",
      "iteration: 22200 loss: 0.0073 lr: 0.005\n",
      "iteration: 22300 loss: 0.0073 lr: 0.005\n",
      "iteration: 22400 loss: 0.0071 lr: 0.005\n",
      "iteration: 22500 loss: 0.0066 lr: 0.005\n",
      "iteration: 22600 loss: 0.0075 lr: 0.005\n",
      "iteration: 22700 loss: 0.0078 lr: 0.005\n",
      "iteration: 22800 loss: 0.0067 lr: 0.005\n",
      "iteration: 22900 loss: 0.0069 lr: 0.005\n",
      "iteration: 23000 loss: 0.0067 lr: 0.005\n",
      "iteration: 23100 loss: 0.0080 lr: 0.005\n",
      "iteration: 23200 loss: 0.0066 lr: 0.005\n",
      "iteration: 23300 loss: 0.0071 lr: 0.005\n",
      "iteration: 23400 loss: 0.0069 lr: 0.005\n",
      "iteration: 23500 loss: 0.0072 lr: 0.005\n",
      "iteration: 23600 loss: 0.0062 lr: 0.005\n",
      "iteration: 23700 loss: 0.0062 lr: 0.005\n",
      "iteration: 23800 loss: 0.0066 lr: 0.005\n",
      "iteration: 23900 loss: 0.0054 lr: 0.005\n",
      "iteration: 24000 loss: 0.0075 lr: 0.005\n",
      "iteration: 24100 loss: 0.0068 lr: 0.005\n",
      "iteration: 24200 loss: 0.0069 lr: 0.005\n",
      "iteration: 24300 loss: 0.0063 lr: 0.005\n",
      "iteration: 24400 loss: 0.0060 lr: 0.005\n",
      "iteration: 24500 loss: 0.0074 lr: 0.005\n",
      "iteration: 24600 loss: 0.0065 lr: 0.005\n",
      "iteration: 24700 loss: 0.0067 lr: 0.005\n",
      "iteration: 24800 loss: 0.0069 lr: 0.005\n",
      "iteration: 24900 loss: 0.0060 lr: 0.005\n",
      "iteration: 25000 loss: 0.0059 lr: 0.005\n",
      "iteration: 25100 loss: 0.0062 lr: 0.005\n",
      "iteration: 25200 loss: 0.0071 lr: 0.005\n",
      "iteration: 25300 loss: 0.0069 lr: 0.005\n",
      "iteration: 25400 loss: 0.0076 lr: 0.005\n",
      "iteration: 25500 loss: 0.0070 lr: 0.005\n",
      "iteration: 25600 loss: 0.0068 lr: 0.005\n",
      "iteration: 25700 loss: 0.0057 lr: 0.005\n",
      "iteration: 25800 loss: 0.0063 lr: 0.005\n",
      "iteration: 25900 loss: 0.0068 lr: 0.005\n",
      "iteration: 26000 loss: 0.0074 lr: 0.005\n",
      "iteration: 26100 loss: 0.0060 lr: 0.005\n",
      "iteration: 26200 loss: 0.0069 lr: 0.005\n",
      "iteration: 26300 loss: 0.0070 lr: 0.005\n",
      "iteration: 26400 loss: 0.0067 lr: 0.005\n",
      "iteration: 26500 loss: 0.0062 lr: 0.005\n",
      "iteration: 26600 loss: 0.0069 lr: 0.005\n",
      "iteration: 26700 loss: 0.0060 lr: 0.005\n",
      "iteration: 26800 loss: 0.0068 lr: 0.005\n",
      "iteration: 26900 loss: 0.0066 lr: 0.005\n",
      "iteration: 27000 loss: 0.0064 lr: 0.005\n",
      "iteration: 27100 loss: 0.0062 lr: 0.005\n",
      "iteration: 27200 loss: 0.0066 lr: 0.005\n",
      "iteration: 27300 loss: 0.0063 lr: 0.005\n",
      "iteration: 27400 loss: 0.0066 lr: 0.005\n",
      "iteration: 27500 loss: 0.0061 lr: 0.005\n",
      "iteration: 27600 loss: 0.0060 lr: 0.005\n",
      "iteration: 27700 loss: 0.0069 lr: 0.005\n",
      "iteration: 27800 loss: 0.0062 lr: 0.005\n",
      "iteration: 27900 loss: 0.0078 lr: 0.005\n",
      "iteration: 28000 loss: 0.0064 lr: 0.005\n",
      "iteration: 28100 loss: 0.0057 lr: 0.005\n",
      "iteration: 28200 loss: 0.0060 lr: 0.005\n",
      "iteration: 28300 loss: 0.0071 lr: 0.005\n",
      "iteration: 28400 loss: 0.0064 lr: 0.005\n",
      "iteration: 28500 loss: 0.0064 lr: 0.005\n",
      "iteration: 28600 loss: 0.0060 lr: 0.005\n",
      "iteration: 28700 loss: 0.0056 lr: 0.005\n",
      "iteration: 28800 loss: 0.0059 lr: 0.005\n",
      "iteration: 28900 loss: 0.0061 lr: 0.005\n",
      "iteration: 29000 loss: 0.0057 lr: 0.005\n",
      "iteration: 29100 loss: 0.0063 lr: 0.005\n",
      "iteration: 29200 loss: 0.0060 lr: 0.005\n",
      "iteration: 29300 loss: 0.0058 lr: 0.005\n",
      "iteration: 29400 loss: 0.0062 lr: 0.005\n",
      "iteration: 29500 loss: 0.0052 lr: 0.005\n",
      "iteration: 29600 loss: 0.0061 lr: 0.005\n",
      "iteration: 29700 loss: 0.0058 lr: 0.005\n",
      "iteration: 29800 loss: 0.0060 lr: 0.005\n",
      "iteration: 29900 loss: 0.0063 lr: 0.005\n",
      "iteration: 30000 loss: 0.0060 lr: 0.005\n",
      "iteration: 30100 loss: 0.0067 lr: 0.005\n",
      "iteration: 30200 loss: 0.0062 lr: 0.005\n",
      "iteration: 30300 loss: 0.0056 lr: 0.005\n",
      "iteration: 30400 loss: 0.0060 lr: 0.005\n",
      "iteration: 30500 loss: 0.0049 lr: 0.005\n",
      "iteration: 30600 loss: 0.0061 lr: 0.005\n",
      "iteration: 30700 loss: 0.0056 lr: 0.005\n",
      "iteration: 30800 loss: 0.0057 lr: 0.005\n",
      "iteration: 30900 loss: 0.0056 lr: 0.005\n",
      "iteration: 31000 loss: 0.0060 lr: 0.005\n",
      "iteration: 31100 loss: 0.0059 lr: 0.005\n",
      "iteration: 31200 loss: 0.0053 lr: 0.005\n",
      "iteration: 31300 loss: 0.0064 lr: 0.005\n",
      "iteration: 31400 loss: 0.0053 lr: 0.005\n",
      "iteration: 31500 loss: 0.0060 lr: 0.005\n",
      "iteration: 31600 loss: 0.0060 lr: 0.005\n",
      "iteration: 31700 loss: 0.0054 lr: 0.005\n",
      "iteration: 31800 loss: 0.0050 lr: 0.005\n",
      "iteration: 31900 loss: 0.0067 lr: 0.005\n",
      "iteration: 32000 loss: 0.0054 lr: 0.005\n",
      "iteration: 32100 loss: 0.0056 lr: 0.005\n",
      "iteration: 32200 loss: 0.0063 lr: 0.005\n",
      "iteration: 32300 loss: 0.0048 lr: 0.005\n",
      "iteration: 32400 loss: 0.0056 lr: 0.005\n",
      "iteration: 32500 loss: 0.0057 lr: 0.005\n",
      "iteration: 32600 loss: 0.0058 lr: 0.005\n",
      "iteration: 32700 loss: 0.0054 lr: 0.005\n",
      "iteration: 32800 loss: 0.0057 lr: 0.005\n",
      "iteration: 32900 loss: 0.0057 lr: 0.005\n",
      "iteration: 33000 loss: 0.0053 lr: 0.005\n",
      "iteration: 33100 loss: 0.0058 lr: 0.005\n",
      "iteration: 33200 loss: 0.0054 lr: 0.005\n",
      "iteration: 33300 loss: 0.0058 lr: 0.005\n",
      "iteration: 33400 loss: 0.0062 lr: 0.005\n",
      "iteration: 33500 loss: 0.0054 lr: 0.005\n",
      "iteration: 33600 loss: 0.0055 lr: 0.005\n",
      "iteration: 33700 loss: 0.0052 lr: 0.005\n",
      "iteration: 33800 loss: 0.0066 lr: 0.005\n",
      "iteration: 33900 loss: 0.0045 lr: 0.005\n",
      "iteration: 34000 loss: 0.0049 lr: 0.005\n",
      "iteration: 34100 loss: 0.0056 lr: 0.005\n",
      "iteration: 34200 loss: 0.0056 lr: 0.005\n",
      "iteration: 34300 loss: 0.0055 lr: 0.005\n",
      "iteration: 34400 loss: 0.0053 lr: 0.005\n",
      "iteration: 34500 loss: 0.0051 lr: 0.005\n",
      "iteration: 34600 loss: 0.0052 lr: 0.005\n",
      "iteration: 34700 loss: 0.0054 lr: 0.005\n",
      "iteration: 34800 loss: 0.0055 lr: 0.005\n",
      "iteration: 34900 loss: 0.0055 lr: 0.005\n",
      "iteration: 35000 loss: 0.0054 lr: 0.005\n",
      "iteration: 35100 loss: 0.0063 lr: 0.005\n",
      "iteration: 35200 loss: 0.0056 lr: 0.005\n",
      "iteration: 35300 loss: 0.0055 lr: 0.005\n",
      "iteration: 35400 loss: 0.0062 lr: 0.005\n",
      "iteration: 35500 loss: 0.0071 lr: 0.005\n",
      "iteration: 35600 loss: 0.0061 lr: 0.005\n",
      "iteration: 35700 loss: 0.0062 lr: 0.005\n",
      "iteration: 35800 loss: 0.0049 lr: 0.005\n",
      "iteration: 35900 loss: 0.0051 lr: 0.005\n",
      "iteration: 36000 loss: 0.0059 lr: 0.005\n",
      "iteration: 36100 loss: 0.0050 lr: 0.005\n",
      "iteration: 36200 loss: 0.0051 lr: 0.005\n",
      "iteration: 36300 loss: 0.0052 lr: 0.005\n",
      "iteration: 36400 loss: 0.0045 lr: 0.005\n",
      "iteration: 36500 loss: 0.0048 lr: 0.005\n",
      "iteration: 36600 loss: 0.0060 lr: 0.005\n",
      "iteration: 36700 loss: 0.0062 lr: 0.005\n",
      "iteration: 36800 loss: 0.0050 lr: 0.005\n",
      "iteration: 36900 loss: 0.0048 lr: 0.005\n",
      "iteration: 37000 loss: 0.0044 lr: 0.005\n",
      "iteration: 37100 loss: 0.0055 lr: 0.005\n",
      "iteration: 37200 loss: 0.0063 lr: 0.005\n",
      "iteration: 37300 loss: 0.0053 lr: 0.005\n",
      "iteration: 37400 loss: 0.0046 lr: 0.005\n",
      "iteration: 37500 loss: 0.0056 lr: 0.005\n",
      "iteration: 37600 loss: 0.0054 lr: 0.005\n",
      "iteration: 37700 loss: 0.0059 lr: 0.005\n",
      "iteration: 37800 loss: 0.0056 lr: 0.005\n",
      "iteration: 37900 loss: 0.0053 lr: 0.005\n",
      "iteration: 38000 loss: 0.0056 lr: 0.005\n",
      "iteration: 38100 loss: 0.0054 lr: 0.005\n",
      "iteration: 38200 loss: 0.0051 lr: 0.005\n",
      "iteration: 38300 loss: 0.0056 lr: 0.005\n",
      "iteration: 38400 loss: 0.0054 lr: 0.005\n",
      "iteration: 38500 loss: 0.0053 lr: 0.005\n",
      "iteration: 38600 loss: 0.0045 lr: 0.005\n",
      "iteration: 38700 loss: 0.0047 lr: 0.005\n",
      "iteration: 38800 loss: 0.0051 lr: 0.005\n",
      "iteration: 38900 loss: 0.0049 lr: 0.005\n",
      "iteration: 39000 loss: 0.0058 lr: 0.005\n",
      "iteration: 39100 loss: 0.0059 lr: 0.005\n",
      "iteration: 39200 loss: 0.0049 lr: 0.005\n",
      "iteration: 39300 loss: 0.0052 lr: 0.005\n",
      "iteration: 39400 loss: 0.0055 lr: 0.005\n",
      "iteration: 39500 loss: 0.0051 lr: 0.005\n",
      "iteration: 39600 loss: 0.0049 lr: 0.005\n",
      "iteration: 39700 loss: 0.0061 lr: 0.005\n",
      "iteration: 39800 loss: 0.0053 lr: 0.005\n",
      "iteration: 39900 loss: 0.0054 lr: 0.005\n",
      "iteration: 40000 loss: 0.0058 lr: 0.005\n",
      "iteration: 40100 loss: 0.0066 lr: 0.005\n",
      "iteration: 40200 loss: 0.0049 lr: 0.005\n",
      "iteration: 40300 loss: 0.0050 lr: 0.005\n",
      "iteration: 40400 loss: 0.0058 lr: 0.005\n",
      "iteration: 40500 loss: 0.0049 lr: 0.005\n",
      "iteration: 40600 loss: 0.0051 lr: 0.005\n",
      "iteration: 40700 loss: 0.0054 lr: 0.005\n",
      "iteration: 40800 loss: 0.0057 lr: 0.005\n",
      "iteration: 40900 loss: 0.0056 lr: 0.005\n",
      "iteration: 41000 loss: 0.0051 lr: 0.005\n",
      "iteration: 41100 loss: 0.0045 lr: 0.005\n",
      "iteration: 41200 loss: 0.0064 lr: 0.005\n",
      "iteration: 41300 loss: 0.0056 lr: 0.005\n",
      "iteration: 41400 loss: 0.0040 lr: 0.005\n",
      "iteration: 41500 loss: 0.0047 lr: 0.005\n",
      "iteration: 41600 loss: 0.0052 lr: 0.005\n",
      "iteration: 41700 loss: 0.0051 lr: 0.005\n",
      "iteration: 41800 loss: 0.0047 lr: 0.005\n",
      "iteration: 41900 loss: 0.0052 lr: 0.005\n",
      "iteration: 42000 loss: 0.0056 lr: 0.005\n",
      "iteration: 42100 loss: 0.0052 lr: 0.005\n",
      "iteration: 42200 loss: 0.0054 lr: 0.005\n",
      "iteration: 42300 loss: 0.0049 lr: 0.005\n",
      "iteration: 42400 loss: 0.0050 lr: 0.005\n",
      "iteration: 42500 loss: 0.0047 lr: 0.005\n",
      "iteration: 42600 loss: 0.0058 lr: 0.005\n",
      "iteration: 42700 loss: 0.0054 lr: 0.005\n",
      "iteration: 42800 loss: 0.0045 lr: 0.005\n",
      "iteration: 42900 loss: 0.0052 lr: 0.005\n",
      "iteration: 43000 loss: 0.0049 lr: 0.005\n",
      "iteration: 43100 loss: 0.0047 lr: 0.005\n",
      "iteration: 43200 loss: 0.0053 lr: 0.005\n",
      "iteration: 43300 loss: 0.0053 lr: 0.005\n",
      "iteration: 43400 loss: 0.0051 lr: 0.005\n",
      "iteration: 43500 loss: 0.0052 lr: 0.005\n",
      "iteration: 43600 loss: 0.0048 lr: 0.005\n",
      "iteration: 43700 loss: 0.0042 lr: 0.005\n",
      "iteration: 43800 loss: 0.0047 lr: 0.005\n",
      "iteration: 43900 loss: 0.0045 lr: 0.005\n",
      "iteration: 44000 loss: 0.0045 lr: 0.005\n",
      "iteration: 44100 loss: 0.0046 lr: 0.005\n",
      "iteration: 44200 loss: 0.0054 lr: 0.005\n",
      "iteration: 44300 loss: 0.0044 lr: 0.005\n",
      "iteration: 44400 loss: 0.0049 lr: 0.005\n",
      "iteration: 44500 loss: 0.0041 lr: 0.005\n",
      "iteration: 44600 loss: 0.0047 lr: 0.005\n",
      "iteration: 44700 loss: 0.0049 lr: 0.005\n",
      "iteration: 44800 loss: 0.0043 lr: 0.005\n",
      "iteration: 44900 loss: 0.0057 lr: 0.005\n",
      "iteration: 45000 loss: 0.0049 lr: 0.005\n",
      "iteration: 45100 loss: 0.0051 lr: 0.005\n",
      "iteration: 45200 loss: 0.0049 lr: 0.005\n",
      "iteration: 45300 loss: 0.0041 lr: 0.005\n",
      "iteration: 45400 loss: 0.0044 lr: 0.005\n",
      "iteration: 45500 loss: 0.0044 lr: 0.005\n",
      "iteration: 45600 loss: 0.0046 lr: 0.005\n",
      "iteration: 45700 loss: 0.0051 lr: 0.005\n",
      "iteration: 45800 loss: 0.0047 lr: 0.005\n",
      "iteration: 45900 loss: 0.0049 lr: 0.005\n",
      "iteration: 46000 loss: 0.0047 lr: 0.005\n",
      "iteration: 46100 loss: 0.0048 lr: 0.005\n",
      "iteration: 46200 loss: 0.0044 lr: 0.005\n",
      "iteration: 46300 loss: 0.0040 lr: 0.005\n",
      "iteration: 46400 loss: 0.0047 lr: 0.005\n",
      "iteration: 46500 loss: 0.0046 lr: 0.005\n",
      "iteration: 46600 loss: 0.0049 lr: 0.005\n",
      "iteration: 46700 loss: 0.0045 lr: 0.005\n",
      "iteration: 46800 loss: 0.0048 lr: 0.005\n",
      "iteration: 46900 loss: 0.0047 lr: 0.005\n",
      "iteration: 47000 loss: 0.0050 lr: 0.005\n",
      "iteration: 47100 loss: 0.0047 lr: 0.005\n",
      "iteration: 47200 loss: 0.0044 lr: 0.005\n",
      "iteration: 47300 loss: 0.0053 lr: 0.005\n",
      "iteration: 47400 loss: 0.0047 lr: 0.005\n",
      "iteration: 47500 loss: 0.0055 lr: 0.005\n",
      "iteration: 47600 loss: 0.0044 lr: 0.005\n",
      "iteration: 47700 loss: 0.0044 lr: 0.005\n",
      "iteration: 47800 loss: 0.0049 lr: 0.005\n",
      "iteration: 47900 loss: 0.0051 lr: 0.005\n",
      "iteration: 48000 loss: 0.0049 lr: 0.005\n",
      "iteration: 48100 loss: 0.0056 lr: 0.005\n",
      "iteration: 48200 loss: 0.0044 lr: 0.005\n",
      "iteration: 48300 loss: 0.0043 lr: 0.005\n",
      "iteration: 48400 loss: 0.0045 lr: 0.005\n",
      "iteration: 48500 loss: 0.0053 lr: 0.005\n",
      "iteration: 48600 loss: 0.0048 lr: 0.005\n",
      "iteration: 48700 loss: 0.0050 lr: 0.005\n",
      "iteration: 48800 loss: 0.0051 lr: 0.005\n",
      "iteration: 48900 loss: 0.0041 lr: 0.005\n",
      "iteration: 49000 loss: 0.0059 lr: 0.005\n",
      "iteration: 49100 loss: 0.0046 lr: 0.005\n",
      "iteration: 49200 loss: 0.0044 lr: 0.005\n",
      "iteration: 49300 loss: 0.0046 lr: 0.005\n",
      "iteration: 49400 loss: 0.0050 lr: 0.005\n",
      "iteration: 49500 loss: 0.0062 lr: 0.005\n",
      "iteration: 49600 loss: 0.0046 lr: 0.005\n",
      "iteration: 49700 loss: 0.0048 lr: 0.005\n",
      "iteration: 49800 loss: 0.0045 lr: 0.005\n",
      "iteration: 49900 loss: 0.0044 lr: 0.005\n",
      "iteration: 50000 loss: 0.0043 lr: 0.005\n",
      "iteration: 50100 loss: 0.0050 lr: 0.005\n",
      "iteration: 50200 loss: 0.0041 lr: 0.005\n",
      "iteration: 50300 loss: 0.0044 lr: 0.005\n",
      "iteration: 50400 loss: 0.0049 lr: 0.005\n",
      "iteration: 50500 loss: 0.0047 lr: 0.005\n",
      "iteration: 50600 loss: 0.0045 lr: 0.005\n",
      "iteration: 50700 loss: 0.0048 lr: 0.005\n",
      "iteration: 50800 loss: 0.0051 lr: 0.005\n",
      "iteration: 50900 loss: 0.0045 lr: 0.005\n",
      "iteration: 51000 loss: 0.0054 lr: 0.005\n",
      "iteration: 51100 loss: 0.0047 lr: 0.005\n",
      "iteration: 51200 loss: 0.0047 lr: 0.005\n",
      "iteration: 51300 loss: 0.0044 lr: 0.005\n",
      "iteration: 51400 loss: 0.0041 lr: 0.005\n",
      "iteration: 51500 loss: 0.0041 lr: 0.005\n",
      "iteration: 51600 loss: 0.0049 lr: 0.005\n",
      "iteration: 51700 loss: 0.0045 lr: 0.005\n",
      "iteration: 51800 loss: 0.0043 lr: 0.005\n",
      "iteration: 51900 loss: 0.0043 lr: 0.005\n",
      "iteration: 52000 loss: 0.0049 lr: 0.005\n",
      "iteration: 52100 loss: 0.0043 lr: 0.005\n",
      "iteration: 52200 loss: 0.0054 lr: 0.005\n",
      "iteration: 52300 loss: 0.0047 lr: 0.005\n",
      "iteration: 52400 loss: 0.0042 lr: 0.005\n",
      "iteration: 52500 loss: 0.0052 lr: 0.005\n",
      "iteration: 52600 loss: 0.0047 lr: 0.005\n",
      "iteration: 52700 loss: 0.0048 lr: 0.005\n",
      "iteration: 52800 loss: 0.0048 lr: 0.005\n",
      "iteration: 52900 loss: 0.0043 lr: 0.005\n",
      "iteration: 53000 loss: 0.0050 lr: 0.005\n",
      "iteration: 53100 loss: 0.0043 lr: 0.005\n",
      "iteration: 53200 loss: 0.0049 lr: 0.005\n",
      "iteration: 53300 loss: 0.0045 lr: 0.005\n",
      "iteration: 53400 loss: 0.0043 lr: 0.005\n",
      "iteration: 53500 loss: 0.0046 lr: 0.005\n",
      "iteration: 53600 loss: 0.0043 lr: 0.005\n",
      "iteration: 53700 loss: 0.0045 lr: 0.005\n",
      "iteration: 53800 loss: 0.0048 lr: 0.005\n",
      "iteration: 53900 loss: 0.0044 lr: 0.005\n",
      "iteration: 54000 loss: 0.0048 lr: 0.005\n",
      "iteration: 54100 loss: 0.0044 lr: 0.005\n",
      "iteration: 54200 loss: 0.0041 lr: 0.005\n",
      "iteration: 54300 loss: 0.0041 lr: 0.005\n",
      "iteration: 54400 loss: 0.0043 lr: 0.005\n",
      "iteration: 54500 loss: 0.0045 lr: 0.005\n",
      "iteration: 54600 loss: 0.0048 lr: 0.005\n",
      "iteration: 54700 loss: 0.0041 lr: 0.005\n",
      "iteration: 54800 loss: 0.0042 lr: 0.005\n",
      "iteration: 54900 loss: 0.0049 lr: 0.005\n",
      "iteration: 55000 loss: 0.0047 lr: 0.005\n",
      "iteration: 55100 loss: 0.0045 lr: 0.005\n",
      "iteration: 55200 loss: 0.0049 lr: 0.005\n",
      "iteration: 55300 loss: 0.0044 lr: 0.005\n",
      "iteration: 55400 loss: 0.0044 lr: 0.005\n",
      "iteration: 55500 loss: 0.0047 lr: 0.005\n",
      "iteration: 55600 loss: 0.0045 lr: 0.005\n",
      "iteration: 55700 loss: 0.0041 lr: 0.005\n",
      "iteration: 55800 loss: 0.0045 lr: 0.005\n",
      "iteration: 55900 loss: 0.0040 lr: 0.005\n",
      "iteration: 56000 loss: 0.0045 lr: 0.005\n",
      "iteration: 56100 loss: 0.0055 lr: 0.005\n",
      "iteration: 56200 loss: 0.0043 lr: 0.005\n",
      "iteration: 56300 loss: 0.0036 lr: 0.005\n",
      "iteration: 56400 loss: 0.0044 lr: 0.005\n",
      "iteration: 56500 loss: 0.0046 lr: 0.005\n",
      "iteration: 56600 loss: 0.0040 lr: 0.005\n",
      "iteration: 56700 loss: 0.0044 lr: 0.005\n",
      "iteration: 56800 loss: 0.0046 lr: 0.005\n",
      "iteration: 56900 loss: 0.0039 lr: 0.005\n",
      "iteration: 57000 loss: 0.0053 lr: 0.005\n",
      "iteration: 57100 loss: 0.0050 lr: 0.005\n",
      "iteration: 57200 loss: 0.0046 lr: 0.005\n",
      "iteration: 57300 loss: 0.0039 lr: 0.005\n",
      "iteration: 57400 loss: 0.0049 lr: 0.005\n",
      "iteration: 57500 loss: 0.0041 lr: 0.005\n",
      "iteration: 57600 loss: 0.0042 lr: 0.005\n",
      "iteration: 57700 loss: 0.0050 lr: 0.005\n",
      "iteration: 57800 loss: 0.0036 lr: 0.005\n",
      "iteration: 57900 loss: 0.0048 lr: 0.005\n",
      "iteration: 58000 loss: 0.0049 lr: 0.005\n",
      "iteration: 58100 loss: 0.0047 lr: 0.005\n",
      "iteration: 58200 loss: 0.0049 lr: 0.005\n",
      "iteration: 58300 loss: 0.0045 lr: 0.005\n",
      "iteration: 58400 loss: 0.0043 lr: 0.005\n",
      "iteration: 58500 loss: 0.0036 lr: 0.005\n",
      "iteration: 58600 loss: 0.0044 lr: 0.005\n",
      "iteration: 58700 loss: 0.0038 lr: 0.005\n",
      "iteration: 58800 loss: 0.0039 lr: 0.005\n",
      "iteration: 58900 loss: 0.0045 lr: 0.005\n",
      "iteration: 59000 loss: 0.0047 lr: 0.005\n",
      "iteration: 59100 loss: 0.0042 lr: 0.005\n",
      "iteration: 59200 loss: 0.0036 lr: 0.005\n",
      "iteration: 59300 loss: 0.0042 lr: 0.005\n",
      "iteration: 59400 loss: 0.0041 lr: 0.005\n",
      "iteration: 59500 loss: 0.0037 lr: 0.005\n",
      "iteration: 59600 loss: 0.0054 lr: 0.005\n",
      "iteration: 59700 loss: 0.0040 lr: 0.005\n",
      "iteration: 59800 loss: 0.0045 lr: 0.005\n",
      "iteration: 59900 loss: 0.0043 lr: 0.005\n",
      "iteration: 60000 loss: 0.0043 lr: 0.005\n",
      "iteration: 60100 loss: 0.0042 lr: 0.005\n",
      "iteration: 60200 loss: 0.0048 lr: 0.005\n",
      "iteration: 60300 loss: 0.0040 lr: 0.005\n",
      "iteration: 60400 loss: 0.0047 lr: 0.005\n",
      "iteration: 60500 loss: 0.0040 lr: 0.005\n",
      "iteration: 60600 loss: 0.0043 lr: 0.005\n",
      "iteration: 60700 loss: 0.0044 lr: 0.005\n",
      "iteration: 60800 loss: 0.0036 lr: 0.005\n",
      "iteration: 60900 loss: 0.0046 lr: 0.005\n",
      "iteration: 61000 loss: 0.0041 lr: 0.005\n",
      "iteration: 61100 loss: 0.0045 lr: 0.005\n",
      "iteration: 61200 loss: 0.0040 lr: 0.005\n",
      "iteration: 61300 loss: 0.0039 lr: 0.005\n",
      "iteration: 61400 loss: 0.0045 lr: 0.005\n",
      "iteration: 61500 loss: 0.0046 lr: 0.005\n",
      "iteration: 61600 loss: 0.0044 lr: 0.005\n",
      "iteration: 61700 loss: 0.0050 lr: 0.005\n",
      "iteration: 61800 loss: 0.0044 lr: 0.005\n",
      "iteration: 61900 loss: 0.0045 lr: 0.005\n",
      "iteration: 62000 loss: 0.0049 lr: 0.005\n",
      "iteration: 62100 loss: 0.0040 lr: 0.005\n",
      "iteration: 62200 loss: 0.0044 lr: 0.005\n",
      "iteration: 62300 loss: 0.0039 lr: 0.005\n",
      "iteration: 62400 loss: 0.0045 lr: 0.005\n",
      "iteration: 62500 loss: 0.0040 lr: 0.005\n",
      "iteration: 62600 loss: 0.0038 lr: 0.005\n",
      "iteration: 62700 loss: 0.0043 lr: 0.005\n",
      "iteration: 62800 loss: 0.0041 lr: 0.005\n",
      "iteration: 62900 loss: 0.0048 lr: 0.005\n",
      "iteration: 63000 loss: 0.0040 lr: 0.005\n",
      "iteration: 63100 loss: 0.0038 lr: 0.005\n",
      "iteration: 63200 loss: 0.0039 lr: 0.005\n",
      "iteration: 63300 loss: 0.0048 lr: 0.005\n",
      "iteration: 63400 loss: 0.0042 lr: 0.005\n",
      "iteration: 63500 loss: 0.0040 lr: 0.005\n",
      "iteration: 63600 loss: 0.0047 lr: 0.005\n",
      "iteration: 63700 loss: 0.0043 lr: 0.005\n",
      "iteration: 63800 loss: 0.0045 lr: 0.005\n",
      "iteration: 63900 loss: 0.0039 lr: 0.005\n",
      "iteration: 64000 loss: 0.0036 lr: 0.005\n",
      "iteration: 64100 loss: 0.0041 lr: 0.005\n",
      "iteration: 64200 loss: 0.0041 lr: 0.005\n",
      "iteration: 64300 loss: 0.0038 lr: 0.005\n",
      "iteration: 64400 loss: 0.0048 lr: 0.005\n",
      "iteration: 64500 loss: 0.0044 lr: 0.005\n",
      "iteration: 64600 loss: 0.0039 lr: 0.005\n",
      "iteration: 64700 loss: 0.0038 lr: 0.005\n",
      "iteration: 64800 loss: 0.0043 lr: 0.005\n",
      "iteration: 64900 loss: 0.0042 lr: 0.005\n",
      "iteration: 65000 loss: 0.0044 lr: 0.005\n",
      "iteration: 65100 loss: 0.0036 lr: 0.005\n",
      "iteration: 65200 loss: 0.0043 lr: 0.005\n",
      "iteration: 65300 loss: 0.0046 lr: 0.005\n",
      "iteration: 65400 loss: 0.0044 lr: 0.005\n",
      "iteration: 65500 loss: 0.0047 lr: 0.005\n",
      "iteration: 65600 loss: 0.0039 lr: 0.005\n",
      "iteration: 65700 loss: 0.0039 lr: 0.005\n",
      "iteration: 65800 loss: 0.0044 lr: 0.005\n",
      "iteration: 65900 loss: 0.0044 lr: 0.005\n",
      "iteration: 66000 loss: 0.0041 lr: 0.005\n",
      "iteration: 66100 loss: 0.0035 lr: 0.005\n",
      "iteration: 66200 loss: 0.0044 lr: 0.005\n",
      "iteration: 66300 loss: 0.0038 lr: 0.005\n",
      "iteration: 66400 loss: 0.0039 lr: 0.005\n",
      "iteration: 66500 loss: 0.0041 lr: 0.005\n",
      "iteration: 66600 loss: 0.0041 lr: 0.005\n",
      "iteration: 66700 loss: 0.0037 lr: 0.005\n",
      "iteration: 66800 loss: 0.0040 lr: 0.005\n",
      "iteration: 66900 loss: 0.0037 lr: 0.005\n",
      "iteration: 67000 loss: 0.0040 lr: 0.005\n",
      "iteration: 67100 loss: 0.0040 lr: 0.005\n",
      "iteration: 67200 loss: 0.0035 lr: 0.005\n",
      "iteration: 67300 loss: 0.0049 lr: 0.005\n",
      "iteration: 67400 loss: 0.0039 lr: 0.005\n",
      "iteration: 67500 loss: 0.0041 lr: 0.005\n",
      "iteration: 67600 loss: 0.0042 lr: 0.005\n",
      "iteration: 67700 loss: 0.0039 lr: 0.005\n",
      "iteration: 67800 loss: 0.0038 lr: 0.005\n",
      "iteration: 67900 loss: 0.0041 lr: 0.005\n",
      "iteration: 68000 loss: 0.0042 lr: 0.005\n",
      "iteration: 68100 loss: 0.0042 lr: 0.005\n",
      "iteration: 68200 loss: 0.0048 lr: 0.005\n",
      "iteration: 68300 loss: 0.0040 lr: 0.005\n",
      "iteration: 68400 loss: 0.0034 lr: 0.005\n",
      "iteration: 68500 loss: 0.0042 lr: 0.005\n",
      "iteration: 68600 loss: 0.0039 lr: 0.005\n",
      "iteration: 68700 loss: 0.0036 lr: 0.005\n",
      "iteration: 68800 loss: 0.0037 lr: 0.005\n",
      "iteration: 68900 loss: 0.0042 lr: 0.005\n",
      "iteration: 69000 loss: 0.0036 lr: 0.005\n",
      "iteration: 69100 loss: 0.0035 lr: 0.005\n",
      "iteration: 69200 loss: 0.0043 lr: 0.005\n",
      "iteration: 69300 loss: 0.0046 lr: 0.005\n",
      "iteration: 69400 loss: 0.0040 lr: 0.005\n",
      "iteration: 69500 loss: 0.0036 lr: 0.005\n",
      "iteration: 69600 loss: 0.0041 lr: 0.005\n",
      "iteration: 69700 loss: 0.0038 lr: 0.005\n",
      "iteration: 69800 loss: 0.0039 lr: 0.005\n",
      "iteration: 69900 loss: 0.0042 lr: 0.005\n",
      "iteration: 70000 loss: 0.0036 lr: 0.005\n",
      "iteration: 70100 loss: 0.0036 lr: 0.005\n",
      "iteration: 70200 loss: 0.0045 lr: 0.005\n",
      "iteration: 70300 loss: 0.0041 lr: 0.005\n",
      "iteration: 70400 loss: 0.0044 lr: 0.005\n",
      "iteration: 70500 loss: 0.0043 lr: 0.005\n",
      "iteration: 70600 loss: 0.0037 lr: 0.005\n",
      "iteration: 70700 loss: 0.0041 lr: 0.005\n",
      "iteration: 70800 loss: 0.0040 lr: 0.005\n",
      "iteration: 70900 loss: 0.0034 lr: 0.005\n",
      "iteration: 71000 loss: 0.0037 lr: 0.005\n",
      "iteration: 71100 loss: 0.0037 lr: 0.005\n",
      "iteration: 71200 loss: 0.0043 lr: 0.005\n",
      "iteration: 71300 loss: 0.0041 lr: 0.005\n",
      "iteration: 71400 loss: 0.0040 lr: 0.005\n",
      "iteration: 71500 loss: 0.0039 lr: 0.005\n",
      "iteration: 71600 loss: 0.0035 lr: 0.005\n",
      "iteration: 71700 loss: 0.0049 lr: 0.005\n",
      "iteration: 71800 loss: 0.0042 lr: 0.005\n",
      "iteration: 71900 loss: 0.0038 lr: 0.005\n",
      "iteration: 72000 loss: 0.0035 lr: 0.005\n",
      "iteration: 72100 loss: 0.0036 lr: 0.005\n",
      "iteration: 72200 loss: 0.0040 lr: 0.005\n",
      "iteration: 72300 loss: 0.0040 lr: 0.005\n",
      "iteration: 72400 loss: 0.0039 lr: 0.005\n",
      "iteration: 72500 loss: 0.0038 lr: 0.005\n",
      "iteration: 72600 loss: 0.0039 lr: 0.005\n",
      "iteration: 72700 loss: 0.0043 lr: 0.005\n",
      "iteration: 72800 loss: 0.0040 lr: 0.005\n",
      "iteration: 72900 loss: 0.0040 lr: 0.005\n",
      "iteration: 73000 loss: 0.0035 lr: 0.005\n",
      "iteration: 73100 loss: 0.0041 lr: 0.005\n",
      "iteration: 73200 loss: 0.0034 lr: 0.005\n",
      "iteration: 73300 loss: 0.0042 lr: 0.005\n",
      "iteration: 73400 loss: 0.0040 lr: 0.005\n",
      "iteration: 73500 loss: 0.0039 lr: 0.005\n",
      "iteration: 73600 loss: 0.0037 lr: 0.005\n",
      "iteration: 73700 loss: 0.0043 lr: 0.005\n",
      "iteration: 73800 loss: 0.0045 lr: 0.005\n",
      "iteration: 73900 loss: 0.0040 lr: 0.005\n",
      "iteration: 74000 loss: 0.0044 lr: 0.005\n",
      "iteration: 74100 loss: 0.0042 lr: 0.005\n",
      "iteration: 74200 loss: 0.0040 lr: 0.005\n",
      "iteration: 74300 loss: 0.0039 lr: 0.005\n",
      "iteration: 74400 loss: 0.0045 lr: 0.005\n",
      "iteration: 74500 loss: 0.0045 lr: 0.005\n",
      "iteration: 74600 loss: 0.0037 lr: 0.005\n",
      "iteration: 74700 loss: 0.0039 lr: 0.005\n",
      "iteration: 74800 loss: 0.0043 lr: 0.005\n",
      "iteration: 74900 loss: 0.0034 lr: 0.005\n",
      "iteration: 75000 loss: 0.0033 lr: 0.005\n",
      "iteration: 75100 loss: 0.0043 lr: 0.005\n",
      "iteration: 75200 loss: 0.0044 lr: 0.005\n",
      "iteration: 75300 loss: 0.0040 lr: 0.005\n",
      "iteration: 75400 loss: 0.0036 lr: 0.005\n",
      "iteration: 75500 loss: 0.0033 lr: 0.005\n",
      "iteration: 75600 loss: 0.0036 lr: 0.005\n",
      "iteration: 75700 loss: 0.0036 lr: 0.005\n",
      "iteration: 75800 loss: 0.0040 lr: 0.005\n",
      "iteration: 75900 loss: 0.0036 lr: 0.005\n",
      "iteration: 76000 loss: 0.0044 lr: 0.005\n",
      "iteration: 76100 loss: 0.0036 lr: 0.005\n",
      "iteration: 76200 loss: 0.0032 lr: 0.005\n",
      "iteration: 76300 loss: 0.0030 lr: 0.005\n",
      "iteration: 76400 loss: 0.0048 lr: 0.005\n",
      "iteration: 76500 loss: 0.0034 lr: 0.005\n",
      "iteration: 76600 loss: 0.0034 lr: 0.005\n",
      "iteration: 76700 loss: 0.0034 lr: 0.005\n",
      "iteration: 76800 loss: 0.0037 lr: 0.005\n",
      "iteration: 76900 loss: 0.0041 lr: 0.005\n",
      "iteration: 77000 loss: 0.0030 lr: 0.005\n",
      "iteration: 77100 loss: 0.0038 lr: 0.005\n",
      "iteration: 77200 loss: 0.0031 lr: 0.005\n",
      "iteration: 77300 loss: 0.0037 lr: 0.005\n",
      "iteration: 77400 loss: 0.0036 lr: 0.005\n",
      "iteration: 77500 loss: 0.0041 lr: 0.005\n",
      "iteration: 77600 loss: 0.0044 lr: 0.005\n",
      "iteration: 77700 loss: 0.0036 lr: 0.005\n",
      "iteration: 77800 loss: 0.0039 lr: 0.005\n",
      "iteration: 77900 loss: 0.0036 lr: 0.005\n",
      "iteration: 78000 loss: 0.0042 lr: 0.005\n",
      "iteration: 78100 loss: 0.0036 lr: 0.005\n",
      "iteration: 78200 loss: 0.0039 lr: 0.005\n",
      "iteration: 78300 loss: 0.0044 lr: 0.005\n",
      "iteration: 78400 loss: 0.0045 lr: 0.005\n",
      "iteration: 78500 loss: 0.0043 lr: 0.005\n",
      "iteration: 78600 loss: 0.0042 lr: 0.005\n",
      "iteration: 78700 loss: 0.0039 lr: 0.005\n",
      "iteration: 78800 loss: 0.0038 lr: 0.005\n",
      "iteration: 78900 loss: 0.0042 lr: 0.005\n",
      "iteration: 79000 loss: 0.0037 lr: 0.005\n",
      "iteration: 79100 loss: 0.0037 lr: 0.005\n",
      "iteration: 79200 loss: 0.0045 lr: 0.005\n",
      "iteration: 79300 loss: 0.0038 lr: 0.005\n",
      "iteration: 79400 loss: 0.0033 lr: 0.005\n",
      "iteration: 79500 loss: 0.0040 lr: 0.005\n",
      "iteration: 79600 loss: 0.0036 lr: 0.005\n",
      "iteration: 79700 loss: 0.0044 lr: 0.005\n",
      "iteration: 79800 loss: 0.0037 lr: 0.005\n",
      "iteration: 79900 loss: 0.0038 lr: 0.005\n",
      "iteration: 80000 loss: 0.0038 lr: 0.005\n",
      "iteration: 80100 loss: 0.0038 lr: 0.005\n",
      "iteration: 80200 loss: 0.0040 lr: 0.005\n",
      "iteration: 80300 loss: 0.0038 lr: 0.005\n",
      "iteration: 80400 loss: 0.0035 lr: 0.005\n",
      "iteration: 80500 loss: 0.0037 lr: 0.005\n",
      "iteration: 80600 loss: 0.0030 lr: 0.005\n",
      "iteration: 80700 loss: 0.0036 lr: 0.005\n",
      "iteration: 80800 loss: 0.0039 lr: 0.005\n",
      "iteration: 80900 loss: 0.0038 lr: 0.005\n",
      "iteration: 81000 loss: 0.0033 lr: 0.005\n",
      "iteration: 81100 loss: 0.0036 lr: 0.005\n",
      "iteration: 81200 loss: 0.0037 lr: 0.005\n",
      "iteration: 81300 loss: 0.0030 lr: 0.005\n",
      "iteration: 81400 loss: 0.0036 lr: 0.005\n",
      "iteration: 81500 loss: 0.0038 lr: 0.005\n",
      "iteration: 81600 loss: 0.0038 lr: 0.005\n",
      "iteration: 81700 loss: 0.0036 lr: 0.005\n",
      "iteration: 81800 loss: 0.0034 lr: 0.005\n",
      "iteration: 81900 loss: 0.0029 lr: 0.005\n",
      "iteration: 82000 loss: 0.0044 lr: 0.005\n",
      "iteration: 82100 loss: 0.0036 lr: 0.005\n",
      "iteration: 82200 loss: 0.0041 lr: 0.005\n",
      "iteration: 82300 loss: 0.0034 lr: 0.005\n",
      "iteration: 82400 loss: 0.0037 lr: 0.005\n",
      "iteration: 82500 loss: 0.0038 lr: 0.005\n",
      "iteration: 82600 loss: 0.0044 lr: 0.005\n",
      "iteration: 82700 loss: 0.0033 lr: 0.005\n",
      "iteration: 82800 loss: 0.0035 lr: 0.005\n",
      "iteration: 82900 loss: 0.0038 lr: 0.005\n",
      "iteration: 83000 loss: 0.0040 lr: 0.005\n",
      "iteration: 83100 loss: 0.0030 lr: 0.005\n",
      "iteration: 83200 loss: 0.0039 lr: 0.005\n",
      "iteration: 83300 loss: 0.0032 lr: 0.005\n",
      "iteration: 83400 loss: 0.0034 lr: 0.005\n",
      "iteration: 83500 loss: 0.0034 lr: 0.005\n",
      "iteration: 83600 loss: 0.0035 lr: 0.005\n",
      "iteration: 83700 loss: 0.0038 lr: 0.005\n",
      "iteration: 83800 loss: 0.0029 lr: 0.005\n",
      "iteration: 83900 loss: 0.0038 lr: 0.005\n",
      "iteration: 84000 loss: 0.0036 lr: 0.005\n",
      "iteration: 84100 loss: 0.0035 lr: 0.005\n",
      "iteration: 84200 loss: 0.0034 lr: 0.005\n",
      "iteration: 84300 loss: 0.0036 lr: 0.005\n",
      "iteration: 84400 loss: 0.0037 lr: 0.005\n",
      "iteration: 84500 loss: 0.0038 lr: 0.005\n",
      "iteration: 84600 loss: 0.0036 lr: 0.005\n",
      "iteration: 84700 loss: 0.0034 lr: 0.005\n",
      "iteration: 84800 loss: 0.0038 lr: 0.005\n",
      "iteration: 84900 loss: 0.0040 lr: 0.005\n",
      "iteration: 85000 loss: 0.0035 lr: 0.005\n",
      "iteration: 85100 loss: 0.0040 lr: 0.005\n",
      "iteration: 85200 loss: 0.0036 lr: 0.005\n",
      "iteration: 85300 loss: 0.0029 lr: 0.005\n",
      "iteration: 85400 loss: 0.0038 lr: 0.005\n",
      "iteration: 85500 loss: 0.0034 lr: 0.005\n",
      "iteration: 85600 loss: 0.0040 lr: 0.005\n",
      "iteration: 85700 loss: 0.0041 lr: 0.005\n",
      "iteration: 85800 loss: 0.0039 lr: 0.005\n",
      "iteration: 85900 loss: 0.0043 lr: 0.005\n",
      "iteration: 86000 loss: 0.0036 lr: 0.005\n",
      "iteration: 86100 loss: 0.0031 lr: 0.005\n",
      "iteration: 86200 loss: 0.0042 lr: 0.005\n",
      "iteration: 86300 loss: 0.0034 lr: 0.005\n",
      "iteration: 86400 loss: 0.0040 lr: 0.005\n",
      "iteration: 86500 loss: 0.0040 lr: 0.005\n",
      "iteration: 86600 loss: 0.0039 lr: 0.005\n",
      "iteration: 86700 loss: 0.0034 lr: 0.005\n",
      "iteration: 86800 loss: 0.0035 lr: 0.005\n",
      "iteration: 86900 loss: 0.0033 lr: 0.005\n",
      "iteration: 87000 loss: 0.0039 lr: 0.005\n",
      "iteration: 87100 loss: 0.0044 lr: 0.005\n",
      "iteration: 87200 loss: 0.0038 lr: 0.005\n",
      "iteration: 87300 loss: 0.0037 lr: 0.005\n",
      "iteration: 87400 loss: 0.0039 lr: 0.005\n",
      "iteration: 87500 loss: 0.0035 lr: 0.005\n",
      "iteration: 87600 loss: 0.0034 lr: 0.005\n",
      "iteration: 87700 loss: 0.0039 lr: 0.005\n",
      "iteration: 87800 loss: 0.0039 lr: 0.005\n",
      "iteration: 87900 loss: 0.0035 lr: 0.005\n",
      "iteration: 88000 loss: 0.0032 lr: 0.005\n",
      "iteration: 88100 loss: 0.0035 lr: 0.005\n",
      "iteration: 88200 loss: 0.0033 lr: 0.005\n",
      "iteration: 88300 loss: 0.0045 lr: 0.005\n",
      "iteration: 88400 loss: 0.0033 lr: 0.005\n",
      "iteration: 88500 loss: 0.0039 lr: 0.005\n",
      "iteration: 88600 loss: 0.0036 lr: 0.005\n",
      "iteration: 88700 loss: 0.0035 lr: 0.005\n",
      "iteration: 88800 loss: 0.0035 lr: 0.005\n",
      "iteration: 88900 loss: 0.0037 lr: 0.005\n",
      "iteration: 89000 loss: 0.0038 lr: 0.005\n",
      "iteration: 89100 loss: 0.0032 lr: 0.005\n",
      "iteration: 89200 loss: 0.0045 lr: 0.005\n",
      "iteration: 89300 loss: 0.0034 lr: 0.005\n",
      "iteration: 89400 loss: 0.0035 lr: 0.005\n",
      "iteration: 89500 loss: 0.0035 lr: 0.005\n",
      "iteration: 89600 loss: 0.0033 lr: 0.005\n",
      "iteration: 89700 loss: 0.0042 lr: 0.005\n",
      "iteration: 89800 loss: 0.0031 lr: 0.005\n",
      "iteration: 89900 loss: 0.0034 lr: 0.005\n",
      "iteration: 90000 loss: 0.0040 lr: 0.005\n",
      "iteration: 90100 loss: 0.0038 lr: 0.005\n",
      "iteration: 90200 loss: 0.0031 lr: 0.005\n",
      "iteration: 90300 loss: 0.0036 lr: 0.005\n",
      "iteration: 90400 loss: 0.0033 lr: 0.005\n",
      "iteration: 90500 loss: 0.0032 lr: 0.005\n",
      "iteration: 90600 loss: 0.0039 lr: 0.005\n",
      "iteration: 90700 loss: 0.0038 lr: 0.005\n",
      "iteration: 90800 loss: 0.0034 lr: 0.005\n",
      "iteration: 90900 loss: 0.0032 lr: 0.005\n",
      "iteration: 91000 loss: 0.0040 lr: 0.005\n",
      "iteration: 91100 loss: 0.0035 lr: 0.005\n",
      "iteration: 91200 loss: 0.0036 lr: 0.005\n",
      "iteration: 91300 loss: 0.0039 lr: 0.005\n",
      "iteration: 91400 loss: 0.0037 lr: 0.005\n",
      "iteration: 91500 loss: 0.0039 lr: 0.005\n",
      "iteration: 91600 loss: 0.0038 lr: 0.005\n",
      "iteration: 91700 loss: 0.0039 lr: 0.005\n",
      "iteration: 91800 loss: 0.0041 lr: 0.005\n",
      "iteration: 91900 loss: 0.0030 lr: 0.005\n",
      "iteration: 92000 loss: 0.0036 lr: 0.005\n",
      "iteration: 92100 loss: 0.0037 lr: 0.005\n",
      "iteration: 92200 loss: 0.0033 lr: 0.005\n",
      "iteration: 92300 loss: 0.0042 lr: 0.005\n",
      "iteration: 92400 loss: 0.0038 lr: 0.005\n",
      "iteration: 92500 loss: 0.0037 lr: 0.005\n",
      "iteration: 92600 loss: 0.0035 lr: 0.005\n",
      "iteration: 92700 loss: 0.0036 lr: 0.005\n",
      "iteration: 92800 loss: 0.0041 lr: 0.005\n",
      "iteration: 92900 loss: 0.0034 lr: 0.005\n",
      "iteration: 93000 loss: 0.0038 lr: 0.005\n",
      "iteration: 93100 loss: 0.0034 lr: 0.005\n",
      "iteration: 93200 loss: 0.0035 lr: 0.005\n",
      "iteration: 93300 loss: 0.0039 lr: 0.005\n",
      "iteration: 93400 loss: 0.0039 lr: 0.005\n",
      "iteration: 93500 loss: 0.0039 lr: 0.005\n",
      "iteration: 93600 loss: 0.0036 lr: 0.005\n",
      "iteration: 93700 loss: 0.0032 lr: 0.005\n",
      "iteration: 93800 loss: 0.0033 lr: 0.005\n",
      "iteration: 93900 loss: 0.0039 lr: 0.005\n",
      "iteration: 94000 loss: 0.0034 lr: 0.005\n",
      "iteration: 94100 loss: 0.0038 lr: 0.005\n",
      "iteration: 94200 loss: 0.0035 lr: 0.005\n",
      "iteration: 94300 loss: 0.0034 lr: 0.005\n",
      "iteration: 94400 loss: 0.0032 lr: 0.005\n",
      "iteration: 94500 loss: 0.0041 lr: 0.005\n",
      "iteration: 94600 loss: 0.0033 lr: 0.005\n",
      "iteration: 94700 loss: 0.0036 lr: 0.005\n",
      "iteration: 94800 loss: 0.0031 lr: 0.005\n",
      "iteration: 94900 loss: 0.0038 lr: 0.005\n",
      "iteration: 95000 loss: 0.0036 lr: 0.005\n",
      "iteration: 95100 loss: 0.0031 lr: 0.005\n",
      "iteration: 95200 loss: 0.0029 lr: 0.005\n",
      "iteration: 95300 loss: 0.0035 lr: 0.005\n",
      "iteration: 95400 loss: 0.0029 lr: 0.005\n",
      "iteration: 95500 loss: 0.0033 lr: 0.005\n",
      "iteration: 95600 loss: 0.0038 lr: 0.005\n",
      "iteration: 95700 loss: 0.0039 lr: 0.005\n",
      "iteration: 95800 loss: 0.0039 lr: 0.005\n",
      "iteration: 95900 loss: 0.0044 lr: 0.005\n",
      "iteration: 96000 loss: 0.0034 lr: 0.005\n",
      "iteration: 96100 loss: 0.0040 lr: 0.005\n",
      "iteration: 96200 loss: 0.0041 lr: 0.005\n",
      "iteration: 96300 loss: 0.0033 lr: 0.005\n",
      "iteration: 96400 loss: 0.0040 lr: 0.005\n",
      "iteration: 96500 loss: 0.0037 lr: 0.005\n",
      "iteration: 96600 loss: 0.0039 lr: 0.005\n",
      "iteration: 96700 loss: 0.0032 lr: 0.005\n",
      "iteration: 96800 loss: 0.0039 lr: 0.005\n",
      "iteration: 96900 loss: 0.0032 lr: 0.005\n",
      "iteration: 97000 loss: 0.0034 lr: 0.005\n",
      "iteration: 97100 loss: 0.0036 lr: 0.005\n",
      "iteration: 97200 loss: 0.0038 lr: 0.005\n",
      "iteration: 97300 loss: 0.0037 lr: 0.005\n",
      "iteration: 97400 loss: 0.0034 lr: 0.005\n",
      "iteration: 97500 loss: 0.0038 lr: 0.005\n",
      "iteration: 97600 loss: 0.0035 lr: 0.005\n",
      "iteration: 97700 loss: 0.0032 lr: 0.005\n",
      "iteration: 97800 loss: 0.0034 lr: 0.005\n",
      "iteration: 97900 loss: 0.0034 lr: 0.005\n",
      "iteration: 98000 loss: 0.0031 lr: 0.005\n",
      "iteration: 98100 loss: 0.0031 lr: 0.005\n",
      "iteration: 98200 loss: 0.0031 lr: 0.005\n",
      "iteration: 98300 loss: 0.0037 lr: 0.005\n",
      "iteration: 98400 loss: 0.0034 lr: 0.005\n",
      "iteration: 98500 loss: 0.0032 lr: 0.005\n",
      "iteration: 98600 loss: 0.0037 lr: 0.005\n",
      "iteration: 98700 loss: 0.0035 lr: 0.005\n",
      "iteration: 98800 loss: 0.0035 lr: 0.005\n",
      "iteration: 98900 loss: 0.0035 lr: 0.005\n",
      "iteration: 99000 loss: 0.0035 lr: 0.005\n",
      "iteration: 99100 loss: 0.0039 lr: 0.005\n",
      "iteration: 99200 loss: 0.0033 lr: 0.005\n",
      "iteration: 99300 loss: 0.0042 lr: 0.005\n",
      "iteration: 99400 loss: 0.0037 lr: 0.005\n",
      "iteration: 99500 loss: 0.0035 lr: 0.005\n",
      "iteration: 99600 loss: 0.0036 lr: 0.005\n",
      "iteration: 99700 loss: 0.0029 lr: 0.005\n",
      "iteration: 99800 loss: 0.0035 lr: 0.005\n",
      "iteration: 99900 loss: 0.0033 lr: 0.005\n",
      "iteration: 100000 loss: 0.0038 lr: 0.005\n",
      "iteration: 100100 loss: 0.0033 lr: 0.005\n",
      "iteration: 100200 loss: 0.0036 lr: 0.005\n",
      "iteration: 100300 loss: 0.0035 lr: 0.005\n",
      "iteration: 100400 loss: 0.0040 lr: 0.005\n",
      "iteration: 100500 loss: 0.0035 lr: 0.005\n",
      "iteration: 100600 loss: 0.0035 lr: 0.005\n",
      "iteration: 100700 loss: 0.0036 lr: 0.005\n",
      "iteration: 100800 loss: 0.0040 lr: 0.005\n",
      "iteration: 100900 loss: 0.0030 lr: 0.005\n",
      "iteration: 101000 loss: 0.0038 lr: 0.005\n",
      "iteration: 101100 loss: 0.0032 lr: 0.005\n",
      "iteration: 101200 loss: 0.0031 lr: 0.005\n",
      "iteration: 101300 loss: 0.0037 lr: 0.005\n",
      "iteration: 101400 loss: 0.0027 lr: 0.005\n",
      "iteration: 101500 loss: 0.0033 lr: 0.005\n",
      "iteration: 101600 loss: 0.0031 lr: 0.005\n",
      "iteration: 101700 loss: 0.0030 lr: 0.005\n",
      "iteration: 101800 loss: 0.0031 lr: 0.005\n",
      "iteration: 101900 loss: 0.0039 lr: 0.005\n",
      "iteration: 102000 loss: 0.0031 lr: 0.005\n",
      "iteration: 102100 loss: 0.0034 lr: 0.005\n",
      "iteration: 102200 loss: 0.0033 lr: 0.005\n",
      "iteration: 102300 loss: 0.0032 lr: 0.005\n",
      "iteration: 102400 loss: 0.0031 lr: 0.005\n",
      "iteration: 102500 loss: 0.0028 lr: 0.005\n",
      "iteration: 102600 loss: 0.0035 lr: 0.005\n",
      "iteration: 102700 loss: 0.0039 lr: 0.005\n",
      "iteration: 102800 loss: 0.0035 lr: 0.005\n",
      "iteration: 102900 loss: 0.0033 lr: 0.005\n",
      "iteration: 103000 loss: 0.0036 lr: 0.005\n",
      "iteration: 103100 loss: 0.0034 lr: 0.005\n",
      "iteration: 103200 loss: 0.0040 lr: 0.005\n",
      "iteration: 103300 loss: 0.0032 lr: 0.005\n",
      "iteration: 103400 loss: 0.0036 lr: 0.005\n",
      "iteration: 103500 loss: 0.0035 lr: 0.005\n",
      "iteration: 103600 loss: 0.0032 lr: 0.005\n",
      "iteration: 103700 loss: 0.0038 lr: 0.005\n",
      "iteration: 103800 loss: 0.0031 lr: 0.005\n",
      "iteration: 103900 loss: 0.0035 lr: 0.005\n",
      "iteration: 104000 loss: 0.0035 lr: 0.005\n",
      "iteration: 104100 loss: 0.0041 lr: 0.005\n",
      "iteration: 104200 loss: 0.0034 lr: 0.005\n",
      "iteration: 104300 loss: 0.0036 lr: 0.005\n",
      "iteration: 104400 loss: 0.0032 lr: 0.005\n",
      "iteration: 104500 loss: 0.0033 lr: 0.005\n",
      "iteration: 104600 loss: 0.0034 lr: 0.005\n",
      "iteration: 104700 loss: 0.0032 lr: 0.005\n",
      "iteration: 104800 loss: 0.0036 lr: 0.005\n",
      "iteration: 104900 loss: 0.0035 lr: 0.005\n",
      "iteration: 105000 loss: 0.0032 lr: 0.005\n",
      "iteration: 105100 loss: 0.0040 lr: 0.005\n",
      "iteration: 105200 loss: 0.0037 lr: 0.005\n",
      "iteration: 105300 loss: 0.0033 lr: 0.005\n",
      "iteration: 105400 loss: 0.0037 lr: 0.005\n",
      "iteration: 105500 loss: 0.0037 lr: 0.005\n",
      "iteration: 105600 loss: 0.0039 lr: 0.005\n",
      "iteration: 105700 loss: 0.0031 lr: 0.005\n",
      "iteration: 105800 loss: 0.0032 lr: 0.005\n",
      "iteration: 105900 loss: 0.0033 lr: 0.005\n",
      "iteration: 106000 loss: 0.0032 lr: 0.005\n",
      "iteration: 106100 loss: 0.0029 lr: 0.005\n",
      "iteration: 106200 loss: 0.0036 lr: 0.005\n",
      "iteration: 106300 loss: 0.0035 lr: 0.005\n",
      "iteration: 106400 loss: 0.0029 lr: 0.005\n",
      "iteration: 106500 loss: 0.0032 lr: 0.005\n",
      "iteration: 106600 loss: 0.0031 lr: 0.005\n",
      "iteration: 106700 loss: 0.0032 lr: 0.005\n",
      "iteration: 106800 loss: 0.0030 lr: 0.005\n",
      "iteration: 106900 loss: 0.0043 lr: 0.005\n",
      "iteration: 107000 loss: 0.0027 lr: 0.005\n",
      "iteration: 107100 loss: 0.0032 lr: 0.005\n",
      "iteration: 107200 loss: 0.0036 lr: 0.005\n",
      "iteration: 107300 loss: 0.0031 lr: 0.005\n",
      "iteration: 107400 loss: 0.0033 lr: 0.005\n",
      "iteration: 107500 loss: 0.0030 lr: 0.005\n",
      "iteration: 107600 loss: 0.0033 lr: 0.005\n",
      "iteration: 107700 loss: 0.0034 lr: 0.005\n",
      "iteration: 107800 loss: 0.0037 lr: 0.005\n",
      "iteration: 107900 loss: 0.0035 lr: 0.005\n",
      "iteration: 108000 loss: 0.0033 lr: 0.005\n",
      "iteration: 108100 loss: 0.0036 lr: 0.005\n",
      "iteration: 108200 loss: 0.0037 lr: 0.005\n",
      "iteration: 108300 loss: 0.0031 lr: 0.005\n",
      "iteration: 108400 loss: 0.0029 lr: 0.005\n",
      "iteration: 108500 loss: 0.0032 lr: 0.005\n",
      "iteration: 108600 loss: 0.0030 lr: 0.005\n",
      "iteration: 108700 loss: 0.0037 lr: 0.005\n",
      "iteration: 108800 loss: 0.0036 lr: 0.005\n",
      "iteration: 108900 loss: 0.0035 lr: 0.005\n",
      "iteration: 109000 loss: 0.0036 lr: 0.005\n",
      "iteration: 109100 loss: 0.0033 lr: 0.005\n",
      "iteration: 109200 loss: 0.0038 lr: 0.005\n",
      "iteration: 109300 loss: 0.0032 lr: 0.005\n",
      "iteration: 109400 loss: 0.0031 lr: 0.005\n",
      "iteration: 109500 loss: 0.0031 lr: 0.005\n",
      "iteration: 109600 loss: 0.0039 lr: 0.005\n",
      "iteration: 109700 loss: 0.0030 lr: 0.005\n",
      "iteration: 109800 loss: 0.0031 lr: 0.005\n",
      "iteration: 109900 loss: 0.0032 lr: 0.005\n",
      "iteration: 110000 loss: 0.0035 lr: 0.005\n",
      "iteration: 110100 loss: 0.0034 lr: 0.005\n",
      "iteration: 110200 loss: 0.0034 lr: 0.005\n",
      "iteration: 110300 loss: 0.0031 lr: 0.005\n",
      "iteration: 110400 loss: 0.0031 lr: 0.005\n",
      "iteration: 110500 loss: 0.0034 lr: 0.005\n",
      "iteration: 110600 loss: 0.0028 lr: 0.005\n",
      "iteration: 110700 loss: 0.0026 lr: 0.005\n",
      "iteration: 110800 loss: 0.0031 lr: 0.005\n",
      "iteration: 110900 loss: 0.0036 lr: 0.005\n",
      "iteration: 111000 loss: 0.0034 lr: 0.005\n",
      "iteration: 111100 loss: 0.0040 lr: 0.005\n",
      "iteration: 111200 loss: 0.0032 lr: 0.005\n",
      "iteration: 111300 loss: 0.0036 lr: 0.005\n",
      "iteration: 111400 loss: 0.0031 lr: 0.005\n",
      "iteration: 111500 loss: 0.0034 lr: 0.005\n",
      "iteration: 111600 loss: 0.0033 lr: 0.005\n",
      "iteration: 111700 loss: 0.0035 lr: 0.005\n",
      "iteration: 111800 loss: 0.0035 lr: 0.005\n",
      "iteration: 111900 loss: 0.0041 lr: 0.005\n",
      "iteration: 112000 loss: 0.0036 lr: 0.005\n",
      "iteration: 112100 loss: 0.0035 lr: 0.005\n",
      "iteration: 112200 loss: 0.0033 lr: 0.005\n",
      "iteration: 112300 loss: 0.0026 lr: 0.005\n",
      "iteration: 112400 loss: 0.0032 lr: 0.005\n",
      "iteration: 112500 loss: 0.0033 lr: 0.005\n",
      "iteration: 112600 loss: 0.0031 lr: 0.005\n",
      "iteration: 112700 loss: 0.0040 lr: 0.005\n",
      "iteration: 112800 loss: 0.0033 lr: 0.005\n",
      "iteration: 112900 loss: 0.0031 lr: 0.005\n",
      "iteration: 113000 loss: 0.0035 lr: 0.005\n",
      "iteration: 113100 loss: 0.0040 lr: 0.005\n",
      "iteration: 113200 loss: 0.0037 lr: 0.005\n",
      "iteration: 113300 loss: 0.0031 lr: 0.005\n",
      "iteration: 113400 loss: 0.0029 lr: 0.005\n",
      "iteration: 113500 loss: 0.0034 lr: 0.005\n",
      "iteration: 113600 loss: 0.0028 lr: 0.005\n",
      "iteration: 113700 loss: 0.0031 lr: 0.005\n",
      "iteration: 113800 loss: 0.0032 lr: 0.005\n",
      "iteration: 113900 loss: 0.0033 lr: 0.005\n",
      "iteration: 114000 loss: 0.0034 lr: 0.005\n",
      "iteration: 114100 loss: 0.0034 lr: 0.005\n",
      "iteration: 114200 loss: 0.0031 lr: 0.005\n",
      "iteration: 114300 loss: 0.0036 lr: 0.005\n",
      "iteration: 114400 loss: 0.0025 lr: 0.005\n",
      "iteration: 114500 loss: 0.0036 lr: 0.005\n",
      "iteration: 114600 loss: 0.0031 lr: 0.005\n",
      "iteration: 114700 loss: 0.0030 lr: 0.005\n",
      "iteration: 114800 loss: 0.0035 lr: 0.005\n",
      "iteration: 114900 loss: 0.0028 lr: 0.005\n",
      "iteration: 115000 loss: 0.0027 lr: 0.005\n",
      "iteration: 115100 loss: 0.0031 lr: 0.005\n",
      "iteration: 115200 loss: 0.0030 lr: 0.005\n",
      "iteration: 115300 loss: 0.0035 lr: 0.005\n",
      "iteration: 115400 loss: 0.0041 lr: 0.005\n",
      "iteration: 115500 loss: 0.0032 lr: 0.005\n",
      "iteration: 115600 loss: 0.0034 lr: 0.005\n",
      "iteration: 115700 loss: 0.0032 lr: 0.005\n",
      "iteration: 115800 loss: 0.0030 lr: 0.005\n",
      "iteration: 115900 loss: 0.0030 lr: 0.005\n",
      "iteration: 116000 loss: 0.0033 lr: 0.005\n",
      "iteration: 116100 loss: 0.0031 lr: 0.005\n",
      "iteration: 116200 loss: 0.0033 lr: 0.005\n",
      "iteration: 116300 loss: 0.0032 lr: 0.005\n",
      "iteration: 116400 loss: 0.0035 lr: 0.005\n",
      "iteration: 116500 loss: 0.0040 lr: 0.005\n",
      "iteration: 116600 loss: 0.0038 lr: 0.005\n",
      "iteration: 116700 loss: 0.0029 lr: 0.005\n",
      "iteration: 116800 loss: 0.0032 lr: 0.005\n",
      "iteration: 116900 loss: 0.0036 lr: 0.005\n",
      "iteration: 117000 loss: 0.0040 lr: 0.005\n",
      "iteration: 117100 loss: 0.0030 lr: 0.005\n",
      "iteration: 117200 loss: 0.0034 lr: 0.005\n",
      "iteration: 117300 loss: 0.0033 lr: 0.005\n",
      "iteration: 117400 loss: 0.0033 lr: 0.005\n",
      "iteration: 117500 loss: 0.0031 lr: 0.005\n",
      "iteration: 117600 loss: 0.0028 lr: 0.005\n",
      "iteration: 117700 loss: 0.0026 lr: 0.005\n",
      "iteration: 117800 loss: 0.0032 lr: 0.005\n",
      "iteration: 117900 loss: 0.0030 lr: 0.005\n",
      "iteration: 118000 loss: 0.0035 lr: 0.005\n",
      "iteration: 118100 loss: 0.0032 lr: 0.005\n",
      "iteration: 118200 loss: 0.0029 lr: 0.005\n",
      "iteration: 118300 loss: 0.0037 lr: 0.005\n",
      "iteration: 118400 loss: 0.0029 lr: 0.005\n",
      "iteration: 118500 loss: 0.0028 lr: 0.005\n",
      "iteration: 118600 loss: 0.0030 lr: 0.005\n",
      "iteration: 118700 loss: 0.0029 lr: 0.005\n",
      "iteration: 118800 loss: 0.0035 lr: 0.005\n",
      "iteration: 118900 loss: 0.0031 lr: 0.005\n",
      "iteration: 119000 loss: 0.0032 lr: 0.005\n",
      "iteration: 119100 loss: 0.0034 lr: 0.005\n",
      "iteration: 119200 loss: 0.0032 lr: 0.005\n",
      "iteration: 119300 loss: 0.0034 lr: 0.005\n",
      "iteration: 119400 loss: 0.0030 lr: 0.005\n",
      "iteration: 119500 loss: 0.0040 lr: 0.005\n",
      "iteration: 119600 loss: 0.0030 lr: 0.005\n",
      "iteration: 119700 loss: 0.0031 lr: 0.005\n",
      "iteration: 119800 loss: 0.0030 lr: 0.005\n",
      "iteration: 119900 loss: 0.0033 lr: 0.005\n",
      "iteration: 120000 loss: 0.0028 lr: 0.005\n",
      "iteration: 120100 loss: 0.0031 lr: 0.005\n",
      "iteration: 120200 loss: 0.0034 lr: 0.005\n",
      "iteration: 120300 loss: 0.0035 lr: 0.005\n",
      "iteration: 120400 loss: 0.0031 lr: 0.005\n",
      "iteration: 120500 loss: 0.0031 lr: 0.005\n",
      "iteration: 120600 loss: 0.0031 lr: 0.005\n",
      "iteration: 120700 loss: 0.0038 lr: 0.005\n",
      "iteration: 120800 loss: 0.0027 lr: 0.005\n",
      "iteration: 120900 loss: 0.0027 lr: 0.005\n",
      "iteration: 121000 loss: 0.0029 lr: 0.005\n",
      "iteration: 121100 loss: 0.0034 lr: 0.005\n",
      "iteration: 121200 loss: 0.0034 lr: 0.005\n",
      "iteration: 121300 loss: 0.0034 lr: 0.005\n",
      "iteration: 121400 loss: 0.0032 lr: 0.005\n",
      "iteration: 121500 loss: 0.0032 lr: 0.005\n",
      "iteration: 121600 loss: 0.0029 lr: 0.005\n",
      "iteration: 121700 loss: 0.0036 lr: 0.005\n",
      "iteration: 121800 loss: 0.0030 lr: 0.005\n",
      "iteration: 121900 loss: 0.0031 lr: 0.005\n",
      "iteration: 122000 loss: 0.0029 lr: 0.005\n",
      "iteration: 122100 loss: 0.0027 lr: 0.005\n",
      "iteration: 122200 loss: 0.0033 lr: 0.005\n",
      "iteration: 122300 loss: 0.0030 lr: 0.005\n",
      "iteration: 122400 loss: 0.0031 lr: 0.005\n",
      "iteration: 122500 loss: 0.0034 lr: 0.005\n",
      "iteration: 122600 loss: 0.0024 lr: 0.005\n",
      "iteration: 122700 loss: 0.0030 lr: 0.005\n",
      "iteration: 122800 loss: 0.0031 lr: 0.005\n",
      "iteration: 122900 loss: 0.0034 lr: 0.005\n",
      "iteration: 123000 loss: 0.0029 lr: 0.005\n",
      "iteration: 123100 loss: 0.0030 lr: 0.005\n",
      "iteration: 123200 loss: 0.0031 lr: 0.005\n",
      "iteration: 123300 loss: 0.0040 lr: 0.005\n",
      "iteration: 123400 loss: 0.0027 lr: 0.005\n",
      "iteration: 123500 loss: 0.0035 lr: 0.005\n",
      "iteration: 123600 loss: 0.0028 lr: 0.005\n",
      "iteration: 123700 loss: 0.0029 lr: 0.005\n",
      "iteration: 123800 loss: 0.0028 lr: 0.005\n",
      "iteration: 123900 loss: 0.0028 lr: 0.005\n",
      "iteration: 124000 loss: 0.0027 lr: 0.005\n",
      "iteration: 124100 loss: 0.0032 lr: 0.005\n",
      "iteration: 124200 loss: 0.0030 lr: 0.005\n",
      "iteration: 124300 loss: 0.0032 lr: 0.005\n",
      "iteration: 124400 loss: 0.0034 lr: 0.005\n",
      "iteration: 124500 loss: 0.0034 lr: 0.005\n",
      "iteration: 124600 loss: 0.0024 lr: 0.005\n",
      "iteration: 124700 loss: 0.0036 lr: 0.005\n",
      "iteration: 124800 loss: 0.0030 lr: 0.005\n",
      "iteration: 124900 loss: 0.0031 lr: 0.005\n",
      "iteration: 125000 loss: 0.0035 lr: 0.005\n",
      "iteration: 125100 loss: 0.0033 lr: 0.005\n",
      "iteration: 125200 loss: 0.0027 lr: 0.005\n",
      "iteration: 125300 loss: 0.0030 lr: 0.005\n",
      "iteration: 125400 loss: 0.0029 lr: 0.005\n",
      "iteration: 125500 loss: 0.0034 lr: 0.005\n",
      "iteration: 125600 loss: 0.0033 lr: 0.005\n",
      "iteration: 125700 loss: 0.0031 lr: 0.005\n",
      "iteration: 125800 loss: 0.0037 lr: 0.005\n",
      "iteration: 125900 loss: 0.0030 lr: 0.005\n",
      "iteration: 126000 loss: 0.0029 lr: 0.005\n",
      "iteration: 126100 loss: 0.0030 lr: 0.005\n",
      "iteration: 126200 loss: 0.0030 lr: 0.005\n",
      "iteration: 126300 loss: 0.0027 lr: 0.005\n",
      "iteration: 126400 loss: 0.0032 lr: 0.005\n",
      "iteration: 126500 loss: 0.0038 lr: 0.005\n",
      "iteration: 126600 loss: 0.0034 lr: 0.005\n",
      "iteration: 126700 loss: 0.0031 lr: 0.005\n",
      "iteration: 126800 loss: 0.0038 lr: 0.005\n",
      "iteration: 126900 loss: 0.0029 lr: 0.005\n",
      "iteration: 127000 loss: 0.0036 lr: 0.005\n",
      "iteration: 127100 loss: 0.0032 lr: 0.005\n",
      "iteration: 127200 loss: 0.0026 lr: 0.005\n",
      "iteration: 127300 loss: 0.0033 lr: 0.005\n",
      "iteration: 127400 loss: 0.0031 lr: 0.005\n",
      "iteration: 127500 loss: 0.0032 lr: 0.005\n",
      "iteration: 127600 loss: 0.0029 lr: 0.005\n",
      "iteration: 127700 loss: 0.0031 lr: 0.005\n",
      "iteration: 127800 loss: 0.0034 lr: 0.005\n",
      "iteration: 127900 loss: 0.0029 lr: 0.005\n",
      "iteration: 128000 loss: 0.0027 lr: 0.005\n",
      "iteration: 128100 loss: 0.0031 lr: 0.005\n",
      "iteration: 128200 loss: 0.0025 lr: 0.005\n",
      "iteration: 128300 loss: 0.0034 lr: 0.005\n",
      "iteration: 128400 loss: 0.0035 lr: 0.005\n",
      "iteration: 128500 loss: 0.0034 lr: 0.005\n",
      "iteration: 128600 loss: 0.0036 lr: 0.005\n",
      "iteration: 128700 loss: 0.0028 lr: 0.005\n",
      "iteration: 128800 loss: 0.0032 lr: 0.005\n",
      "iteration: 128900 loss: 0.0030 lr: 0.005\n",
      "iteration: 129000 loss: 0.0035 lr: 0.005\n",
      "iteration: 129100 loss: 0.0036 lr: 0.005\n",
      "iteration: 129200 loss: 0.0035 lr: 0.005\n",
      "iteration: 129300 loss: 0.0030 lr: 0.005\n",
      "iteration: 129400 loss: 0.0025 lr: 0.005\n",
      "iteration: 129500 loss: 0.0035 lr: 0.005\n",
      "iteration: 129600 loss: 0.0030 lr: 0.005\n",
      "iteration: 129700 loss: 0.0028 lr: 0.005\n",
      "iteration: 129800 loss: 0.0027 lr: 0.005\n",
      "iteration: 129900 loss: 0.0028 lr: 0.005\n",
      "iteration: 130000 loss: 0.0033 lr: 0.005\n",
      "iteration: 130100 loss: 0.0031 lr: 0.005\n",
      "iteration: 130200 loss: 0.0033 lr: 0.005\n",
      "iteration: 130300 loss: 0.0031 lr: 0.005\n",
      "iteration: 130400 loss: 0.0027 lr: 0.005\n",
      "iteration: 130500 loss: 0.0038 lr: 0.005\n",
      "iteration: 130600 loss: 0.0034 lr: 0.005\n",
      "iteration: 130700 loss: 0.0030 lr: 0.005\n",
      "iteration: 130800 loss: 0.0031 lr: 0.005\n",
      "iteration: 130900 loss: 0.0036 lr: 0.005\n",
      "iteration: 131000 loss: 0.0029 lr: 0.005\n",
      "iteration: 131100 loss: 0.0029 lr: 0.005\n",
      "iteration: 131200 loss: 0.0031 lr: 0.005\n",
      "iteration: 131300 loss: 0.0028 lr: 0.005\n",
      "iteration: 131400 loss: 0.0027 lr: 0.005\n",
      "iteration: 131500 loss: 0.0028 lr: 0.005\n",
      "iteration: 131600 loss: 0.0034 lr: 0.005\n",
      "iteration: 131700 loss: 0.0029 lr: 0.005\n",
      "iteration: 131800 loss: 0.0027 lr: 0.005\n",
      "iteration: 131900 loss: 0.0029 lr: 0.005\n",
      "iteration: 132000 loss: 0.0030 lr: 0.005\n",
      "iteration: 132100 loss: 0.0036 lr: 0.005\n",
      "iteration: 132200 loss: 0.0027 lr: 0.005\n",
      "iteration: 132300 loss: 0.0027 lr: 0.005\n",
      "iteration: 132400 loss: 0.0028 lr: 0.005\n",
      "iteration: 132500 loss: 0.0027 lr: 0.005\n",
      "iteration: 132600 loss: 0.0034 lr: 0.005\n",
      "iteration: 132700 loss: 0.0032 lr: 0.005\n",
      "iteration: 132800 loss: 0.0033 lr: 0.005\n",
      "iteration: 132900 loss: 0.0025 lr: 0.005\n",
      "iteration: 133000 loss: 0.0030 lr: 0.005\n",
      "iteration: 133100 loss: 0.0032 lr: 0.005\n",
      "iteration: 133200 loss: 0.0029 lr: 0.005\n",
      "iteration: 133300 loss: 0.0025 lr: 0.005\n",
      "iteration: 133400 loss: 0.0032 lr: 0.005\n",
      "iteration: 133500 loss: 0.0037 lr: 0.005\n",
      "iteration: 133600 loss: 0.0029 lr: 0.005\n",
      "iteration: 133700 loss: 0.0031 lr: 0.005\n",
      "iteration: 133800 loss: 0.0029 lr: 0.005\n",
      "iteration: 133900 loss: 0.0033 lr: 0.005\n",
      "iteration: 134000 loss: 0.0030 lr: 0.005\n",
      "iteration: 134100 loss: 0.0029 lr: 0.005\n",
      "iteration: 134200 loss: 0.0027 lr: 0.005\n",
      "iteration: 134300 loss: 0.0025 lr: 0.005\n",
      "iteration: 134400 loss: 0.0031 lr: 0.005\n",
      "iteration: 134500 loss: 0.0035 lr: 0.005\n",
      "iteration: 134600 loss: 0.0034 lr: 0.005\n",
      "iteration: 134700 loss: 0.0025 lr: 0.005\n",
      "iteration: 134800 loss: 0.0035 lr: 0.005\n",
      "iteration: 134900 loss: 0.0032 lr: 0.005\n",
      "iteration: 135000 loss: 0.0029 lr: 0.005\n",
      "iteration: 135100 loss: 0.0027 lr: 0.005\n",
      "iteration: 135200 loss: 0.0031 lr: 0.005\n",
      "iteration: 135300 loss: 0.0028 lr: 0.005\n",
      "iteration: 135400 loss: 0.0032 lr: 0.005\n",
      "iteration: 135500 loss: 0.0032 lr: 0.005\n",
      "iteration: 135600 loss: 0.0034 lr: 0.005\n",
      "iteration: 135700 loss: 0.0031 lr: 0.005\n",
      "iteration: 135800 loss: 0.0030 lr: 0.005\n",
      "iteration: 135900 loss: 0.0030 lr: 0.005\n",
      "iteration: 136000 loss: 0.0033 lr: 0.005\n",
      "iteration: 136100 loss: 0.0029 lr: 0.005\n",
      "iteration: 136200 loss: 0.0029 lr: 0.005\n",
      "iteration: 136300 loss: 0.0034 lr: 0.005\n",
      "iteration: 136400 loss: 0.0030 lr: 0.005\n",
      "iteration: 136500 loss: 0.0030 lr: 0.005\n",
      "iteration: 136600 loss: 0.0033 lr: 0.005\n",
      "iteration: 136700 loss: 0.0028 lr: 0.005\n",
      "iteration: 136800 loss: 0.0036 lr: 0.005\n",
      "iteration: 136900 loss: 0.0030 lr: 0.005\n",
      "iteration: 137000 loss: 0.0030 lr: 0.005\n",
      "iteration: 137100 loss: 0.0045 lr: 0.005\n",
      "iteration: 137200 loss: 0.0029 lr: 0.005\n",
      "iteration: 137300 loss: 0.0033 lr: 0.005\n",
      "iteration: 137400 loss: 0.0030 lr: 0.005\n",
      "iteration: 137500 loss: 0.0028 lr: 0.005\n",
      "iteration: 137600 loss: 0.0029 lr: 0.005\n",
      "iteration: 137700 loss: 0.0030 lr: 0.005\n",
      "iteration: 137800 loss: 0.0028 lr: 0.005\n",
      "iteration: 137900 loss: 0.0028 lr: 0.005\n",
      "iteration: 138000 loss: 0.0027 lr: 0.005\n",
      "iteration: 138100 loss: 0.0037 lr: 0.005\n",
      "iteration: 138200 loss: 0.0029 lr: 0.005\n",
      "iteration: 138300 loss: 0.0033 lr: 0.005\n",
      "iteration: 138400 loss: 0.0031 lr: 0.005\n",
      "iteration: 138500 loss: 0.0037 lr: 0.005\n",
      "iteration: 138600 loss: 0.0026 lr: 0.005\n",
      "iteration: 138700 loss: 0.0028 lr: 0.005\n",
      "iteration: 138800 loss: 0.0031 lr: 0.005\n",
      "iteration: 138900 loss: 0.0028 lr: 0.005\n",
      "iteration: 139000 loss: 0.0027 lr: 0.005\n",
      "iteration: 139100 loss: 0.0034 lr: 0.005\n",
      "iteration: 139200 loss: 0.0025 lr: 0.005\n",
      "iteration: 139300 loss: 0.0033 lr: 0.005\n",
      "iteration: 139400 loss: 0.0031 lr: 0.005\n",
      "iteration: 139500 loss: 0.0032 lr: 0.005\n",
      "iteration: 139600 loss: 0.0025 lr: 0.005\n",
      "iteration: 139700 loss: 0.0029 lr: 0.005\n",
      "iteration: 139800 loss: 0.0027 lr: 0.005\n",
      "iteration: 139900 loss: 0.0037 lr: 0.005\n",
      "iteration: 140000 loss: 0.0030 lr: 0.005\n",
      "iteration: 140100 loss: 0.0037 lr: 0.005\n",
      "iteration: 140200 loss: 0.0027 lr: 0.005\n",
      "iteration: 140300 loss: 0.0032 lr: 0.005\n",
      "iteration: 140400 loss: 0.0030 lr: 0.005\n",
      "iteration: 140500 loss: 0.0031 lr: 0.005\n",
      "iteration: 140600 loss: 0.0034 lr: 0.005\n",
      "iteration: 140700 loss: 0.0031 lr: 0.005\n",
      "iteration: 140800 loss: 0.0028 lr: 0.005\n",
      "iteration: 140900 loss: 0.0031 lr: 0.005\n",
      "iteration: 141000 loss: 0.0031 lr: 0.005\n",
      "iteration: 141100 loss: 0.0028 lr: 0.005\n",
      "iteration: 141200 loss: 0.0032 lr: 0.005\n",
      "iteration: 141300 loss: 0.0029 lr: 0.005\n",
      "iteration: 141400 loss: 0.0031 lr: 0.005\n",
      "iteration: 141500 loss: 0.0027 lr: 0.005\n",
      "iteration: 141600 loss: 0.0031 lr: 0.005\n",
      "iteration: 141700 loss: 0.0035 lr: 0.005\n",
      "iteration: 141800 loss: 0.0028 lr: 0.005\n",
      "iteration: 141900 loss: 0.0027 lr: 0.005\n",
      "iteration: 142000 loss: 0.0033 lr: 0.005\n",
      "iteration: 142100 loss: 0.0028 lr: 0.005\n",
      "iteration: 142200 loss: 0.0027 lr: 0.005\n",
      "iteration: 142300 loss: 0.0032 lr: 0.005\n",
      "iteration: 142400 loss: 0.0030 lr: 0.005\n",
      "iteration: 142500 loss: 0.0036 lr: 0.005\n",
      "iteration: 142600 loss: 0.0028 lr: 0.005\n",
      "iteration: 142700 loss: 0.0033 lr: 0.005\n",
      "iteration: 142800 loss: 0.0026 lr: 0.005\n",
      "iteration: 142900 loss: 0.0034 lr: 0.005\n",
      "iteration: 143000 loss: 0.0029 lr: 0.005\n",
      "iteration: 143100 loss: 0.0027 lr: 0.005\n",
      "iteration: 143200 loss: 0.0027 lr: 0.005\n",
      "iteration: 143300 loss: 0.0033 lr: 0.005\n",
      "iteration: 143400 loss: 0.0030 lr: 0.005\n",
      "iteration: 143500 loss: 0.0035 lr: 0.005\n",
      "iteration: 143600 loss: 0.0028 lr: 0.005\n",
      "iteration: 143700 loss: 0.0036 lr: 0.005\n",
      "iteration: 143800 loss: 0.0029 lr: 0.005\n",
      "iteration: 143900 loss: 0.0030 lr: 0.005\n",
      "iteration: 144000 loss: 0.0028 lr: 0.005\n",
      "iteration: 144100 loss: 0.0025 lr: 0.005\n",
      "iteration: 144200 loss: 0.0025 lr: 0.005\n",
      "iteration: 144300 loss: 0.0033 lr: 0.005\n",
      "iteration: 144400 loss: 0.0025 lr: 0.005\n",
      "iteration: 144500 loss: 0.0029 lr: 0.005\n",
      "iteration: 144600 loss: 0.0034 lr: 0.005\n",
      "iteration: 144700 loss: 0.0039 lr: 0.005\n",
      "iteration: 144800 loss: 0.0031 lr: 0.005\n",
      "iteration: 144900 loss: 0.0031 lr: 0.005\n",
      "iteration: 145000 loss: 0.0025 lr: 0.005\n",
      "iteration: 145100 loss: 0.0032 lr: 0.005\n",
      "iteration: 145200 loss: 0.0031 lr: 0.005\n",
      "iteration: 145300 loss: 0.0031 lr: 0.005\n",
      "iteration: 145400 loss: 0.0031 lr: 0.005\n",
      "iteration: 145500 loss: 0.0033 lr: 0.005\n",
      "iteration: 145600 loss: 0.0029 lr: 0.005\n",
      "iteration: 145700 loss: 0.0025 lr: 0.005\n",
      "iteration: 145800 loss: 0.0033 lr: 0.005\n",
      "iteration: 145900 loss: 0.0034 lr: 0.005\n",
      "iteration: 146000 loss: 0.0029 lr: 0.005\n",
      "iteration: 146100 loss: 0.0028 lr: 0.005\n",
      "iteration: 146200 loss: 0.0026 lr: 0.005\n",
      "iteration: 146300 loss: 0.0027 lr: 0.005\n",
      "iteration: 146400 loss: 0.0033 lr: 0.005\n",
      "iteration: 146500 loss: 0.0029 lr: 0.005\n",
      "iteration: 146600 loss: 0.0029 lr: 0.005\n",
      "iteration: 146700 loss: 0.0027 lr: 0.005\n",
      "iteration: 146800 loss: 0.0031 lr: 0.005\n",
      "iteration: 146900 loss: 0.0032 lr: 0.005\n",
      "iteration: 147000 loss: 0.0029 lr: 0.005\n",
      "iteration: 147100 loss: 0.0030 lr: 0.005\n",
      "iteration: 147200 loss: 0.0033 lr: 0.005\n",
      "iteration: 147300 loss: 0.0027 lr: 0.005\n",
      "iteration: 147400 loss: 0.0026 lr: 0.005\n",
      "iteration: 147500 loss: 0.0033 lr: 0.005\n",
      "iteration: 147600 loss: 0.0030 lr: 0.005\n",
      "iteration: 147700 loss: 0.0028 lr: 0.005\n",
      "iteration: 147800 loss: 0.0032 lr: 0.005\n",
      "iteration: 147900 loss: 0.0027 lr: 0.005\n",
      "iteration: 148000 loss: 0.0032 lr: 0.005\n",
      "iteration: 148100 loss: 0.0033 lr: 0.005\n",
      "iteration: 148200 loss: 0.0029 lr: 0.005\n",
      "iteration: 148300 loss: 0.0023 lr: 0.005\n",
      "iteration: 148400 loss: 0.0029 lr: 0.005\n",
      "iteration: 148500 loss: 0.0033 lr: 0.005\n",
      "iteration: 148600 loss: 0.0030 lr: 0.005\n",
      "iteration: 148700 loss: 0.0029 lr: 0.005\n",
      "iteration: 148800 loss: 0.0031 lr: 0.005\n",
      "iteration: 148900 loss: 0.0027 lr: 0.005\n",
      "iteration: 149000 loss: 0.0027 lr: 0.005\n",
      "iteration: 149100 loss: 0.0026 lr: 0.005\n",
      "iteration: 149200 loss: 0.0032 lr: 0.005\n",
      "iteration: 149300 loss: 0.0032 lr: 0.005\n",
      "iteration: 149400 loss: 0.0034 lr: 0.005\n",
      "iteration: 149500 loss: 0.0029 lr: 0.005\n",
      "iteration: 149600 loss: 0.0026 lr: 0.005\n",
      "iteration: 149700 loss: 0.0029 lr: 0.005\n",
      "iteration: 149800 loss: 0.0029 lr: 0.005\n",
      "iteration: 149900 loss: 0.0031 lr: 0.005\n",
      "iteration: 150000 loss: 0.0040 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1377, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1455, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 83, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1396, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:\n",
      "\n",
      "Detected at node 'fifo_queue_enqueue' defined at (most recent call last):\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "      \"__main__\", mod_spec)\n",
      "    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "      app.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "      self.io_loop.start()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "      self._run_once()\n",
      "    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "      handle._run()\n",
      "    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "      ret = callback()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "      self.run()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "      yielded = self.gen.send(value)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "      yield self.process_one()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 346, in wrapper\n",
      "      runner = Runner(result, future, yielded)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1080, in __init__\n",
      "      self.run()\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "      yielded = self.gen.send(value)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "      yield gen.maybe_future(dispatch(*args))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "      yield gen.maybe_future(handler(stream, idents, msg))\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "      user_expressions, allow_stdin,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "      yielded = next(result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "      res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "      raw_cell, store_history, silent, shell_futures)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "      return runner(coro)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "      interactivity=interactivity, compiler=compiler, result=result)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "      if (await self.run_code(code, result,  async_=asy)):\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"<ipython-input-13-59243f7f6815>\", line 2, in <module>\n",
      "      deeplabcut.train_network(path_config_file, shuffle=10, displayiters=100, saveiters=500)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "      allow_growth=allow_growth,\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "      batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "    File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "      enqueue_op = q.enqueue(placeholders_list)\n",
      "Node: 'fifo_queue_enqueue'\n",
      "Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_enqueue':\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 346, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1080, in __init__\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-59243f7f6815>\", line 2, in <module>\n",
      "    deeplabcut.train_network(path_config_file, shuffle=10, displayiters=100, saveiters=500)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 217, in train_network\n",
      "    allow_growth=allow_growth,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 168, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 69, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 348, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4065, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n",
      "    attrs=attr_protos, op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3784, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2175, in __init__\n",
      "    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "deeplabcut.train_network(path_config_file, shuffle=10, displayiters=100, saveiters=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4343537,
     "status": "ok",
     "timestamp": 1663199740753,
     "user": {
      "displayName": "Bowen Jumbo",
      "userId": "15176400549218670876"
     },
     "user_tz": -60
    },
    "id": "gylu6JR5U4-l",
    "outputId": "cc8af346-ec2c-44ab-a033-f526f81fb61e"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle13.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_0.35_224.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle13.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 150000]],\n",
      " 'net_type': 'mobilenet_v2_0.35',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/content/drive/My '\n",
      "                 'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/content/drive/My '\n",
      "                    'Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle13/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading ImageNet-pretrained mobilenet_v2_0.35\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 500\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/dlc-models/iteration-3/Yolo_1000_trainAug7-trainset90shuffle13/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['left_tip', 'right_tip', 'pitch1', 'pitch2'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Yolo_1000_train_Jumbo90shuffle13.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_0.35_224.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-3/UnaugmentedDataSet_Yolo_1000_trainAug7/Documentation_data-Yolo_1000_train_90shuffle13.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 150000]], 'net_type': 'mobilenet_v2_0.35', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.1557 lr: 0.005\n",
      "iteration: 200 loss: 0.0367 lr: 0.005\n",
      "iteration: 300 loss: 0.0315 lr: 0.005\n",
      "iteration: 400 loss: 0.0278 lr: 0.005\n",
      "iteration: 500 loss: 0.0274 lr: 0.005\n",
      "iteration: 600 loss: 0.0243 lr: 0.005\n",
      "iteration: 700 loss: 0.0230 lr: 0.005\n",
      "iteration: 800 loss: 0.0240 lr: 0.005\n",
      "iteration: 900 loss: 0.0226 lr: 0.005\n",
      "iteration: 1000 loss: 0.0226 lr: 0.005\n",
      "iteration: 1100 loss: 0.0206 lr: 0.005\n",
      "iteration: 1200 loss: 0.0201 lr: 0.005\n",
      "iteration: 1300 loss: 0.0197 lr: 0.005\n",
      "iteration: 1400 loss: 0.0188 lr: 0.005\n",
      "iteration: 1500 loss: 0.0197 lr: 0.005\n",
      "iteration: 1600 loss: 0.0201 lr: 0.005\n",
      "iteration: 1700 loss: 0.0201 lr: 0.005\n",
      "iteration: 1800 loss: 0.0201 lr: 0.005\n",
      "iteration: 1900 loss: 0.0186 lr: 0.005\n",
      "iteration: 2000 loss: 0.0201 lr: 0.005\n",
      "iteration: 2100 loss: 0.0186 lr: 0.005\n",
      "iteration: 2200 loss: 0.0184 lr: 0.005\n",
      "iteration: 2300 loss: 0.0201 lr: 0.005\n",
      "iteration: 2400 loss: 0.0184 lr: 0.005\n",
      "iteration: 2500 loss: 0.0178 lr: 0.005\n",
      "iteration: 2600 loss: 0.0204 lr: 0.005\n",
      "iteration: 2700 loss: 0.0176 lr: 0.005\n",
      "iteration: 2800 loss: 0.0169 lr: 0.005\n",
      "iteration: 2900 loss: 0.0175 lr: 0.005\n",
      "iteration: 3000 loss: 0.0170 lr: 0.005\n",
      "iteration: 3100 loss: 0.0179 lr: 0.005\n",
      "iteration: 3200 loss: 0.0171 lr: 0.005\n",
      "iteration: 3300 loss: 0.0178 lr: 0.005\n",
      "iteration: 3400 loss: 0.0162 lr: 0.005\n",
      "iteration: 3500 loss: 0.0178 lr: 0.005\n",
      "iteration: 3600 loss: 0.0157 lr: 0.005\n",
      "iteration: 3700 loss: 0.0168 lr: 0.005\n",
      "iteration: 3800 loss: 0.0169 lr: 0.005\n",
      "iteration: 3900 loss: 0.0169 lr: 0.005\n",
      "iteration: 4000 loss: 0.0167 lr: 0.005\n",
      "iteration: 4100 loss: 0.0158 lr: 0.005\n",
      "iteration: 4200 loss: 0.0171 lr: 0.005\n",
      "iteration: 4300 loss: 0.0156 lr: 0.005\n",
      "iteration: 4400 loss: 0.0157 lr: 0.005\n",
      "iteration: 4500 loss: 0.0157 lr: 0.005\n",
      "iteration: 4600 loss: 0.0164 lr: 0.005\n",
      "iteration: 4700 loss: 0.0156 lr: 0.005\n",
      "iteration: 4800 loss: 0.0139 lr: 0.005\n",
      "iteration: 4900 loss: 0.0150 lr: 0.005\n",
      "iteration: 5000 loss: 0.0147 lr: 0.005\n",
      "iteration: 5100 loss: 0.0165 lr: 0.005\n",
      "iteration: 5200 loss: 0.0149 lr: 0.005\n",
      "iteration: 5300 loss: 0.0169 lr: 0.005\n",
      "iteration: 5400 loss: 0.0158 lr: 0.005\n",
      "iteration: 5500 loss: 0.0150 lr: 0.005\n",
      "iteration: 5600 loss: 0.0153 lr: 0.005\n",
      "iteration: 5700 loss: 0.0156 lr: 0.005\n",
      "iteration: 5800 loss: 0.0148 lr: 0.005\n",
      "iteration: 5900 loss: 0.0156 lr: 0.005\n",
      "iteration: 6000 loss: 0.0139 lr: 0.005\n",
      "iteration: 6100 loss: 0.0157 lr: 0.005\n",
      "iteration: 6200 loss: 0.0153 lr: 0.005\n",
      "iteration: 6300 loss: 0.0151 lr: 0.005\n",
      "iteration: 6400 loss: 0.0143 lr: 0.005\n",
      "iteration: 6500 loss: 0.0153 lr: 0.005\n",
      "iteration: 6600 loss: 0.0142 lr: 0.005\n",
      "iteration: 6700 loss: 0.0162 lr: 0.005\n",
      "iteration: 6800 loss: 0.0142 lr: 0.005\n",
      "iteration: 6900 loss: 0.0144 lr: 0.005\n",
      "iteration: 7000 loss: 0.0154 lr: 0.005\n",
      "iteration: 7100 loss: 0.0138 lr: 0.005\n",
      "iteration: 7200 loss: 0.0149 lr: 0.005\n",
      "iteration: 7300 loss: 0.0148 lr: 0.005\n",
      "iteration: 7400 loss: 0.0151 lr: 0.005\n",
      "iteration: 7500 loss: 0.0138 lr: 0.005\n",
      "iteration: 7600 loss: 0.0132 lr: 0.005\n",
      "iteration: 7700 loss: 0.0141 lr: 0.005\n",
      "iteration: 7800 loss: 0.0146 lr: 0.005\n",
      "iteration: 7900 loss: 0.0154 lr: 0.005\n",
      "iteration: 8000 loss: 0.0147 lr: 0.005\n",
      "iteration: 8100 loss: 0.0147 lr: 0.005\n",
      "iteration: 8200 loss: 0.0146 lr: 0.005\n",
      "iteration: 8300 loss: 0.0145 lr: 0.005\n",
      "iteration: 8400 loss: 0.0136 lr: 0.005\n",
      "iteration: 8500 loss: 0.0129 lr: 0.005\n",
      "iteration: 8600 loss: 0.0135 lr: 0.005\n",
      "iteration: 8700 loss: 0.0146 lr: 0.005\n",
      "iteration: 8800 loss: 0.0130 lr: 0.005\n",
      "iteration: 8900 loss: 0.0147 lr: 0.005\n",
      "iteration: 9000 loss: 0.0114 lr: 0.005\n",
      "iteration: 9100 loss: 0.0127 lr: 0.005\n",
      "iteration: 9200 loss: 0.0133 lr: 0.005\n",
      "iteration: 9300 loss: 0.0146 lr: 0.005\n",
      "iteration: 9400 loss: 0.0139 lr: 0.005\n",
      "iteration: 9500 loss: 0.0134 lr: 0.005\n",
      "iteration: 9600 loss: 0.0131 lr: 0.005\n",
      "iteration: 9700 loss: 0.0138 lr: 0.005\n",
      "iteration: 9800 loss: 0.0123 lr: 0.005\n",
      "iteration: 9900 loss: 0.0127 lr: 0.005\n",
      "iteration: 10000 loss: 0.0126 lr: 0.005\n",
      "iteration: 10100 loss: 0.0122 lr: 0.005\n",
      "iteration: 10200 loss: 0.0124 lr: 0.005\n",
      "iteration: 10300 loss: 0.0131 lr: 0.005\n",
      "iteration: 10400 loss: 0.0138 lr: 0.005\n",
      "iteration: 10500 loss: 0.0126 lr: 0.005\n",
      "iteration: 10600 loss: 0.0135 lr: 0.005\n",
      "iteration: 10700 loss: 0.0133 lr: 0.005\n",
      "iteration: 10800 loss: 0.0120 lr: 0.005\n",
      "iteration: 10900 loss: 0.0129 lr: 0.005\n",
      "iteration: 11000 loss: 0.0121 lr: 0.005\n",
      "iteration: 11100 loss: 0.0116 lr: 0.005\n",
      "iteration: 11200 loss: 0.0126 lr: 0.005\n",
      "iteration: 11300 loss: 0.0124 lr: 0.005\n",
      "iteration: 11400 loss: 0.0128 lr: 0.005\n",
      "iteration: 11500 loss: 0.0130 lr: 0.005\n",
      "iteration: 11600 loss: 0.0126 lr: 0.005\n",
      "iteration: 11700 loss: 0.0115 lr: 0.005\n",
      "iteration: 11800 loss: 0.0121 lr: 0.005\n",
      "iteration: 11900 loss: 0.0133 lr: 0.005\n",
      "iteration: 12000 loss: 0.0123 lr: 0.005\n",
      "iteration: 12100 loss: 0.0115 lr: 0.005\n",
      "iteration: 12200 loss: 0.0118 lr: 0.005\n",
      "iteration: 12300 loss: 0.0107 lr: 0.005\n",
      "iteration: 12400 loss: 0.0119 lr: 0.005\n",
      "iteration: 12500 loss: 0.0129 lr: 0.005\n",
      "iteration: 12600 loss: 0.0116 lr: 0.005\n",
      "iteration: 12700 loss: 0.0116 lr: 0.005\n",
      "iteration: 12800 loss: 0.0123 lr: 0.005\n",
      "iteration: 12900 loss: 0.0112 lr: 0.005\n",
      "iteration: 13000 loss: 0.0116 lr: 0.005\n",
      "iteration: 13100 loss: 0.0115 lr: 0.005\n",
      "iteration: 13200 loss: 0.0118 lr: 0.005\n",
      "iteration: 13300 loss: 0.0111 lr: 0.005\n",
      "iteration: 13400 loss: 0.0120 lr: 0.005\n",
      "iteration: 13500 loss: 0.0102 lr: 0.005\n",
      "iteration: 13600 loss: 0.0110 lr: 0.005\n",
      "iteration: 13700 loss: 0.0124 lr: 0.005\n",
      "iteration: 13800 loss: 0.0116 lr: 0.005\n",
      "iteration: 13900 loss: 0.0120 lr: 0.005\n",
      "iteration: 14000 loss: 0.0114 lr: 0.005\n",
      "iteration: 14100 loss: 0.0118 lr: 0.005\n",
      "iteration: 14200 loss: 0.0115 lr: 0.005\n",
      "iteration: 14300 loss: 0.0117 lr: 0.005\n",
      "iteration: 14400 loss: 0.0114 lr: 0.005\n",
      "iteration: 14500 loss: 0.0116 lr: 0.005\n",
      "iteration: 14600 loss: 0.0110 lr: 0.005\n",
      "iteration: 14700 loss: 0.0101 lr: 0.005\n",
      "iteration: 14800 loss: 0.0107 lr: 0.005\n",
      "iteration: 14900 loss: 0.0106 lr: 0.005\n",
      "iteration: 15000 loss: 0.0108 lr: 0.005\n",
      "iteration: 15100 loss: 0.0110 lr: 0.005\n",
      "iteration: 15200 loss: 0.0104 lr: 0.005\n",
      "iteration: 15300 loss: 0.0107 lr: 0.005\n",
      "iteration: 15400 loss: 0.0102 lr: 0.005\n",
      "iteration: 15500 loss: 0.0112 lr: 0.005\n",
      "iteration: 15600 loss: 0.0104 lr: 0.005\n",
      "iteration: 15700 loss: 0.0108 lr: 0.005\n",
      "iteration: 15800 loss: 0.0109 lr: 0.005\n",
      "iteration: 15900 loss: 0.0112 lr: 0.005\n",
      "iteration: 16000 loss: 0.0112 lr: 0.005\n",
      "iteration: 16100 loss: 0.0105 lr: 0.005\n",
      "iteration: 16200 loss: 0.0098 lr: 0.005\n",
      "iteration: 16300 loss: 0.0099 lr: 0.005\n",
      "iteration: 16400 loss: 0.0113 lr: 0.005\n",
      "iteration: 16500 loss: 0.0114 lr: 0.005\n",
      "iteration: 16600 loss: 0.0114 lr: 0.005\n",
      "iteration: 16700 loss: 0.0108 lr: 0.005\n",
      "iteration: 16800 loss: 0.0103 lr: 0.005\n",
      "iteration: 16900 loss: 0.0107 lr: 0.005\n",
      "iteration: 17000 loss: 0.0095 lr: 0.005\n",
      "iteration: 17100 loss: 0.0103 lr: 0.005\n",
      "iteration: 17200 loss: 0.0102 lr: 0.005\n",
      "iteration: 17300 loss: 0.0100 lr: 0.005\n",
      "iteration: 17400 loss: 0.0104 lr: 0.005\n",
      "iteration: 17500 loss: 0.0105 lr: 0.005\n",
      "iteration: 17600 loss: 0.0111 lr: 0.005\n",
      "iteration: 17700 loss: 0.0103 lr: 0.005\n",
      "iteration: 17800 loss: 0.0097 lr: 0.005\n",
      "iteration: 17900 loss: 0.0090 lr: 0.005\n",
      "iteration: 18000 loss: 0.0102 lr: 0.005\n",
      "iteration: 18100 loss: 0.0098 lr: 0.005\n",
      "iteration: 18200 loss: 0.0097 lr: 0.005\n",
      "iteration: 18300 loss: 0.0106 lr: 0.005\n",
      "iteration: 18400 loss: 0.0101 lr: 0.005\n",
      "iteration: 18500 loss: 0.0097 lr: 0.005\n",
      "iteration: 18600 loss: 0.0103 lr: 0.005\n",
      "iteration: 18700 loss: 0.0102 lr: 0.005\n",
      "iteration: 18800 loss: 0.0092 lr: 0.005\n",
      "iteration: 18900 loss: 0.0097 lr: 0.005\n",
      "iteration: 19000 loss: 0.0094 lr: 0.005\n",
      "iteration: 19100 loss: 0.0100 lr: 0.005\n",
      "iteration: 19200 loss: 0.0096 lr: 0.005\n",
      "iteration: 19300 loss: 0.0088 lr: 0.005\n",
      "iteration: 19400 loss: 0.0102 lr: 0.005\n",
      "iteration: 19500 loss: 0.0098 lr: 0.005\n",
      "iteration: 19600 loss: 0.0101 lr: 0.005\n",
      "iteration: 19700 loss: 0.0091 lr: 0.005\n",
      "iteration: 19800 loss: 0.0107 lr: 0.005\n",
      "iteration: 19900 loss: 0.0104 lr: 0.005\n",
      "iteration: 20000 loss: 0.0087 lr: 0.005\n",
      "iteration: 20100 loss: 0.0107 lr: 0.005\n",
      "iteration: 20200 loss: 0.0092 lr: 0.005\n",
      "iteration: 20300 loss: 0.0100 lr: 0.005\n",
      "iteration: 20400 loss: 0.0088 lr: 0.005\n",
      "iteration: 20500 loss: 0.0089 lr: 0.005\n",
      "iteration: 20600 loss: 0.0087 lr: 0.005\n",
      "iteration: 20700 loss: 0.0082 lr: 0.005\n",
      "iteration: 20800 loss: 0.0093 lr: 0.005\n",
      "iteration: 20900 loss: 0.0095 lr: 0.005\n",
      "iteration: 21000 loss: 0.0093 lr: 0.005\n",
      "iteration: 21100 loss: 0.0091 lr: 0.005\n",
      "iteration: 21200 loss: 0.0085 lr: 0.005\n",
      "iteration: 21300 loss: 0.0085 lr: 0.005\n",
      "iteration: 21400 loss: 0.0088 lr: 0.005\n",
      "iteration: 21500 loss: 0.0098 lr: 0.005\n",
      "iteration: 21600 loss: 0.0088 lr: 0.005\n",
      "iteration: 21700 loss: 0.0084 lr: 0.005\n",
      "iteration: 21800 loss: 0.0077 lr: 0.005\n",
      "iteration: 21900 loss: 0.0097 lr: 0.005\n",
      "iteration: 22000 loss: 0.0091 lr: 0.005\n",
      "iteration: 22100 loss: 0.0089 lr: 0.005\n",
      "iteration: 22200 loss: 0.0088 lr: 0.005\n",
      "iteration: 22300 loss: 0.0090 lr: 0.005\n",
      "iteration: 22400 loss: 0.0087 lr: 0.005\n",
      "iteration: 22500 loss: 0.0085 lr: 0.005\n",
      "iteration: 22600 loss: 0.0084 lr: 0.005\n",
      "iteration: 22700 loss: 0.0081 lr: 0.005\n",
      "iteration: 22800 loss: 0.0092 lr: 0.005\n",
      "iteration: 22900 loss: 0.0091 lr: 0.005\n",
      "iteration: 23000 loss: 0.0094 lr: 0.005\n",
      "iteration: 23100 loss: 0.0089 lr: 0.005\n",
      "iteration: 23200 loss: 0.0090 lr: 0.005\n",
      "iteration: 23300 loss: 0.0091 lr: 0.005\n",
      "iteration: 23400 loss: 0.0101 lr: 0.005\n",
      "iteration: 23500 loss: 0.0088 lr: 0.005\n",
      "iteration: 23600 loss: 0.0085 lr: 0.005\n",
      "iteration: 23700 loss: 0.0092 lr: 0.005\n",
      "iteration: 23800 loss: 0.0082 lr: 0.005\n",
      "iteration: 23900 loss: 0.0085 lr: 0.005\n",
      "iteration: 24000 loss: 0.0089 lr: 0.005\n",
      "iteration: 24100 loss: 0.0086 lr: 0.005\n",
      "iteration: 24200 loss: 0.0080 lr: 0.005\n",
      "iteration: 24300 loss: 0.0089 lr: 0.005\n",
      "iteration: 24400 loss: 0.0088 lr: 0.005\n",
      "iteration: 24500 loss: 0.0080 lr: 0.005\n",
      "iteration: 24600 loss: 0.0083 lr: 0.005\n",
      "iteration: 24700 loss: 0.0077 lr: 0.005\n",
      "iteration: 24800 loss: 0.0088 lr: 0.005\n",
      "iteration: 24900 loss: 0.0081 lr: 0.005\n",
      "iteration: 25000 loss: 0.0090 lr: 0.005\n",
      "iteration: 25100 loss: 0.0087 lr: 0.005\n",
      "iteration: 25200 loss: 0.0087 lr: 0.005\n",
      "iteration: 25300 loss: 0.0082 lr: 0.005\n",
      "iteration: 25400 loss: 0.0091 lr: 0.005\n",
      "iteration: 25500 loss: 0.0088 lr: 0.005\n",
      "iteration: 25600 loss: 0.0079 lr: 0.005\n",
      "iteration: 25700 loss: 0.0081 lr: 0.005\n",
      "iteration: 25800 loss: 0.0090 lr: 0.005\n",
      "iteration: 25900 loss: 0.0090 lr: 0.005\n",
      "iteration: 26000 loss: 0.0090 lr: 0.005\n",
      "iteration: 26100 loss: 0.0092 lr: 0.005\n",
      "iteration: 26200 loss: 0.0088 lr: 0.005\n",
      "iteration: 26300 loss: 0.0081 lr: 0.005\n",
      "iteration: 26400 loss: 0.0071 lr: 0.005\n",
      "iteration: 26500 loss: 0.0084 lr: 0.005\n",
      "iteration: 26600 loss: 0.0090 lr: 0.005\n",
      "iteration: 26700 loss: 0.0087 lr: 0.005\n",
      "iteration: 26800 loss: 0.0081 lr: 0.005\n",
      "iteration: 26900 loss: 0.0089 lr: 0.005\n",
      "iteration: 27000 loss: 0.0081 lr: 0.005\n",
      "iteration: 27100 loss: 0.0087 lr: 0.005\n",
      "iteration: 27200 loss: 0.0085 lr: 0.005\n",
      "iteration: 27300 loss: 0.0079 lr: 0.005\n",
      "iteration: 27400 loss: 0.0076 lr: 0.005\n",
      "iteration: 27500 loss: 0.0082 lr: 0.005\n",
      "iteration: 27600 loss: 0.0078 lr: 0.005\n",
      "iteration: 27700 loss: 0.0078 lr: 0.005\n",
      "iteration: 27800 loss: 0.0073 lr: 0.005\n",
      "iteration: 27900 loss: 0.0079 lr: 0.005\n",
      "iteration: 28000 loss: 0.0081 lr: 0.005\n",
      "iteration: 28100 loss: 0.0080 lr: 0.005\n",
      "iteration: 28200 loss: 0.0084 lr: 0.005\n",
      "iteration: 28300 loss: 0.0080 lr: 0.005\n",
      "iteration: 28400 loss: 0.0079 lr: 0.005\n",
      "iteration: 28500 loss: 0.0077 lr: 0.005\n",
      "iteration: 28600 loss: 0.0080 lr: 0.005\n",
      "iteration: 28700 loss: 0.0078 lr: 0.005\n",
      "iteration: 28800 loss: 0.0087 lr: 0.005\n",
      "iteration: 28900 loss: 0.0078 lr: 0.005\n",
      "iteration: 29000 loss: 0.0084 lr: 0.005\n",
      "iteration: 29100 loss: 0.0075 lr: 0.005\n",
      "iteration: 29200 loss: 0.0088 lr: 0.005\n",
      "iteration: 29300 loss: 0.0085 lr: 0.005\n",
      "iteration: 29400 loss: 0.0072 lr: 0.005\n",
      "iteration: 29500 loss: 0.0084 lr: 0.005\n",
      "iteration: 29600 loss: 0.0074 lr: 0.005\n",
      "iteration: 29700 loss: 0.0090 lr: 0.005\n",
      "iteration: 29800 loss: 0.0089 lr: 0.005\n",
      "iteration: 29900 loss: 0.0079 lr: 0.005\n",
      "iteration: 30000 loss: 0.0064 lr: 0.005\n",
      "iteration: 30100 loss: 0.0074 lr: 0.005\n",
      "iteration: 30200 loss: 0.0076 lr: 0.005\n",
      "iteration: 30300 loss: 0.0078 lr: 0.005\n",
      "iteration: 30400 loss: 0.0077 lr: 0.005\n",
      "iteration: 30500 loss: 0.0072 lr: 0.005\n",
      "iteration: 30600 loss: 0.0077 lr: 0.005\n",
      "iteration: 30700 loss: 0.0075 lr: 0.005\n",
      "iteration: 30800 loss: 0.0072 lr: 0.005\n",
      "iteration: 30900 loss: 0.0072 lr: 0.005\n",
      "iteration: 31000 loss: 0.0077 lr: 0.005\n",
      "iteration: 31100 loss: 0.0070 lr: 0.005\n",
      "iteration: 31200 loss: 0.0071 lr: 0.005\n",
      "iteration: 31300 loss: 0.0077 lr: 0.005\n",
      "iteration: 31400 loss: 0.0075 lr: 0.005\n",
      "iteration: 31500 loss: 0.0074 lr: 0.005\n",
      "iteration: 31600 loss: 0.0076 lr: 0.005\n",
      "iteration: 31700 loss: 0.0082 lr: 0.005\n",
      "iteration: 31800 loss: 0.0071 lr: 0.005\n",
      "iteration: 31900 loss: 0.0079 lr: 0.005\n",
      "iteration: 32000 loss: 0.0078 lr: 0.005\n",
      "iteration: 32100 loss: 0.0066 lr: 0.005\n",
      "iteration: 32200 loss: 0.0082 lr: 0.005\n",
      "iteration: 32300 loss: 0.0076 lr: 0.005\n",
      "iteration: 32400 loss: 0.0072 lr: 0.005\n",
      "iteration: 32500 loss: 0.0077 lr: 0.005\n",
      "iteration: 32600 loss: 0.0064 lr: 0.005\n",
      "iteration: 32700 loss: 0.0080 lr: 0.005\n",
      "iteration: 32800 loss: 0.0074 lr: 0.005\n",
      "iteration: 32900 loss: 0.0070 lr: 0.005\n",
      "iteration: 33000 loss: 0.0071 lr: 0.005\n",
      "iteration: 33100 loss: 0.0081 lr: 0.005\n",
      "iteration: 33200 loss: 0.0077 lr: 0.005\n",
      "iteration: 33300 loss: 0.0076 lr: 0.005\n",
      "iteration: 33400 loss: 0.0083 lr: 0.005\n",
      "iteration: 33500 loss: 0.0068 lr: 0.005\n",
      "iteration: 33600 loss: 0.0071 lr: 0.005\n",
      "iteration: 33700 loss: 0.0078 lr: 0.005\n",
      "iteration: 33800 loss: 0.0077 lr: 0.005\n",
      "iteration: 33900 loss: 0.0071 lr: 0.005\n",
      "iteration: 34000 loss: 0.0073 lr: 0.005\n",
      "iteration: 34100 loss: 0.0064 lr: 0.005\n",
      "iteration: 34200 loss: 0.0070 lr: 0.005\n",
      "iteration: 34300 loss: 0.0084 lr: 0.005\n",
      "iteration: 34400 loss: 0.0068 lr: 0.005\n",
      "iteration: 34500 loss: 0.0079 lr: 0.005\n",
      "iteration: 34600 loss: 0.0068 lr: 0.005\n",
      "iteration: 34700 loss: 0.0073 lr: 0.005\n",
      "iteration: 34800 loss: 0.0070 lr: 0.005\n",
      "iteration: 34900 loss: 0.0076 lr: 0.005\n",
      "iteration: 35000 loss: 0.0071 lr: 0.005\n",
      "iteration: 35100 loss: 0.0079 lr: 0.005\n",
      "iteration: 35200 loss: 0.0074 lr: 0.005\n",
      "iteration: 35300 loss: 0.0077 lr: 0.005\n",
      "iteration: 35400 loss: 0.0069 lr: 0.005\n",
      "iteration: 35500 loss: 0.0072 lr: 0.005\n",
      "iteration: 35600 loss: 0.0073 lr: 0.005\n",
      "iteration: 35700 loss: 0.0064 lr: 0.005\n",
      "iteration: 35800 loss: 0.0076 lr: 0.005\n",
      "iteration: 35900 loss: 0.0070 lr: 0.005\n",
      "iteration: 36000 loss: 0.0070 lr: 0.005\n",
      "iteration: 36100 loss: 0.0063 lr: 0.005\n",
      "iteration: 36200 loss: 0.0071 lr: 0.005\n",
      "iteration: 36300 loss: 0.0065 lr: 0.005\n",
      "iteration: 36400 loss: 0.0078 lr: 0.005\n",
      "iteration: 36500 loss: 0.0069 lr: 0.005\n",
      "iteration: 36600 loss: 0.0070 lr: 0.005\n",
      "iteration: 36700 loss: 0.0059 lr: 0.005\n",
      "iteration: 36800 loss: 0.0065 lr: 0.005\n",
      "iteration: 36900 loss: 0.0065 lr: 0.005\n",
      "iteration: 37000 loss: 0.0063 lr: 0.005\n",
      "iteration: 37100 loss: 0.0060 lr: 0.005\n",
      "iteration: 37200 loss: 0.0057 lr: 0.005\n",
      "iteration: 37300 loss: 0.0070 lr: 0.005\n",
      "iteration: 37400 loss: 0.0067 lr: 0.005\n",
      "iteration: 37500 loss: 0.0062 lr: 0.005\n",
      "iteration: 37600 loss: 0.0066 lr: 0.005\n",
      "iteration: 37700 loss: 0.0063 lr: 0.005\n",
      "iteration: 37800 loss: 0.0062 lr: 0.005\n",
      "iteration: 37900 loss: 0.0059 lr: 0.005\n",
      "iteration: 38000 loss: 0.0066 lr: 0.005\n",
      "iteration: 38100 loss: 0.0068 lr: 0.005\n",
      "iteration: 38200 loss: 0.0067 lr: 0.005\n",
      "iteration: 38300 loss: 0.0076 lr: 0.005\n",
      "iteration: 38400 loss: 0.0072 lr: 0.005\n",
      "iteration: 38500 loss: 0.0068 lr: 0.005\n",
      "iteration: 38600 loss: 0.0068 lr: 0.005\n",
      "iteration: 38700 loss: 0.0062 lr: 0.005\n",
      "iteration: 38800 loss: 0.0062 lr: 0.005\n",
      "iteration: 38900 loss: 0.0059 lr: 0.005\n",
      "iteration: 39000 loss: 0.0059 lr: 0.005\n",
      "iteration: 39100 loss: 0.0068 lr: 0.005\n",
      "iteration: 39200 loss: 0.0066 lr: 0.005\n",
      "iteration: 39300 loss: 0.0066 lr: 0.005\n",
      "iteration: 39400 loss: 0.0061 lr: 0.005\n",
      "iteration: 39500 loss: 0.0071 lr: 0.005\n",
      "iteration: 39600 loss: 0.0069 lr: 0.005\n",
      "iteration: 39700 loss: 0.0068 lr: 0.005\n",
      "iteration: 39800 loss: 0.0065 lr: 0.005\n",
      "iteration: 39900 loss: 0.0058 lr: 0.005\n",
      "iteration: 40000 loss: 0.0055 lr: 0.005\n",
      "iteration: 40100 loss: 0.0063 lr: 0.005\n",
      "iteration: 40200 loss: 0.0079 lr: 0.005\n",
      "iteration: 40300 loss: 0.0071 lr: 0.005\n",
      "iteration: 40400 loss: 0.0059 lr: 0.005\n",
      "iteration: 40500 loss: 0.0059 lr: 0.005\n",
      "iteration: 40600 loss: 0.0067 lr: 0.005\n",
      "iteration: 40700 loss: 0.0067 lr: 0.005\n",
      "iteration: 40800 loss: 0.0076 lr: 0.005\n",
      "iteration: 40900 loss: 0.0069 lr: 0.005\n",
      "iteration: 41000 loss: 0.0062 lr: 0.005\n",
      "iteration: 41100 loss: 0.0074 lr: 0.005\n",
      "iteration: 41200 loss: 0.0069 lr: 0.005\n",
      "iteration: 41300 loss: 0.0072 lr: 0.005\n",
      "iteration: 41400 loss: 0.0062 lr: 0.005\n",
      "iteration: 41500 loss: 0.0066 lr: 0.005\n",
      "iteration: 41600 loss: 0.0063 lr: 0.005\n",
      "iteration: 41700 loss: 0.0060 lr: 0.005\n",
      "iteration: 41800 loss: 0.0063 lr: 0.005\n",
      "iteration: 41900 loss: 0.0062 lr: 0.005\n",
      "iteration: 42000 loss: 0.0060 lr: 0.005\n",
      "iteration: 42100 loss: 0.0062 lr: 0.005\n",
      "iteration: 42200 loss: 0.0067 lr: 0.005\n",
      "iteration: 42300 loss: 0.0062 lr: 0.005\n",
      "iteration: 42400 loss: 0.0074 lr: 0.005\n",
      "iteration: 42500 loss: 0.0061 lr: 0.005\n",
      "iteration: 42600 loss: 0.0061 lr: 0.005\n",
      "iteration: 42700 loss: 0.0075 lr: 0.005\n",
      "iteration: 42800 loss: 0.0064 lr: 0.005\n",
      "iteration: 42900 loss: 0.0061 lr: 0.005\n",
      "iteration: 43000 loss: 0.0055 lr: 0.005\n",
      "iteration: 43100 loss: 0.0064 lr: 0.005\n",
      "iteration: 43200 loss: 0.0057 lr: 0.005\n",
      "iteration: 43300 loss: 0.0066 lr: 0.005\n",
      "iteration: 43400 loss: 0.0062 lr: 0.005\n",
      "iteration: 43500 loss: 0.0062 lr: 0.005\n",
      "iteration: 43600 loss: 0.0071 lr: 0.005\n",
      "iteration: 43700 loss: 0.0066 lr: 0.005\n",
      "iteration: 43800 loss: 0.0067 lr: 0.005\n",
      "iteration: 43900 loss: 0.0069 lr: 0.005\n",
      "iteration: 44000 loss: 0.0059 lr: 0.005\n",
      "iteration: 44100 loss: 0.0058 lr: 0.005\n",
      "iteration: 44200 loss: 0.0066 lr: 0.005\n",
      "iteration: 44300 loss: 0.0060 lr: 0.005\n",
      "iteration: 44400 loss: 0.0062 lr: 0.005\n",
      "iteration: 44500 loss: 0.0072 lr: 0.005\n",
      "iteration: 44600 loss: 0.0061 lr: 0.005\n",
      "iteration: 44700 loss: 0.0060 lr: 0.005\n",
      "iteration: 44800 loss: 0.0065 lr: 0.005\n",
      "iteration: 44900 loss: 0.0063 lr: 0.005\n",
      "iteration: 45000 loss: 0.0059 lr: 0.005\n",
      "iteration: 45100 loss: 0.0060 lr: 0.005\n",
      "iteration: 45200 loss: 0.0061 lr: 0.005\n",
      "iteration: 45300 loss: 0.0056 lr: 0.005\n",
      "iteration: 45400 loss: 0.0061 lr: 0.005\n",
      "iteration: 45500 loss: 0.0060 lr: 0.005\n",
      "iteration: 45600 loss: 0.0065 lr: 0.005\n",
      "iteration: 45700 loss: 0.0062 lr: 0.005\n",
      "iteration: 45800 loss: 0.0062 lr: 0.005\n",
      "iteration: 45900 loss: 0.0079 lr: 0.005\n",
      "iteration: 46000 loss: 0.0058 lr: 0.005\n",
      "iteration: 46100 loss: 0.0058 lr: 0.005\n",
      "iteration: 46200 loss: 0.0069 lr: 0.005\n",
      "iteration: 46300 loss: 0.0061 lr: 0.005\n",
      "iteration: 46400 loss: 0.0059 lr: 0.005\n",
      "iteration: 46500 loss: 0.0055 lr: 0.005\n",
      "iteration: 46600 loss: 0.0048 lr: 0.005\n",
      "iteration: 46700 loss: 0.0066 lr: 0.005\n",
      "iteration: 46800 loss: 0.0055 lr: 0.005\n",
      "iteration: 46900 loss: 0.0059 lr: 0.005\n",
      "iteration: 47000 loss: 0.0069 lr: 0.005\n",
      "iteration: 47100 loss: 0.0068 lr: 0.005\n",
      "iteration: 47200 loss: 0.0057 lr: 0.005\n",
      "iteration: 47300 loss: 0.0054 lr: 0.005\n",
      "iteration: 47400 loss: 0.0057 lr: 0.005\n",
      "iteration: 47500 loss: 0.0057 lr: 0.005\n",
      "iteration: 47600 loss: 0.0067 lr: 0.005\n",
      "iteration: 47700 loss: 0.0061 lr: 0.005\n",
      "iteration: 47800 loss: 0.0063 lr: 0.005\n",
      "iteration: 47900 loss: 0.0063 lr: 0.005\n",
      "iteration: 48000 loss: 0.0055 lr: 0.005\n",
      "iteration: 48100 loss: 0.0064 lr: 0.005\n",
      "iteration: 48200 loss: 0.0061 lr: 0.005\n",
      "iteration: 48300 loss: 0.0060 lr: 0.005\n",
      "iteration: 48400 loss: 0.0061 lr: 0.005\n",
      "iteration: 48500 loss: 0.0058 lr: 0.005\n",
      "iteration: 48600 loss: 0.0063 lr: 0.005\n",
      "iteration: 48700 loss: 0.0072 lr: 0.005\n",
      "iteration: 48800 loss: 0.0061 lr: 0.005\n",
      "iteration: 48900 loss: 0.0067 lr: 0.005\n",
      "iteration: 49000 loss: 0.0062 lr: 0.005\n",
      "iteration: 49100 loss: 0.0062 lr: 0.005\n",
      "iteration: 49200 loss: 0.0061 lr: 0.005\n",
      "iteration: 49300 loss: 0.0060 lr: 0.005\n",
      "iteration: 49400 loss: 0.0064 lr: 0.005\n",
      "iteration: 49500 loss: 0.0056 lr: 0.005\n",
      "iteration: 49600 loss: 0.0057 lr: 0.005\n",
      "iteration: 49700 loss: 0.0068 lr: 0.005\n",
      "iteration: 49800 loss: 0.0057 lr: 0.005\n",
      "iteration: 49900 loss: 0.0056 lr: 0.005\n",
      "iteration: 50000 loss: 0.0062 lr: 0.005\n",
      "iteration: 50100 loss: 0.0060 lr: 0.005\n",
      "iteration: 50200 loss: 0.0069 lr: 0.005\n",
      "iteration: 50300 loss: 0.0062 lr: 0.005\n",
      "iteration: 50400 loss: 0.0060 lr: 0.005\n",
      "iteration: 50500 loss: 0.0062 lr: 0.005\n",
      "iteration: 50600 loss: 0.0057 lr: 0.005\n",
      "iteration: 50700 loss: 0.0055 lr: 0.005\n",
      "iteration: 50800 loss: 0.0056 lr: 0.005\n",
      "iteration: 50900 loss: 0.0065 lr: 0.005\n",
      "iteration: 51000 loss: 0.0060 lr: 0.005\n",
      "iteration: 51100 loss: 0.0059 lr: 0.005\n",
      "iteration: 51200 loss: 0.0051 lr: 0.005\n",
      "iteration: 51300 loss: 0.0054 lr: 0.005\n",
      "iteration: 51400 loss: 0.0070 lr: 0.005\n",
      "iteration: 51500 loss: 0.0056 lr: 0.005\n",
      "iteration: 51600 loss: 0.0065 lr: 0.005\n",
      "iteration: 51700 loss: 0.0052 lr: 0.005\n",
      "iteration: 51800 loss: 0.0066 lr: 0.005\n",
      "iteration: 51900 loss: 0.0056 lr: 0.005\n",
      "iteration: 52000 loss: 0.0062 lr: 0.005\n",
      "iteration: 52100 loss: 0.0061 lr: 0.005\n",
      "iteration: 52200 loss: 0.0059 lr: 0.005\n",
      "iteration: 52300 loss: 0.0065 lr: 0.005\n",
      "iteration: 52400 loss: 0.0060 lr: 0.005\n",
      "iteration: 52500 loss: 0.0064 lr: 0.005\n",
      "iteration: 52600 loss: 0.0058 lr: 0.005\n",
      "iteration: 52700 loss: 0.0057 lr: 0.005\n",
      "iteration: 52800 loss: 0.0053 lr: 0.005\n",
      "iteration: 52900 loss: 0.0059 lr: 0.005\n",
      "iteration: 53000 loss: 0.0065 lr: 0.005\n",
      "iteration: 53100 loss: 0.0064 lr: 0.005\n",
      "iteration: 53200 loss: 0.0067 lr: 0.005\n",
      "iteration: 53300 loss: 0.0055 lr: 0.005\n",
      "iteration: 53400 loss: 0.0059 lr: 0.005\n",
      "iteration: 53500 loss: 0.0058 lr: 0.005\n",
      "iteration: 53600 loss: 0.0063 lr: 0.005\n",
      "iteration: 53700 loss: 0.0053 lr: 0.005\n",
      "iteration: 53800 loss: 0.0058 lr: 0.005\n",
      "iteration: 53900 loss: 0.0065 lr: 0.005\n",
      "iteration: 54000 loss: 0.0064 lr: 0.005\n",
      "iteration: 54100 loss: 0.0055 lr: 0.005\n",
      "iteration: 54200 loss: 0.0063 lr: 0.005\n",
      "iteration: 54300 loss: 0.0060 lr: 0.005\n",
      "iteration: 54400 loss: 0.0063 lr: 0.005\n",
      "iteration: 54500 loss: 0.0054 lr: 0.005\n",
      "iteration: 54600 loss: 0.0056 lr: 0.005\n",
      "iteration: 54700 loss: 0.0052 lr: 0.005\n",
      "iteration: 54800 loss: 0.0059 lr: 0.005\n",
      "iteration: 54900 loss: 0.0061 lr: 0.005\n",
      "iteration: 55000 loss: 0.0066 lr: 0.005\n",
      "iteration: 55100 loss: 0.0058 lr: 0.005\n",
      "iteration: 55200 loss: 0.0063 lr: 0.005\n",
      "iteration: 55300 loss: 0.0070 lr: 0.005\n",
      "iteration: 55400 loss: 0.0052 lr: 0.005\n",
      "iteration: 55500 loss: 0.0056 lr: 0.005\n",
      "iteration: 55600 loss: 0.0055 lr: 0.005\n",
      "iteration: 55700 loss: 0.0051 lr: 0.005\n",
      "iteration: 55800 loss: 0.0063 lr: 0.005\n",
      "iteration: 55900 loss: 0.0052 lr: 0.005\n",
      "iteration: 56000 loss: 0.0058 lr: 0.005\n",
      "iteration: 56100 loss: 0.0066 lr: 0.005\n",
      "iteration: 56200 loss: 0.0054 lr: 0.005\n",
      "iteration: 56300 loss: 0.0053 lr: 0.005\n",
      "iteration: 56400 loss: 0.0063 lr: 0.005\n",
      "iteration: 56500 loss: 0.0052 lr: 0.005\n",
      "iteration: 56600 loss: 0.0053 lr: 0.005\n",
      "iteration: 56700 loss: 0.0060 lr: 0.005\n",
      "iteration: 56800 loss: 0.0059 lr: 0.005\n",
      "iteration: 56900 loss: 0.0055 lr: 0.005\n",
      "iteration: 57000 loss: 0.0062 lr: 0.005\n",
      "iteration: 57100 loss: 0.0059 lr: 0.005\n",
      "iteration: 57200 loss: 0.0054 lr: 0.005\n",
      "iteration: 57300 loss: 0.0063 lr: 0.005\n",
      "iteration: 57400 loss: 0.0053 lr: 0.005\n",
      "iteration: 57500 loss: 0.0059 lr: 0.005\n",
      "iteration: 57600 loss: 0.0053 lr: 0.005\n",
      "iteration: 57700 loss: 0.0053 lr: 0.005\n",
      "iteration: 57800 loss: 0.0049 lr: 0.005\n",
      "iteration: 57900 loss: 0.0061 lr: 0.005\n",
      "iteration: 58000 loss: 0.0058 lr: 0.005\n",
      "iteration: 58100 loss: 0.0058 lr: 0.005\n",
      "iteration: 58200 loss: 0.0052 lr: 0.005\n",
      "iteration: 58300 loss: 0.0054 lr: 0.005\n",
      "iteration: 58400 loss: 0.0058 lr: 0.005\n",
      "iteration: 58500 loss: 0.0063 lr: 0.005\n",
      "iteration: 58600 loss: 0.0054 lr: 0.005\n",
      "iteration: 58700 loss: 0.0064 lr: 0.005\n",
      "iteration: 58800 loss: 0.0054 lr: 0.005\n",
      "iteration: 58900 loss: 0.0051 lr: 0.005\n",
      "iteration: 59000 loss: 0.0055 lr: 0.005\n",
      "iteration: 59100 loss: 0.0057 lr: 0.005\n",
      "iteration: 59200 loss: 0.0055 lr: 0.005\n",
      "iteration: 59300 loss: 0.0062 lr: 0.005\n",
      "iteration: 59400 loss: 0.0050 lr: 0.005\n",
      "iteration: 59500 loss: 0.0053 lr: 0.005\n",
      "iteration: 59600 loss: 0.0049 lr: 0.005\n",
      "iteration: 59700 loss: 0.0055 lr: 0.005\n",
      "iteration: 59800 loss: 0.0055 lr: 0.005\n",
      "iteration: 59900 loss: 0.0058 lr: 0.005\n",
      "iteration: 60000 loss: 0.0054 lr: 0.005\n",
      "iteration: 60100 loss: 0.0072 lr: 0.005\n",
      "iteration: 60200 loss: 0.0059 lr: 0.005\n",
      "iteration: 60300 loss: 0.0062 lr: 0.005\n",
      "iteration: 60400 loss: 0.0054 lr: 0.005\n",
      "iteration: 60500 loss: 0.0054 lr: 0.005\n",
      "iteration: 60600 loss: 0.0055 lr: 0.005\n",
      "iteration: 60700 loss: 0.0060 lr: 0.005\n",
      "iteration: 60800 loss: 0.0062 lr: 0.005\n",
      "iteration: 60900 loss: 0.0052 lr: 0.005\n",
      "iteration: 61000 loss: 0.0049 lr: 0.005\n",
      "iteration: 61100 loss: 0.0056 lr: 0.005\n",
      "iteration: 61200 loss: 0.0047 lr: 0.005\n",
      "iteration: 61300 loss: 0.0054 lr: 0.005\n",
      "iteration: 61400 loss: 0.0057 lr: 0.005\n",
      "iteration: 61500 loss: 0.0054 lr: 0.005\n",
      "iteration: 61600 loss: 0.0061 lr: 0.005\n",
      "iteration: 61700 loss: 0.0051 lr: 0.005\n",
      "iteration: 61800 loss: 0.0056 lr: 0.005\n",
      "iteration: 61900 loss: 0.0048 lr: 0.005\n",
      "iteration: 62000 loss: 0.0060 lr: 0.005\n",
      "iteration: 62100 loss: 0.0060 lr: 0.005\n",
      "iteration: 62200 loss: 0.0060 lr: 0.005\n",
      "iteration: 62300 loss: 0.0059 lr: 0.005\n",
      "iteration: 62400 loss: 0.0057 lr: 0.005\n",
      "iteration: 62500 loss: 0.0055 lr: 0.005\n",
      "iteration: 62600 loss: 0.0055 lr: 0.005\n",
      "iteration: 62700 loss: 0.0058 lr: 0.005\n",
      "iteration: 62800 loss: 0.0056 lr: 0.005\n",
      "iteration: 62900 loss: 0.0065 lr: 0.005\n",
      "iteration: 63000 loss: 0.0053 lr: 0.005\n",
      "iteration: 63100 loss: 0.0053 lr: 0.005\n",
      "iteration: 63200 loss: 0.0049 lr: 0.005\n",
      "iteration: 63300 loss: 0.0052 lr: 0.005\n",
      "iteration: 63400 loss: 0.0054 lr: 0.005\n",
      "iteration: 63500 loss: 0.0052 lr: 0.005\n",
      "iteration: 63600 loss: 0.0054 lr: 0.005\n",
      "iteration: 63700 loss: 0.0058 lr: 0.005\n",
      "iteration: 63800 loss: 0.0056 lr: 0.005\n",
      "iteration: 63900 loss: 0.0054 lr: 0.005\n",
      "iteration: 64000 loss: 0.0058 lr: 0.005\n",
      "iteration: 64100 loss: 0.0050 lr: 0.005\n",
      "iteration: 64200 loss: 0.0056 lr: 0.005\n",
      "iteration: 64300 loss: 0.0059 lr: 0.005\n",
      "iteration: 64400 loss: 0.0052 lr: 0.005\n",
      "iteration: 64500 loss: 0.0062 lr: 0.005\n",
      "iteration: 64600 loss: 0.0046 lr: 0.005\n",
      "iteration: 64700 loss: 0.0055 lr: 0.005\n",
      "iteration: 64800 loss: 0.0058 lr: 0.005\n",
      "iteration: 64900 loss: 0.0056 lr: 0.005\n",
      "iteration: 65000 loss: 0.0047 lr: 0.005\n",
      "iteration: 65100 loss: 0.0057 lr: 0.005\n",
      "iteration: 65200 loss: 0.0054 lr: 0.005\n",
      "iteration: 65300 loss: 0.0054 lr: 0.005\n",
      "iteration: 65400 loss: 0.0050 lr: 0.005\n",
      "iteration: 65500 loss: 0.0042 lr: 0.005\n",
      "iteration: 65600 loss: 0.0053 lr: 0.005\n",
      "iteration: 65700 loss: 0.0051 lr: 0.005\n",
      "iteration: 65800 loss: 0.0055 lr: 0.005\n",
      "iteration: 65900 loss: 0.0051 lr: 0.005\n",
      "iteration: 66000 loss: 0.0057 lr: 0.005\n",
      "iteration: 66100 loss: 0.0056 lr: 0.005\n",
      "iteration: 66200 loss: 0.0054 lr: 0.005\n",
      "iteration: 66300 loss: 0.0050 lr: 0.005\n",
      "iteration: 66400 loss: 0.0050 lr: 0.005\n",
      "iteration: 66500 loss: 0.0061 lr: 0.005\n",
      "iteration: 66600 loss: 0.0054 lr: 0.005\n",
      "iteration: 66700 loss: 0.0064 lr: 0.005\n",
      "iteration: 66800 loss: 0.0051 lr: 0.005\n",
      "iteration: 66900 loss: 0.0054 lr: 0.005\n",
      "iteration: 67000 loss: 0.0059 lr: 0.005\n",
      "iteration: 67100 loss: 0.0053 lr: 0.005\n",
      "iteration: 67200 loss: 0.0043 lr: 0.005\n",
      "iteration: 67300 loss: 0.0053 lr: 0.005\n",
      "iteration: 67400 loss: 0.0058 lr: 0.005\n",
      "iteration: 67500 loss: 0.0065 lr: 0.005\n",
      "iteration: 67600 loss: 0.0059 lr: 0.005\n",
      "iteration: 67700 loss: 0.0051 lr: 0.005\n",
      "iteration: 67800 loss: 0.0054 lr: 0.005\n",
      "iteration: 67900 loss: 0.0053 lr: 0.005\n",
      "iteration: 68000 loss: 0.0051 lr: 0.005\n",
      "iteration: 68100 loss: 0.0051 lr: 0.005\n",
      "iteration: 68200 loss: 0.0052 lr: 0.005\n",
      "iteration: 68300 loss: 0.0053 lr: 0.005\n",
      "iteration: 68400 loss: 0.0057 lr: 0.005\n",
      "iteration: 68500 loss: 0.0051 lr: 0.005\n",
      "iteration: 68600 loss: 0.0053 lr: 0.005\n",
      "iteration: 68700 loss: 0.0064 lr: 0.005\n",
      "iteration: 68800 loss: 0.0052 lr: 0.005\n",
      "iteration: 68900 loss: 0.0057 lr: 0.005\n",
      "iteration: 69000 loss: 0.0067 lr: 0.005\n",
      "iteration: 69100 loss: 0.0056 lr: 0.005\n",
      "iteration: 69200 loss: 0.0053 lr: 0.005\n",
      "iteration: 69300 loss: 0.0047 lr: 0.005\n",
      "iteration: 69400 loss: 0.0053 lr: 0.005\n",
      "iteration: 69500 loss: 0.0049 lr: 0.005\n",
      "iteration: 69600 loss: 0.0053 lr: 0.005\n",
      "iteration: 69700 loss: 0.0045 lr: 0.005\n",
      "iteration: 69800 loss: 0.0048 lr: 0.005\n",
      "iteration: 69900 loss: 0.0046 lr: 0.005\n",
      "iteration: 70000 loss: 0.0049 lr: 0.005\n",
      "iteration: 70100 loss: 0.0046 lr: 0.005\n",
      "iteration: 70200 loss: 0.0056 lr: 0.005\n",
      "iteration: 70300 loss: 0.0053 lr: 0.005\n",
      "iteration: 70400 loss: 0.0050 lr: 0.005\n",
      "iteration: 70500 loss: 0.0054 lr: 0.005\n",
      "iteration: 70600 loss: 0.0047 lr: 0.005\n",
      "iteration: 70700 loss: 0.0048 lr: 0.005\n",
      "iteration: 70800 loss: 0.0057 lr: 0.005\n",
      "iteration: 70900 loss: 0.0058 lr: 0.005\n",
      "iteration: 71000 loss: 0.0057 lr: 0.005\n",
      "iteration: 71100 loss: 0.0059 lr: 0.005\n",
      "iteration: 71200 loss: 0.0047 lr: 0.005\n",
      "iteration: 71300 loss: 0.0054 lr: 0.005\n",
      "iteration: 71400 loss: 0.0052 lr: 0.005\n",
      "iteration: 71500 loss: 0.0050 lr: 0.005\n",
      "iteration: 71600 loss: 0.0047 lr: 0.005\n",
      "iteration: 71700 loss: 0.0051 lr: 0.005\n",
      "iteration: 71800 loss: 0.0056 lr: 0.005\n",
      "iteration: 71900 loss: 0.0056 lr: 0.005\n",
      "iteration: 72000 loss: 0.0050 lr: 0.005\n",
      "iteration: 72100 loss: 0.0053 lr: 0.005\n",
      "iteration: 72200 loss: 0.0050 lr: 0.005\n",
      "iteration: 72300 loss: 0.0053 lr: 0.005\n",
      "iteration: 72400 loss: 0.0052 lr: 0.005\n",
      "iteration: 72500 loss: 0.0052 lr: 0.005\n",
      "iteration: 72600 loss: 0.0051 lr: 0.005\n",
      "iteration: 72700 loss: 0.0047 lr: 0.005\n",
      "iteration: 72800 loss: 0.0057 lr: 0.005\n",
      "iteration: 72900 loss: 0.0045 lr: 0.005\n",
      "iteration: 73000 loss: 0.0060 lr: 0.005\n",
      "iteration: 73100 loss: 0.0052 lr: 0.005\n",
      "iteration: 73200 loss: 0.0056 lr: 0.005\n",
      "iteration: 73300 loss: 0.0053 lr: 0.005\n",
      "iteration: 73400 loss: 0.0055 lr: 0.005\n",
      "iteration: 73500 loss: 0.0049 lr: 0.005\n",
      "iteration: 73600 loss: 0.0051 lr: 0.005\n",
      "iteration: 73700 loss: 0.0055 lr: 0.005\n",
      "iteration: 73800 loss: 0.0051 lr: 0.005\n",
      "iteration: 73900 loss: 0.0053 lr: 0.005\n",
      "iteration: 74000 loss: 0.0047 lr: 0.005\n",
      "iteration: 74100 loss: 0.0049 lr: 0.005\n",
      "iteration: 74200 loss: 0.0050 lr: 0.005\n",
      "iteration: 74300 loss: 0.0045 lr: 0.005\n",
      "iteration: 74400 loss: 0.0046 lr: 0.005\n",
      "iteration: 74500 loss: 0.0049 lr: 0.005\n",
      "iteration: 74600 loss: 0.0056 lr: 0.005\n",
      "iteration: 74700 loss: 0.0054 lr: 0.005\n",
      "iteration: 74800 loss: 0.0047 lr: 0.005\n",
      "iteration: 74900 loss: 0.0047 lr: 0.005\n",
      "iteration: 75000 loss: 0.0048 lr: 0.005\n",
      "iteration: 75100 loss: 0.0051 lr: 0.005\n",
      "iteration: 75200 loss: 0.0058 lr: 0.005\n",
      "iteration: 75300 loss: 0.0048 lr: 0.005\n",
      "iteration: 75400 loss: 0.0049 lr: 0.005\n",
      "iteration: 75500 loss: 0.0050 lr: 0.005\n",
      "iteration: 75600 loss: 0.0048 lr: 0.005\n",
      "iteration: 75700 loss: 0.0053 lr: 0.005\n",
      "iteration: 75800 loss: 0.0050 lr: 0.005\n",
      "iteration: 75900 loss: 0.0046 lr: 0.005\n",
      "iteration: 76000 loss: 0.0047 lr: 0.005\n",
      "iteration: 76100 loss: 0.0043 lr: 0.005\n",
      "iteration: 76200 loss: 0.0042 lr: 0.005\n",
      "iteration: 76300 loss: 0.0051 lr: 0.005\n",
      "iteration: 76400 loss: 0.0050 lr: 0.005\n",
      "iteration: 76500 loss: 0.0050 lr: 0.005\n",
      "iteration: 76600 loss: 0.0051 lr: 0.005\n",
      "iteration: 76700 loss: 0.0045 lr: 0.005\n",
      "iteration: 76800 loss: 0.0042 lr: 0.005\n",
      "iteration: 76900 loss: 0.0054 lr: 0.005\n",
      "iteration: 77000 loss: 0.0049 lr: 0.005\n",
      "iteration: 77100 loss: 0.0045 lr: 0.005\n",
      "iteration: 77200 loss: 0.0054 lr: 0.005\n",
      "iteration: 77300 loss: 0.0051 lr: 0.005\n",
      "iteration: 77400 loss: 0.0044 lr: 0.005\n",
      "iteration: 77500 loss: 0.0049 lr: 0.005\n",
      "iteration: 77600 loss: 0.0048 lr: 0.005\n",
      "iteration: 77700 loss: 0.0059 lr: 0.005\n",
      "iteration: 77800 loss: 0.0051 lr: 0.005\n",
      "iteration: 77900 loss: 0.0051 lr: 0.005\n",
      "iteration: 78000 loss: 0.0062 lr: 0.005\n",
      "iteration: 78100 loss: 0.0052 lr: 0.005\n",
      "iteration: 78200 loss: 0.0048 lr: 0.005\n",
      "iteration: 78300 loss: 0.0047 lr: 0.005\n",
      "iteration: 78400 loss: 0.0060 lr: 0.005\n",
      "iteration: 78500 loss: 0.0044 lr: 0.005\n",
      "iteration: 78600 loss: 0.0047 lr: 0.005\n",
      "iteration: 78700 loss: 0.0054 lr: 0.005\n",
      "iteration: 78800 loss: 0.0058 lr: 0.005\n",
      "iteration: 78900 loss: 0.0056 lr: 0.005\n",
      "iteration: 79000 loss: 0.0050 lr: 0.005\n",
      "iteration: 79100 loss: 0.0050 lr: 0.005\n",
      "iteration: 79200 loss: 0.0050 lr: 0.005\n",
      "iteration: 79300 loss: 0.0059 lr: 0.005\n",
      "iteration: 79400 loss: 0.0049 lr: 0.005\n",
      "iteration: 79500 loss: 0.0055 lr: 0.005\n",
      "iteration: 79600 loss: 0.0046 lr: 0.005\n",
      "iteration: 79700 loss: 0.0052 lr: 0.005\n",
      "iteration: 79800 loss: 0.0053 lr: 0.005\n",
      "iteration: 79900 loss: 0.0047 lr: 0.005\n",
      "iteration: 80000 loss: 0.0047 lr: 0.005\n",
      "iteration: 80100 loss: 0.0050 lr: 0.005\n",
      "iteration: 80200 loss: 0.0055 lr: 0.005\n",
      "iteration: 80300 loss: 0.0053 lr: 0.005\n",
      "iteration: 80400 loss: 0.0037 lr: 0.005\n",
      "iteration: 80500 loss: 0.0049 lr: 0.005\n",
      "iteration: 80600 loss: 0.0049 lr: 0.005\n",
      "iteration: 80700 loss: 0.0050 lr: 0.005\n",
      "iteration: 80800 loss: 0.0052 lr: 0.005\n",
      "iteration: 80900 loss: 0.0054 lr: 0.005\n",
      "iteration: 81000 loss: 0.0052 lr: 0.005\n",
      "iteration: 81100 loss: 0.0048 lr: 0.005\n",
      "iteration: 81200 loss: 0.0043 lr: 0.005\n",
      "iteration: 81300 loss: 0.0054 lr: 0.005\n",
      "iteration: 81400 loss: 0.0052 lr: 0.005\n",
      "iteration: 81500 loss: 0.0052 lr: 0.005\n",
      "iteration: 81600 loss: 0.0053 lr: 0.005\n",
      "iteration: 81700 loss: 0.0045 lr: 0.005\n",
      "iteration: 81800 loss: 0.0051 lr: 0.005\n",
      "iteration: 81900 loss: 0.0046 lr: 0.005\n",
      "iteration: 82000 loss: 0.0047 lr: 0.005\n",
      "iteration: 82100 loss: 0.0043 lr: 0.005\n",
      "iteration: 82200 loss: 0.0055 lr: 0.005\n",
      "iteration: 82300 loss: 0.0051 lr: 0.005\n",
      "iteration: 82400 loss: 0.0061 lr: 0.005\n",
      "iteration: 82500 loss: 0.0047 lr: 0.005\n",
      "iteration: 82600 loss: 0.0053 lr: 0.005\n",
      "iteration: 82700 loss: 0.0048 lr: 0.005\n",
      "iteration: 82800 loss: 0.0049 lr: 0.005\n",
      "iteration: 82900 loss: 0.0042 lr: 0.005\n",
      "iteration: 83000 loss: 0.0044 lr: 0.005\n",
      "iteration: 83100 loss: 0.0046 lr: 0.005\n",
      "iteration: 83200 loss: 0.0059 lr: 0.005\n",
      "iteration: 83300 loss: 0.0055 lr: 0.005\n",
      "iteration: 83400 loss: 0.0056 lr: 0.005\n",
      "iteration: 83500 loss: 0.0052 lr: 0.005\n",
      "iteration: 83600 loss: 0.0056 lr: 0.005\n",
      "iteration: 83700 loss: 0.0055 lr: 0.005\n",
      "iteration: 83800 loss: 0.0059 lr: 0.005\n",
      "iteration: 83900 loss: 0.0046 lr: 0.005\n",
      "iteration: 84000 loss: 0.0053 lr: 0.005\n",
      "iteration: 84100 loss: 0.0047 lr: 0.005\n",
      "iteration: 84200 loss: 0.0050 lr: 0.005\n",
      "iteration: 84300 loss: 0.0055 lr: 0.005\n",
      "iteration: 84400 loss: 0.0050 lr: 0.005\n",
      "iteration: 84500 loss: 0.0056 lr: 0.005\n",
      "iteration: 84600 loss: 0.0049 lr: 0.005\n",
      "iteration: 84700 loss: 0.0041 lr: 0.005\n",
      "iteration: 84800 loss: 0.0045 lr: 0.005\n",
      "iteration: 84900 loss: 0.0044 lr: 0.005\n",
      "iteration: 85000 loss: 0.0049 lr: 0.005\n",
      "iteration: 85100 loss: 0.0053 lr: 0.005\n",
      "iteration: 85200 loss: 0.0046 lr: 0.005\n",
      "iteration: 85300 loss: 0.0044 lr: 0.005\n",
      "iteration: 85400 loss: 0.0047 lr: 0.005\n",
      "iteration: 85500 loss: 0.0051 lr: 0.005\n",
      "iteration: 85600 loss: 0.0055 lr: 0.005\n",
      "iteration: 85700 loss: 0.0048 lr: 0.005\n",
      "iteration: 85800 loss: 0.0052 lr: 0.005\n",
      "iteration: 85900 loss: 0.0050 lr: 0.005\n",
      "iteration: 86000 loss: 0.0051 lr: 0.005\n",
      "iteration: 86100 loss: 0.0058 lr: 0.005\n",
      "iteration: 86200 loss: 0.0048 lr: 0.005\n",
      "iteration: 86300 loss: 0.0052 lr: 0.005\n",
      "iteration: 86400 loss: 0.0048 lr: 0.005\n",
      "iteration: 86500 loss: 0.0047 lr: 0.005\n",
      "iteration: 86600 loss: 0.0043 lr: 0.005\n",
      "iteration: 86700 loss: 0.0044 lr: 0.005\n",
      "iteration: 86800 loss: 0.0049 lr: 0.005\n",
      "iteration: 86900 loss: 0.0044 lr: 0.005\n",
      "iteration: 87000 loss: 0.0049 lr: 0.005\n",
      "iteration: 87100 loss: 0.0045 lr: 0.005\n",
      "iteration: 87200 loss: 0.0046 lr: 0.005\n",
      "iteration: 87300 loss: 0.0043 lr: 0.005\n",
      "iteration: 87400 loss: 0.0045 lr: 0.005\n",
      "iteration: 87500 loss: 0.0047 lr: 0.005\n",
      "iteration: 87600 loss: 0.0044 lr: 0.005\n",
      "iteration: 87700 loss: 0.0053 lr: 0.005\n",
      "iteration: 87800 loss: 0.0049 lr: 0.005\n",
      "iteration: 87900 loss: 0.0054 lr: 0.005\n",
      "iteration: 88000 loss: 0.0048 lr: 0.005\n",
      "iteration: 88100 loss: 0.0048 lr: 0.005\n",
      "iteration: 88200 loss: 0.0051 lr: 0.005\n",
      "iteration: 88300 loss: 0.0053 lr: 0.005\n",
      "iteration: 88400 loss: 0.0053 lr: 0.005\n",
      "iteration: 88500 loss: 0.0047 lr: 0.005\n",
      "iteration: 88600 loss: 0.0048 lr: 0.005\n",
      "iteration: 88700 loss: 0.0047 lr: 0.005\n",
      "iteration: 88800 loss: 0.0051 lr: 0.005\n",
      "iteration: 88900 loss: 0.0045 lr: 0.005\n",
      "iteration: 89000 loss: 0.0055 lr: 0.005\n",
      "iteration: 89100 loss: 0.0054 lr: 0.005\n",
      "iteration: 89200 loss: 0.0049 lr: 0.005\n",
      "iteration: 89300 loss: 0.0051 lr: 0.005\n",
      "iteration: 89400 loss: 0.0049 lr: 0.005\n",
      "iteration: 89500 loss: 0.0047 lr: 0.005\n",
      "iteration: 89600 loss: 0.0054 lr: 0.005\n",
      "iteration: 89700 loss: 0.0048 lr: 0.005\n",
      "iteration: 89800 loss: 0.0058 lr: 0.005\n",
      "iteration: 89900 loss: 0.0048 lr: 0.005\n",
      "iteration: 90000 loss: 0.0051 lr: 0.005\n",
      "iteration: 90100 loss: 0.0053 lr: 0.005\n",
      "iteration: 90200 loss: 0.0045 lr: 0.005\n",
      "iteration: 90300 loss: 0.0049 lr: 0.005\n",
      "iteration: 90400 loss: 0.0056 lr: 0.005\n",
      "iteration: 90500 loss: 0.0053 lr: 0.005\n",
      "iteration: 90600 loss: 0.0054 lr: 0.005\n",
      "iteration: 90700 loss: 0.0056 lr: 0.005\n",
      "iteration: 90800 loss: 0.0042 lr: 0.005\n",
      "iteration: 90900 loss: 0.0050 lr: 0.005\n",
      "iteration: 91000 loss: 0.0052 lr: 0.005\n",
      "iteration: 91100 loss: 0.0045 lr: 0.005\n",
      "iteration: 91200 loss: 0.0057 lr: 0.005\n",
      "iteration: 91300 loss: 0.0047 lr: 0.005\n",
      "iteration: 91400 loss: 0.0043 lr: 0.005\n",
      "iteration: 91500 loss: 0.0052 lr: 0.005\n",
      "iteration: 91600 loss: 0.0055 lr: 0.005\n",
      "iteration: 91700 loss: 0.0040 lr: 0.005\n",
      "iteration: 91800 loss: 0.0048 lr: 0.005\n",
      "iteration: 91900 loss: 0.0047 lr: 0.005\n",
      "iteration: 92000 loss: 0.0048 lr: 0.005\n",
      "iteration: 92100 loss: 0.0048 lr: 0.005\n",
      "iteration: 92200 loss: 0.0049 lr: 0.005\n",
      "iteration: 92300 loss: 0.0051 lr: 0.005\n",
      "iteration: 92400 loss: 0.0048 lr: 0.005\n",
      "iteration: 92500 loss: 0.0049 lr: 0.005\n",
      "iteration: 92600 loss: 0.0052 lr: 0.005\n",
      "iteration: 92700 loss: 0.0048 lr: 0.005\n",
      "iteration: 92800 loss: 0.0044 lr: 0.005\n",
      "iteration: 92900 loss: 0.0045 lr: 0.005\n",
      "iteration: 93000 loss: 0.0053 lr: 0.005\n",
      "iteration: 93100 loss: 0.0043 lr: 0.005\n",
      "iteration: 93200 loss: 0.0050 lr: 0.005\n",
      "iteration: 93300 loss: 0.0045 lr: 0.005\n",
      "iteration: 93400 loss: 0.0046 lr: 0.005\n",
      "iteration: 93500 loss: 0.0046 lr: 0.005\n",
      "iteration: 93600 loss: 0.0044 lr: 0.005\n",
      "iteration: 93700 loss: 0.0047 lr: 0.005\n",
      "iteration: 93800 loss: 0.0050 lr: 0.005\n",
      "iteration: 93900 loss: 0.0054 lr: 0.005\n",
      "iteration: 94000 loss: 0.0047 lr: 0.005\n",
      "iteration: 94100 loss: 0.0045 lr: 0.005\n",
      "iteration: 94200 loss: 0.0053 lr: 0.005\n",
      "iteration: 94300 loss: 0.0046 lr: 0.005\n",
      "iteration: 94400 loss: 0.0047 lr: 0.005\n",
      "iteration: 94500 loss: 0.0050 lr: 0.005\n",
      "iteration: 94600 loss: 0.0047 lr: 0.005\n",
      "iteration: 94700 loss: 0.0048 lr: 0.005\n",
      "iteration: 94800 loss: 0.0042 lr: 0.005\n",
      "iteration: 94900 loss: 0.0044 lr: 0.005\n",
      "iteration: 95000 loss: 0.0046 lr: 0.005\n",
      "iteration: 95100 loss: 0.0053 lr: 0.005\n",
      "iteration: 95200 loss: 0.0042 lr: 0.005\n",
      "iteration: 95300 loss: 0.0048 lr: 0.005\n",
      "iteration: 95400 loss: 0.0049 lr: 0.005\n",
      "iteration: 95500 loss: 0.0036 lr: 0.005\n",
      "iteration: 95600 loss: 0.0049 lr: 0.005\n",
      "iteration: 95700 loss: 0.0047 lr: 0.005\n",
      "iteration: 95800 loss: 0.0041 lr: 0.005\n",
      "iteration: 95900 loss: 0.0047 lr: 0.005\n",
      "iteration: 96000 loss: 0.0041 lr: 0.005\n",
      "iteration: 96100 loss: 0.0054 lr: 0.005\n",
      "iteration: 96200 loss: 0.0048 lr: 0.005\n",
      "iteration: 96300 loss: 0.0049 lr: 0.005\n",
      "iteration: 96400 loss: 0.0044 lr: 0.005\n",
      "iteration: 96500 loss: 0.0045 lr: 0.005\n",
      "iteration: 96600 loss: 0.0060 lr: 0.005\n",
      "iteration: 96700 loss: 0.0054 lr: 0.005\n",
      "iteration: 96800 loss: 0.0049 lr: 0.005\n",
      "iteration: 96900 loss: 0.0046 lr: 0.005\n",
      "iteration: 97000 loss: 0.0043 lr: 0.005\n",
      "iteration: 97100 loss: 0.0060 lr: 0.005\n",
      "iteration: 97200 loss: 0.0046 lr: 0.005\n",
      "iteration: 97300 loss: 0.0051 lr: 0.005\n",
      "iteration: 97400 loss: 0.0051 lr: 0.005\n",
      "iteration: 97500 loss: 0.0047 lr: 0.005\n",
      "iteration: 97600 loss: 0.0050 lr: 0.005\n",
      "iteration: 97700 loss: 0.0043 lr: 0.005\n",
      "iteration: 97800 loss: 0.0048 lr: 0.005\n",
      "iteration: 97900 loss: 0.0049 lr: 0.005\n",
      "iteration: 98000 loss: 0.0052 lr: 0.005\n",
      "iteration: 98100 loss: 0.0048 lr: 0.005\n",
      "iteration: 98200 loss: 0.0046 lr: 0.005\n",
      "iteration: 98300 loss: 0.0051 lr: 0.005\n",
      "iteration: 98400 loss: 0.0048 lr: 0.005\n",
      "iteration: 98500 loss: 0.0044 lr: 0.005\n",
      "iteration: 98600 loss: 0.0046 lr: 0.005\n",
      "iteration: 98700 loss: 0.0043 lr: 0.005\n",
      "iteration: 98800 loss: 0.0052 lr: 0.005\n",
      "iteration: 98900 loss: 0.0042 lr: 0.005\n",
      "iteration: 99000 loss: 0.0041 lr: 0.005\n",
      "iteration: 99100 loss: 0.0049 lr: 0.005\n",
      "iteration: 99200 loss: 0.0044 lr: 0.005\n",
      "iteration: 99300 loss: 0.0046 lr: 0.005\n",
      "iteration: 99400 loss: 0.0042 lr: 0.005\n",
      "iteration: 99500 loss: 0.0047 lr: 0.005\n",
      "iteration: 99600 loss: 0.0045 lr: 0.005\n",
      "iteration: 99700 loss: 0.0042 lr: 0.005\n",
      "iteration: 99800 loss: 0.0042 lr: 0.005\n",
      "iteration: 99900 loss: 0.0041 lr: 0.005\n",
      "iteration: 100000 loss: 0.0050 lr: 0.005\n",
      "iteration: 100100 loss: 0.0043 lr: 0.005\n",
      "iteration: 100200 loss: 0.0049 lr: 0.005\n",
      "iteration: 100300 loss: 0.0046 lr: 0.005\n",
      "iteration: 100400 loss: 0.0047 lr: 0.005\n",
      "iteration: 100500 loss: 0.0041 lr: 0.005\n",
      "iteration: 100600 loss: 0.0047 lr: 0.005\n",
      "iteration: 100700 loss: 0.0053 lr: 0.005\n",
      "iteration: 100800 loss: 0.0047 lr: 0.005\n",
      "iteration: 100900 loss: 0.0050 lr: 0.005\n",
      "iteration: 101000 loss: 0.0052 lr: 0.005\n",
      "iteration: 101100 loss: 0.0047 lr: 0.005\n",
      "iteration: 101200 loss: 0.0046 lr: 0.005\n",
      "iteration: 101300 loss: 0.0038 lr: 0.005\n",
      "iteration: 101400 loss: 0.0040 lr: 0.005\n",
      "iteration: 101500 loss: 0.0040 lr: 0.005\n",
      "iteration: 101600 loss: 0.0049 lr: 0.005\n",
      "iteration: 101700 loss: 0.0053 lr: 0.005\n",
      "iteration: 101800 loss: 0.0049 lr: 0.005\n",
      "iteration: 101900 loss: 0.0040 lr: 0.005\n",
      "iteration: 102000 loss: 0.0041 lr: 0.005\n",
      "iteration: 102100 loss: 0.0048 lr: 0.005\n",
      "iteration: 102200 loss: 0.0045 lr: 0.005\n",
      "iteration: 102300 loss: 0.0044 lr: 0.005\n",
      "iteration: 102400 loss: 0.0039 lr: 0.005\n",
      "iteration: 102500 loss: 0.0042 lr: 0.005\n",
      "iteration: 102600 loss: 0.0048 lr: 0.005\n",
      "iteration: 102700 loss: 0.0043 lr: 0.005\n",
      "iteration: 102800 loss: 0.0038 lr: 0.005\n",
      "iteration: 102900 loss: 0.0049 lr: 0.005\n",
      "iteration: 103000 loss: 0.0052 lr: 0.005\n",
      "iteration: 103100 loss: 0.0042 lr: 0.005\n",
      "iteration: 103200 loss: 0.0045 lr: 0.005\n",
      "iteration: 103300 loss: 0.0047 lr: 0.005\n",
      "iteration: 103400 loss: 0.0046 lr: 0.005\n",
      "iteration: 103500 loss: 0.0047 lr: 0.005\n",
      "iteration: 103600 loss: 0.0049 lr: 0.005\n",
      "iteration: 103700 loss: 0.0046 lr: 0.005\n",
      "iteration: 103800 loss: 0.0039 lr: 0.005\n",
      "iteration: 103900 loss: 0.0045 lr: 0.005\n",
      "iteration: 104000 loss: 0.0052 lr: 0.005\n",
      "iteration: 104100 loss: 0.0040 lr: 0.005\n",
      "iteration: 104200 loss: 0.0052 lr: 0.005\n",
      "iteration: 104300 loss: 0.0042 lr: 0.005\n",
      "iteration: 104400 loss: 0.0051 lr: 0.005\n",
      "iteration: 104500 loss: 0.0047 lr: 0.005\n",
      "iteration: 104600 loss: 0.0050 lr: 0.005\n",
      "iteration: 104700 loss: 0.0049 lr: 0.005\n",
      "iteration: 104800 loss: 0.0056 lr: 0.005\n",
      "iteration: 104900 loss: 0.0039 lr: 0.005\n",
      "iteration: 105000 loss: 0.0041 lr: 0.005\n",
      "iteration: 105100 loss: 0.0048 lr: 0.005\n",
      "iteration: 105200 loss: 0.0043 lr: 0.005\n",
      "iteration: 105300 loss: 0.0040 lr: 0.005\n",
      "iteration: 105400 loss: 0.0045 lr: 0.005\n",
      "iteration: 105500 loss: 0.0042 lr: 0.005\n",
      "iteration: 105600 loss: 0.0041 lr: 0.005\n",
      "iteration: 105700 loss: 0.0042 lr: 0.005\n",
      "iteration: 105800 loss: 0.0046 lr: 0.005\n",
      "iteration: 105900 loss: 0.0038 lr: 0.005\n",
      "iteration: 106000 loss: 0.0055 lr: 0.005\n",
      "iteration: 106100 loss: 0.0043 lr: 0.005\n",
      "iteration: 106200 loss: 0.0046 lr: 0.005\n",
      "iteration: 106300 loss: 0.0042 lr: 0.005\n",
      "iteration: 106400 loss: 0.0042 lr: 0.005\n",
      "iteration: 106500 loss: 0.0046 lr: 0.005\n",
      "iteration: 106600 loss: 0.0042 lr: 0.005\n",
      "iteration: 106700 loss: 0.0051 lr: 0.005\n",
      "iteration: 106800 loss: 0.0044 lr: 0.005\n",
      "iteration: 106900 loss: 0.0054 lr: 0.005\n",
      "iteration: 107000 loss: 0.0045 lr: 0.005\n",
      "iteration: 107100 loss: 0.0041 lr: 0.005\n",
      "iteration: 107200 loss: 0.0042 lr: 0.005\n",
      "iteration: 107300 loss: 0.0045 lr: 0.005\n",
      "iteration: 107400 loss: 0.0049 lr: 0.005\n",
      "iteration: 107500 loss: 0.0043 lr: 0.005\n",
      "iteration: 107600 loss: 0.0038 lr: 0.005\n",
      "iteration: 107700 loss: 0.0054 lr: 0.005\n",
      "iteration: 107800 loss: 0.0043 lr: 0.005\n",
      "iteration: 107900 loss: 0.0043 lr: 0.005\n",
      "iteration: 108000 loss: 0.0046 lr: 0.005\n",
      "iteration: 108100 loss: 0.0048 lr: 0.005\n",
      "iteration: 108200 loss: 0.0039 lr: 0.005\n",
      "iteration: 108300 loss: 0.0050 lr: 0.005\n",
      "iteration: 108400 loss: 0.0047 lr: 0.005\n",
      "iteration: 108500 loss: 0.0044 lr: 0.005\n",
      "iteration: 108600 loss: 0.0042 lr: 0.005\n",
      "iteration: 108700 loss: 0.0052 lr: 0.005\n",
      "iteration: 108800 loss: 0.0047 lr: 0.005\n",
      "iteration: 108900 loss: 0.0043 lr: 0.005\n",
      "iteration: 109000 loss: 0.0044 lr: 0.005\n",
      "iteration: 109100 loss: 0.0042 lr: 0.005\n",
      "iteration: 109200 loss: 0.0045 lr: 0.005\n",
      "iteration: 109300 loss: 0.0047 lr: 0.005\n",
      "iteration: 109400 loss: 0.0044 lr: 0.005\n",
      "iteration: 109500 loss: 0.0046 lr: 0.005\n",
      "iteration: 109600 loss: 0.0046 lr: 0.005\n",
      "iteration: 109700 loss: 0.0040 lr: 0.005\n",
      "iteration: 109800 loss: 0.0040 lr: 0.005\n",
      "iteration: 109900 loss: 0.0046 lr: 0.005\n",
      "iteration: 110000 loss: 0.0050 lr: 0.005\n",
      "iteration: 110100 loss: 0.0051 lr: 0.005\n",
      "iteration: 110200 loss: 0.0045 lr: 0.005\n",
      "iteration: 110300 loss: 0.0043 lr: 0.005\n",
      "iteration: 110400 loss: 0.0050 lr: 0.005\n",
      "iteration: 110500 loss: 0.0046 lr: 0.005\n",
      "iteration: 110600 loss: 0.0042 lr: 0.005\n",
      "iteration: 110700 loss: 0.0039 lr: 0.005\n",
      "iteration: 110800 loss: 0.0044 lr: 0.005\n",
      "iteration: 110900 loss: 0.0049 lr: 0.005\n",
      "iteration: 111000 loss: 0.0043 lr: 0.005\n",
      "iteration: 111100 loss: 0.0045 lr: 0.005\n",
      "iteration: 111200 loss: 0.0043 lr: 0.005\n",
      "iteration: 111300 loss: 0.0045 lr: 0.005\n",
      "iteration: 111400 loss: 0.0047 lr: 0.005\n",
      "iteration: 111500 loss: 0.0044 lr: 0.005\n",
      "iteration: 111600 loss: 0.0042 lr: 0.005\n",
      "iteration: 111700 loss: 0.0042 lr: 0.005\n",
      "iteration: 111800 loss: 0.0044 lr: 0.005\n",
      "iteration: 111900 loss: 0.0045 lr: 0.005\n",
      "iteration: 112000 loss: 0.0040 lr: 0.005\n",
      "iteration: 112100 loss: 0.0042 lr: 0.005\n",
      "iteration: 112200 loss: 0.0034 lr: 0.005\n",
      "iteration: 112300 loss: 0.0052 lr: 0.005\n",
      "iteration: 112400 loss: 0.0047 lr: 0.005\n",
      "iteration: 112500 loss: 0.0041 lr: 0.005\n",
      "iteration: 112600 loss: 0.0045 lr: 0.005\n",
      "iteration: 112700 loss: 0.0040 lr: 0.005\n",
      "iteration: 112800 loss: 0.0039 lr: 0.005\n",
      "iteration: 112900 loss: 0.0038 lr: 0.005\n",
      "iteration: 113000 loss: 0.0038 lr: 0.005\n",
      "iteration: 113100 loss: 0.0041 lr: 0.005\n",
      "iteration: 113200 loss: 0.0042 lr: 0.005\n",
      "iteration: 113300 loss: 0.0036 lr: 0.005\n",
      "iteration: 113400 loss: 0.0050 lr: 0.005\n",
      "iteration: 113500 loss: 0.0045 lr: 0.005\n",
      "iteration: 113600 loss: 0.0039 lr: 0.005\n",
      "iteration: 113700 loss: 0.0039 lr: 0.005\n",
      "iteration: 113800 loss: 0.0045 lr: 0.005\n",
      "iteration: 113900 loss: 0.0043 lr: 0.005\n",
      "iteration: 114000 loss: 0.0052 lr: 0.005\n",
      "iteration: 114100 loss: 0.0043 lr: 0.005\n",
      "iteration: 114200 loss: 0.0042 lr: 0.005\n",
      "iteration: 114300 loss: 0.0049 lr: 0.005\n",
      "iteration: 114400 loss: 0.0047 lr: 0.005\n",
      "iteration: 114500 loss: 0.0053 lr: 0.005\n",
      "iteration: 114600 loss: 0.0041 lr: 0.005\n",
      "iteration: 114700 loss: 0.0049 lr: 0.005\n",
      "iteration: 114800 loss: 0.0043 lr: 0.005\n",
      "iteration: 114900 loss: 0.0050 lr: 0.005\n",
      "iteration: 115000 loss: 0.0043 lr: 0.005\n",
      "iteration: 115100 loss: 0.0046 lr: 0.005\n",
      "iteration: 115200 loss: 0.0047 lr: 0.005\n",
      "iteration: 115300 loss: 0.0041 lr: 0.005\n",
      "iteration: 115400 loss: 0.0043 lr: 0.005\n",
      "iteration: 115500 loss: 0.0042 lr: 0.005\n",
      "iteration: 115600 loss: 0.0040 lr: 0.005\n",
      "iteration: 115700 loss: 0.0044 lr: 0.005\n",
      "iteration: 115800 loss: 0.0045 lr: 0.005\n",
      "iteration: 115900 loss: 0.0038 lr: 0.005\n",
      "iteration: 116000 loss: 0.0044 lr: 0.005\n",
      "iteration: 116100 loss: 0.0043 lr: 0.005\n",
      "iteration: 116200 loss: 0.0050 lr: 0.005\n",
      "iteration: 116300 loss: 0.0040 lr: 0.005\n",
      "iteration: 116400 loss: 0.0043 lr: 0.005\n",
      "iteration: 116500 loss: 0.0042 lr: 0.005\n",
      "iteration: 116600 loss: 0.0044 lr: 0.005\n",
      "iteration: 116700 loss: 0.0044 lr: 0.005\n",
      "iteration: 116800 loss: 0.0043 lr: 0.005\n",
      "iteration: 116900 loss: 0.0035 lr: 0.005\n",
      "iteration: 117000 loss: 0.0045 lr: 0.005\n",
      "iteration: 117100 loss: 0.0041 lr: 0.005\n",
      "iteration: 117200 loss: 0.0039 lr: 0.005\n",
      "iteration: 117300 loss: 0.0043 lr: 0.005\n",
      "iteration: 117400 loss: 0.0045 lr: 0.005\n",
      "iteration: 117500 loss: 0.0043 lr: 0.005\n",
      "iteration: 117600 loss: 0.0048 lr: 0.005\n",
      "iteration: 117700 loss: 0.0039 lr: 0.005\n",
      "iteration: 117800 loss: 0.0040 lr: 0.005\n",
      "iteration: 117900 loss: 0.0041 lr: 0.005\n",
      "iteration: 118000 loss: 0.0047 lr: 0.005\n",
      "iteration: 118100 loss: 0.0045 lr: 0.005\n",
      "iteration: 118200 loss: 0.0044 lr: 0.005\n",
      "iteration: 118300 loss: 0.0041 lr: 0.005\n",
      "iteration: 118400 loss: 0.0042 lr: 0.005\n",
      "iteration: 118500 loss: 0.0048 lr: 0.005\n",
      "iteration: 118600 loss: 0.0035 lr: 0.005\n",
      "iteration: 118700 loss: 0.0044 lr: 0.005\n",
      "iteration: 118800 loss: 0.0039 lr: 0.005\n",
      "iteration: 118900 loss: 0.0040 lr: 0.005\n",
      "iteration: 119000 loss: 0.0041 lr: 0.005\n",
      "iteration: 119100 loss: 0.0049 lr: 0.005\n",
      "iteration: 119200 loss: 0.0043 lr: 0.005\n",
      "iteration: 119300 loss: 0.0039 lr: 0.005\n",
      "iteration: 119400 loss: 0.0040 lr: 0.005\n",
      "iteration: 119500 loss: 0.0038 lr: 0.005\n",
      "iteration: 119600 loss: 0.0044 lr: 0.005\n",
      "iteration: 119700 loss: 0.0042 lr: 0.005\n",
      "iteration: 119800 loss: 0.0037 lr: 0.005\n",
      "iteration: 119900 loss: 0.0047 lr: 0.005\n",
      "iteration: 120000 loss: 0.0042 lr: 0.005\n",
      "iteration: 120100 loss: 0.0041 lr: 0.005\n",
      "iteration: 120200 loss: 0.0044 lr: 0.005\n",
      "iteration: 120300 loss: 0.0036 lr: 0.005\n",
      "iteration: 120400 loss: 0.0047 lr: 0.005\n",
      "iteration: 120500 loss: 0.0042 lr: 0.005\n",
      "iteration: 120600 loss: 0.0039 lr: 0.005\n",
      "iteration: 120700 loss: 0.0043 lr: 0.005\n",
      "iteration: 120800 loss: 0.0045 lr: 0.005\n",
      "iteration: 120900 loss: 0.0046 lr: 0.005\n",
      "iteration: 121000 loss: 0.0044 lr: 0.005\n",
      "iteration: 121100 loss: 0.0044 lr: 0.005\n",
      "iteration: 121200 loss: 0.0046 lr: 0.005\n",
      "iteration: 121300 loss: 0.0043 lr: 0.005\n",
      "iteration: 121400 loss: 0.0041 lr: 0.005\n",
      "iteration: 121500 loss: 0.0036 lr: 0.005\n",
      "iteration: 121600 loss: 0.0038 lr: 0.005\n",
      "iteration: 121700 loss: 0.0045 lr: 0.005\n",
      "iteration: 121800 loss: 0.0043 lr: 0.005\n",
      "iteration: 121900 loss: 0.0039 lr: 0.005\n",
      "iteration: 122000 loss: 0.0041 lr: 0.005\n",
      "iteration: 122100 loss: 0.0045 lr: 0.005\n",
      "iteration: 122200 loss: 0.0040 lr: 0.005\n",
      "iteration: 122300 loss: 0.0053 lr: 0.005\n",
      "iteration: 122400 loss: 0.0041 lr: 0.005\n",
      "iteration: 122500 loss: 0.0041 lr: 0.005\n",
      "iteration: 122600 loss: 0.0043 lr: 0.005\n",
      "iteration: 122700 loss: 0.0048 lr: 0.005\n",
      "iteration: 122800 loss: 0.0049 lr: 0.005\n",
      "iteration: 122900 loss: 0.0049 lr: 0.005\n",
      "iteration: 123000 loss: 0.0044 lr: 0.005\n",
      "iteration: 123100 loss: 0.0047 lr: 0.005\n",
      "iteration: 123200 loss: 0.0036 lr: 0.005\n",
      "iteration: 123300 loss: 0.0037 lr: 0.005\n",
      "iteration: 123400 loss: 0.0042 lr: 0.005\n",
      "iteration: 123500 loss: 0.0042 lr: 0.005\n",
      "iteration: 123600 loss: 0.0049 lr: 0.005\n",
      "iteration: 123700 loss: 0.0043 lr: 0.005\n",
      "iteration: 123800 loss: 0.0043 lr: 0.005\n",
      "iteration: 123900 loss: 0.0045 lr: 0.005\n",
      "iteration: 124000 loss: 0.0043 lr: 0.005\n",
      "iteration: 124100 loss: 0.0034 lr: 0.005\n",
      "iteration: 124200 loss: 0.0043 lr: 0.005\n",
      "iteration: 124300 loss: 0.0041 lr: 0.005\n",
      "iteration: 124400 loss: 0.0038 lr: 0.005\n",
      "iteration: 124500 loss: 0.0043 lr: 0.005\n",
      "iteration: 124600 loss: 0.0047 lr: 0.005\n",
      "iteration: 124700 loss: 0.0038 lr: 0.005\n",
      "iteration: 124800 loss: 0.0039 lr: 0.005\n",
      "iteration: 124900 loss: 0.0045 lr: 0.005\n",
      "iteration: 125000 loss: 0.0045 lr: 0.005\n",
      "iteration: 125100 loss: 0.0042 lr: 0.005\n",
      "iteration: 125200 loss: 0.0046 lr: 0.005\n",
      "iteration: 125300 loss: 0.0045 lr: 0.005\n",
      "iteration: 125400 loss: 0.0045 lr: 0.005\n",
      "iteration: 125500 loss: 0.0047 lr: 0.005\n",
      "iteration: 125600 loss: 0.0042 lr: 0.005\n",
      "iteration: 125700 loss: 0.0040 lr: 0.005\n",
      "iteration: 125800 loss: 0.0043 lr: 0.005\n",
      "iteration: 125900 loss: 0.0039 lr: 0.005\n",
      "iteration: 126000 loss: 0.0042 lr: 0.005\n",
      "iteration: 126100 loss: 0.0041 lr: 0.005\n",
      "iteration: 126200 loss: 0.0043 lr: 0.005\n",
      "iteration: 126300 loss: 0.0051 lr: 0.005\n",
      "iteration: 126400 loss: 0.0046 lr: 0.005\n",
      "iteration: 126500 loss: 0.0040 lr: 0.005\n",
      "iteration: 126600 loss: 0.0040 lr: 0.005\n",
      "iteration: 126700 loss: 0.0050 lr: 0.005\n",
      "iteration: 126800 loss: 0.0037 lr: 0.005\n",
      "iteration: 126900 loss: 0.0045 lr: 0.005\n",
      "iteration: 127000 loss: 0.0043 lr: 0.005\n",
      "iteration: 127100 loss: 0.0044 lr: 0.005\n",
      "iteration: 127200 loss: 0.0039 lr: 0.005\n",
      "iteration: 127300 loss: 0.0043 lr: 0.005\n",
      "iteration: 127400 loss: 0.0038 lr: 0.005\n",
      "iteration: 127500 loss: 0.0043 lr: 0.005\n",
      "iteration: 127600 loss: 0.0048 lr: 0.005\n",
      "iteration: 127700 loss: 0.0041 lr: 0.005\n",
      "iteration: 127800 loss: 0.0042 lr: 0.005\n",
      "iteration: 127900 loss: 0.0038 lr: 0.005\n",
      "iteration: 128000 loss: 0.0035 lr: 0.005\n",
      "iteration: 128100 loss: 0.0041 lr: 0.005\n",
      "iteration: 128200 loss: 0.0050 lr: 0.005\n",
      "iteration: 128300 loss: 0.0042 lr: 0.005\n",
      "iteration: 128400 loss: 0.0046 lr: 0.005\n",
      "iteration: 128500 loss: 0.0045 lr: 0.005\n",
      "iteration: 128600 loss: 0.0040 lr: 0.005\n",
      "iteration: 128700 loss: 0.0035 lr: 0.005\n",
      "iteration: 128800 loss: 0.0044 lr: 0.005\n",
      "iteration: 128900 loss: 0.0047 lr: 0.005\n",
      "iteration: 129000 loss: 0.0037 lr: 0.005\n",
      "iteration: 129100 loss: 0.0046 lr: 0.005\n",
      "iteration: 129200 loss: 0.0042 lr: 0.005\n",
      "iteration: 129300 loss: 0.0039 lr: 0.005\n",
      "iteration: 129400 loss: 0.0037 lr: 0.005\n",
      "iteration: 129500 loss: 0.0048 lr: 0.005\n",
      "iteration: 129600 loss: 0.0047 lr: 0.005\n",
      "iteration: 129700 loss: 0.0037 lr: 0.005\n",
      "iteration: 129800 loss: 0.0040 lr: 0.005\n",
      "iteration: 129900 loss: 0.0040 lr: 0.005\n",
      "iteration: 130000 loss: 0.0036 lr: 0.005\n",
      "iteration: 130100 loss: 0.0042 lr: 0.005\n",
      "iteration: 130200 loss: 0.0036 lr: 0.005\n",
      "iteration: 130300 loss: 0.0043 lr: 0.005\n",
      "iteration: 130400 loss: 0.0044 lr: 0.005\n",
      "iteration: 130500 loss: 0.0042 lr: 0.005\n",
      "iteration: 130600 loss: 0.0043 lr: 0.005\n",
      "iteration: 130700 loss: 0.0044 lr: 0.005\n",
      "iteration: 130800 loss: 0.0050 lr: 0.005\n",
      "iteration: 130900 loss: 0.0040 lr: 0.005\n",
      "iteration: 131000 loss: 0.0045 lr: 0.005\n",
      "iteration: 131100 loss: 0.0044 lr: 0.005\n",
      "iteration: 131200 loss: 0.0044 lr: 0.005\n",
      "iteration: 131300 loss: 0.0042 lr: 0.005\n",
      "iteration: 131400 loss: 0.0040 lr: 0.005\n",
      "iteration: 131500 loss: 0.0047 lr: 0.005\n",
      "iteration: 131600 loss: 0.0038 lr: 0.005\n",
      "iteration: 131700 loss: 0.0045 lr: 0.005\n",
      "iteration: 131800 loss: 0.0046 lr: 0.005\n",
      "iteration: 131900 loss: 0.0038 lr: 0.005\n",
      "iteration: 132000 loss: 0.0039 lr: 0.005\n",
      "iteration: 132100 loss: 0.0038 lr: 0.005\n",
      "iteration: 132200 loss: 0.0041 lr: 0.005\n",
      "iteration: 132300 loss: 0.0043 lr: 0.005\n",
      "iteration: 132400 loss: 0.0042 lr: 0.005\n",
      "iteration: 132500 loss: 0.0037 lr: 0.005\n",
      "iteration: 132600 loss: 0.0047 lr: 0.005\n",
      "iteration: 132700 loss: 0.0040 lr: 0.005\n",
      "iteration: 132800 loss: 0.0046 lr: 0.005\n",
      "iteration: 132900 loss: 0.0040 lr: 0.005\n",
      "iteration: 133000 loss: 0.0046 lr: 0.005\n",
      "iteration: 133100 loss: 0.0051 lr: 0.005\n",
      "iteration: 133200 loss: 0.0038 lr: 0.005\n",
      "iteration: 133300 loss: 0.0042 lr: 0.005\n",
      "iteration: 133400 loss: 0.0040 lr: 0.005\n",
      "iteration: 133500 loss: 0.0050 lr: 0.005\n",
      "iteration: 133600 loss: 0.0049 lr: 0.005\n",
      "iteration: 133700 loss: 0.0044 lr: 0.005\n",
      "iteration: 133800 loss: 0.0039 lr: 0.005\n",
      "iteration: 133900 loss: 0.0045 lr: 0.005\n",
      "iteration: 134000 loss: 0.0041 lr: 0.005\n",
      "iteration: 134100 loss: 0.0040 lr: 0.005\n",
      "iteration: 134200 loss: 0.0045 lr: 0.005\n",
      "iteration: 134300 loss: 0.0041 lr: 0.005\n",
      "iteration: 134400 loss: 0.0040 lr: 0.005\n",
      "iteration: 134500 loss: 0.0039 lr: 0.005\n",
      "iteration: 134600 loss: 0.0036 lr: 0.005\n",
      "iteration: 134700 loss: 0.0042 lr: 0.005\n",
      "iteration: 134800 loss: 0.0044 lr: 0.005\n",
      "iteration: 134900 loss: 0.0042 lr: 0.005\n",
      "iteration: 135000 loss: 0.0047 lr: 0.005\n",
      "iteration: 135100 loss: 0.0047 lr: 0.005\n",
      "iteration: 135200 loss: 0.0050 lr: 0.005\n",
      "iteration: 135300 loss: 0.0038 lr: 0.005\n",
      "iteration: 135400 loss: 0.0042 lr: 0.005\n",
      "iteration: 135500 loss: 0.0043 lr: 0.005\n",
      "iteration: 135600 loss: 0.0042 lr: 0.005\n",
      "iteration: 135700 loss: 0.0039 lr: 0.005\n",
      "iteration: 135800 loss: 0.0035 lr: 0.005\n",
      "iteration: 135900 loss: 0.0046 lr: 0.005\n",
      "iteration: 136000 loss: 0.0047 lr: 0.005\n",
      "iteration: 136100 loss: 0.0039 lr: 0.005\n",
      "iteration: 136200 loss: 0.0039 lr: 0.005\n",
      "iteration: 136300 loss: 0.0043 lr: 0.005\n",
      "iteration: 136400 loss: 0.0036 lr: 0.005\n",
      "iteration: 136500 loss: 0.0040 lr: 0.005\n",
      "iteration: 136600 loss: 0.0034 lr: 0.005\n",
      "iteration: 136700 loss: 0.0041 lr: 0.005\n",
      "iteration: 136800 loss: 0.0044 lr: 0.005\n",
      "iteration: 136900 loss: 0.0043 lr: 0.005\n",
      "iteration: 137000 loss: 0.0043 lr: 0.005\n",
      "iteration: 137100 loss: 0.0032 lr: 0.005\n",
      "iteration: 137200 loss: 0.0040 lr: 0.005\n",
      "iteration: 137300 loss: 0.0036 lr: 0.005\n",
      "iteration: 137400 loss: 0.0033 lr: 0.005\n",
      "iteration: 137500 loss: 0.0043 lr: 0.005\n",
      "iteration: 137600 loss: 0.0040 lr: 0.005\n",
      "iteration: 137700 loss: 0.0041 lr: 0.005\n",
      "iteration: 137800 loss: 0.0040 lr: 0.005\n",
      "iteration: 137900 loss: 0.0044 lr: 0.005\n",
      "iteration: 138000 loss: 0.0042 lr: 0.005\n",
      "iteration: 138100 loss: 0.0048 lr: 0.005\n",
      "iteration: 138200 loss: 0.0043 lr: 0.005\n",
      "iteration: 138300 loss: 0.0039 lr: 0.005\n",
      "iteration: 138400 loss: 0.0042 lr: 0.005\n",
      "iteration: 138500 loss: 0.0041 lr: 0.005\n",
      "iteration: 138600 loss: 0.0035 lr: 0.005\n",
      "iteration: 138700 loss: 0.0043 lr: 0.005\n",
      "iteration: 138800 loss: 0.0047 lr: 0.005\n",
      "iteration: 138900 loss: 0.0038 lr: 0.005\n",
      "iteration: 139000 loss: 0.0041 lr: 0.005\n",
      "iteration: 139100 loss: 0.0035 lr: 0.005\n",
      "iteration: 139200 loss: 0.0040 lr: 0.005\n",
      "iteration: 139300 loss: 0.0033 lr: 0.005\n",
      "iteration: 139400 loss: 0.0038 lr: 0.005\n",
      "iteration: 139500 loss: 0.0045 lr: 0.005\n",
      "iteration: 139600 loss: 0.0044 lr: 0.005\n",
      "iteration: 139700 loss: 0.0034 lr: 0.005\n",
      "iteration: 139800 loss: 0.0041 lr: 0.005\n",
      "iteration: 139900 loss: 0.0043 lr: 0.005\n",
      "iteration: 140000 loss: 0.0038 lr: 0.005\n",
      "iteration: 140100 loss: 0.0044 lr: 0.005\n",
      "iteration: 140200 loss: 0.0035 lr: 0.005\n",
      "iteration: 140300 loss: 0.0036 lr: 0.005\n",
      "iteration: 140400 loss: 0.0041 lr: 0.005\n",
      "iteration: 140500 loss: 0.0043 lr: 0.005\n",
      "iteration: 140600 loss: 0.0037 lr: 0.005\n",
      "iteration: 140700 loss: 0.0037 lr: 0.005\n",
      "iteration: 140800 loss: 0.0035 lr: 0.005\n",
      "iteration: 140900 loss: 0.0041 lr: 0.005\n",
      "iteration: 141000 loss: 0.0045 lr: 0.005\n",
      "iteration: 141100 loss: 0.0036 lr: 0.005\n",
      "iteration: 141200 loss: 0.0041 lr: 0.005\n",
      "iteration: 141300 loss: 0.0044 lr: 0.005\n",
      "iteration: 141400 loss: 0.0041 lr: 0.005\n",
      "iteration: 141500 loss: 0.0041 lr: 0.005\n",
      "iteration: 141600 loss: 0.0037 lr: 0.005\n",
      "iteration: 141700 loss: 0.0046 lr: 0.005\n",
      "iteration: 141800 loss: 0.0042 lr: 0.005\n",
      "iteration: 141900 loss: 0.0037 lr: 0.005\n",
      "iteration: 142000 loss: 0.0034 lr: 0.005\n",
      "iteration: 142100 loss: 0.0044 lr: 0.005\n",
      "iteration: 142200 loss: 0.0037 lr: 0.005\n",
      "iteration: 142300 loss: 0.0040 lr: 0.005\n",
      "iteration: 142400 loss: 0.0040 lr: 0.005\n",
      "iteration: 142500 loss: 0.0040 lr: 0.005\n",
      "iteration: 142600 loss: 0.0040 lr: 0.005\n",
      "iteration: 142700 loss: 0.0038 lr: 0.005\n",
      "iteration: 142800 loss: 0.0037 lr: 0.005\n",
      "iteration: 142900 loss: 0.0035 lr: 0.005\n",
      "iteration: 143000 loss: 0.0038 lr: 0.005\n",
      "iteration: 143100 loss: 0.0037 lr: 0.005\n",
      "iteration: 143200 loss: 0.0042 lr: 0.005\n",
      "iteration: 143300 loss: 0.0040 lr: 0.005\n",
      "iteration: 143400 loss: 0.0044 lr: 0.005\n",
      "iteration: 143500 loss: 0.0039 lr: 0.005\n",
      "iteration: 143600 loss: 0.0040 lr: 0.005\n",
      "iteration: 143700 loss: 0.0039 lr: 0.005\n",
      "iteration: 143800 loss: 0.0036 lr: 0.005\n",
      "iteration: 143900 loss: 0.0040 lr: 0.005\n",
      "iteration: 144000 loss: 0.0036 lr: 0.005\n",
      "iteration: 144100 loss: 0.0039 lr: 0.005\n",
      "iteration: 144200 loss: 0.0051 lr: 0.005\n",
      "iteration: 144300 loss: 0.0040 lr: 0.005\n",
      "iteration: 144400 loss: 0.0040 lr: 0.005\n",
      "iteration: 144500 loss: 0.0035 lr: 0.005\n",
      "iteration: 144600 loss: 0.0041 lr: 0.005\n",
      "iteration: 144700 loss: 0.0042 lr: 0.005\n",
      "iteration: 144800 loss: 0.0036 lr: 0.005\n",
      "iteration: 144900 loss: 0.0038 lr: 0.005\n",
      "iteration: 145000 loss: 0.0043 lr: 0.005\n",
      "iteration: 145100 loss: 0.0049 lr: 0.005\n",
      "iteration: 145200 loss: 0.0047 lr: 0.005\n",
      "iteration: 145300 loss: 0.0033 lr: 0.005\n",
      "iteration: 145400 loss: 0.0040 lr: 0.005\n",
      "iteration: 145500 loss: 0.0036 lr: 0.005\n",
      "iteration: 145600 loss: 0.0039 lr: 0.005\n",
      "iteration: 145700 loss: 0.0035 lr: 0.005\n",
      "iteration: 145800 loss: 0.0045 lr: 0.005\n",
      "iteration: 145900 loss: 0.0035 lr: 0.005\n",
      "iteration: 146000 loss: 0.0043 lr: 0.005\n",
      "iteration: 146100 loss: 0.0043 lr: 0.005\n",
      "iteration: 146200 loss: 0.0039 lr: 0.005\n",
      "iteration: 146300 loss: 0.0041 lr: 0.005\n",
      "iteration: 146400 loss: 0.0044 lr: 0.005\n",
      "iteration: 146500 loss: 0.0038 lr: 0.005\n",
      "iteration: 146600 loss: 0.0034 lr: 0.005\n",
      "iteration: 146700 loss: 0.0038 lr: 0.005\n",
      "iteration: 146800 loss: 0.0042 lr: 0.005\n",
      "iteration: 146900 loss: 0.0040 lr: 0.005\n",
      "iteration: 147000 loss: 0.0040 lr: 0.005\n",
      "iteration: 147100 loss: 0.0036 lr: 0.005\n",
      "iteration: 147200 loss: 0.0038 lr: 0.005\n",
      "iteration: 147300 loss: 0.0040 lr: 0.005\n",
      "iteration: 147400 loss: 0.0044 lr: 0.005\n",
      "iteration: 147500 loss: 0.0042 lr: 0.005\n",
      "iteration: 147600 loss: 0.0037 lr: 0.005\n",
      "iteration: 147700 loss: 0.0034 lr: 0.005\n",
      "iteration: 147800 loss: 0.0040 lr: 0.005\n",
      "iteration: 147900 loss: 0.0041 lr: 0.005\n",
      "iteration: 148000 loss: 0.0037 lr: 0.005\n",
      "iteration: 148100 loss: 0.0039 lr: 0.005\n",
      "iteration: 148200 loss: 0.0045 lr: 0.005\n",
      "iteration: 148300 loss: 0.0038 lr: 0.005\n",
      "iteration: 148400 loss: 0.0041 lr: 0.005\n",
      "iteration: 148500 loss: 0.0044 lr: 0.005\n",
      "iteration: 148600 loss: 0.0039 lr: 0.005\n",
      "iteration: 148700 loss: 0.0042 lr: 0.005\n",
      "iteration: 148800 loss: 0.0036 lr: 0.005\n",
      "iteration: 148900 loss: 0.0039 lr: 0.005\n",
      "iteration: 149000 loss: 0.0034 lr: 0.005\n",
      "iteration: 149100 loss: 0.0033 lr: 0.005\n",
      "iteration: 149200 loss: 0.0040 lr: 0.005\n",
      "iteration: 149300 loss: 0.0039 lr: 0.005\n",
      "iteration: 149400 loss: 0.0041 lr: 0.005\n",
      "iteration: 149500 loss: 0.0043 lr: 0.005\n",
      "iteration: 149600 loss: 0.0038 lr: 0.005\n",
      "iteration: 149700 loss: 0.0038 lr: 0.005\n",
      "iteration: 149800 loss: 0.0045 lr: 0.005\n",
      "iteration: 149900 loss: 0.0037 lr: 0.005\n",
      "iteration: 150000 loss: 0.0035 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 83, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1115, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "deeplabcut.train_network(path_config_file, shuffle=13, displayiters=100, saveiters=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1308376,
     "status": "ok",
     "timestamp": 1663248263352,
     "user": {
      "displayName": "Bowen Jumbo",
      "userId": "15176400549218670876"
     },
     "user_tz": -60
    },
    "id": "OY0Rtu8aI9ia",
    "outputId": "d71140e3-44eb-4b18-fbfe-3f9c64d1acd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_Yolo_1000_trainAug7shuffle7_148000  with # of training iterations: 148000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:04, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148000\n",
      "Results for 148000  training iterations: 90 7 train error: 3.38 pixels. Test error: 4.11  pixels.\n",
      "With pcutoff of 0.8  train error: 3.09 pixels. Test error: 3.37 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet50_Yolo_1000_trainAug7shuffle7_148500  with # of training iterations: 148500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:04, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148500\n",
      "Results for 148500  training iterations: 90 7 train error: 2.98 pixels. Test error: 4.04  pixels.\n",
      "With pcutoff of 0.8  train error: 2.65 pixels. Test error: 2.87 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet50_Yolo_1000_trainAug7shuffle7_149000  with # of training iterations: 149000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:03, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149000\n",
      "Results for 149000  training iterations: 90 7 train error: 3.15 pixels. Test error: 5.21  pixels.\n",
      "With pcutoff of 0.8  train error: 2.77 pixels. Test error: 3.06 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet50_Yolo_1000_trainAug7shuffle7_149500  with # of training iterations: 149500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:03, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149500\n",
      "Results for 149500  training iterations: 90 7 train error: 3.08 pixels. Test error: 4.13  pixels.\n",
      "With pcutoff of 0.8  train error: 2.79 pixels. Test error: 3.08 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet50_Yolo_1000_trainAug7shuffle7_150000  with # of training iterations: 150000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:03, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-150000\n",
      "Results for 150000  training iterations: 90 7 train error: 2.9 pixels. Test error: 3.8  pixels.\n",
      "With pcutoff of 0.8  train error: 2.62 pixels. Test error: 3.0 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n",
      "Running  DLC_resnet101_Yolo_1000_trainAug7shuffle8_148000  with # of training iterations: 148000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:19, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148000\n",
      "Results for 148000  training iterations: 90 8 train error: 3.43 pixels. Test error: 8.06  pixels.\n",
      "With pcutoff of 0.8  train error: 2.87 pixels. Test error: 3.28 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet101_Yolo_1000_trainAug7shuffle8_148500  with # of training iterations: 148500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:18, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148500\n",
      "Results for 148500  training iterations: 90 8 train error: 3.13 pixels. Test error: 5.61  pixels.\n",
      "With pcutoff of 0.8  train error: 2.89 pixels. Test error: 4.04 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet101_Yolo_1000_trainAug7shuffle8_149000  with # of training iterations: 149000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:19, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149000\n",
      "Results for 149000  training iterations: 90 8 train error: 3.17 pixels. Test error: 7.84  pixels.\n",
      "With pcutoff of 0.8  train error: 2.81 pixels. Test error: 3.13 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet101_Yolo_1000_trainAug7shuffle8_149500  with # of training iterations: 149500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:19, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149500\n",
      "Results for 149500  training iterations: 90 8 train error: 2.99 pixels. Test error: 6.85  pixels.\n",
      "With pcutoff of 0.8  train error: 2.63 pixels. Test error: 3.1 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_resnet101_Yolo_1000_trainAug7shuffle8_150000  with # of training iterations: 150000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [01:19, 17.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-150000\n",
      "Results for 150000  training iterations: 90 8 train error: 2.78 pixels. Test error: 4.82  pixels.\n",
      "With pcutoff of 0.8  train error: 2.38 pixels. Test error: 2.76 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n",
      "Running  DLC_mobnet_100_Yolo_1000_trainAug7shuffle10_148000  with # of training iterations: 148000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:49, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148000\n",
      "Results for 148000  training iterations: 90 10 train error: 3.81 pixels. Test error: 4.29  pixels.\n",
      "With pcutoff of 0.8  train error: 2.8 pixels. Test error: 2.71 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_100_Yolo_1000_trainAug7shuffle10_148500  with # of training iterations: 148500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:49, 28.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148500\n",
      "Results for 148500  training iterations: 90 10 train error: 3.71 pixels. Test error: 4.64  pixels.\n",
      "With pcutoff of 0.8  train error: 2.57 pixels. Test error: 3.18 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_100_Yolo_1000_trainAug7shuffle10_149000  with # of training iterations: 149000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:49, 28.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149000\n",
      "Results for 149000  training iterations: 90 10 train error: 3.62 pixels. Test error: 4.57  pixels.\n",
      "With pcutoff of 0.8  train error: 2.44 pixels. Test error: 2.71 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_100_Yolo_1000_trainAug7shuffle10_149500  with # of training iterations: 149500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:49, 28.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149500\n",
      "Results for 149500  training iterations: 90 10 train error: 4.01 pixels. Test error: 4.71  pixels.\n",
      "With pcutoff of 0.8  train error: 3.0 pixels. Test error: 3.2 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_100_Yolo_1000_trainAug7shuffle10_150000  with # of training iterations: 150000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:49, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-150000\n",
      "Results for 150000  training iterations: 90 10 train error: 3.65 pixels. Test error: 6.01  pixels.\n",
      "With pcutoff of 0.8  train error: 2.44 pixels. Test error: 2.93 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n",
      "Running  DLC_mobnet_35_Yolo_1000_trainAug7shuffle13_148000  with # of training iterations: 148000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:46, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148000\n",
      "Results for 148000  training iterations: 90 13 train error: 8.13 pixels. Test error: 8.24  pixels.\n",
      "With pcutoff of 0.8  train error: 4.25 pixels. Test error: 3.95 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_35_Yolo_1000_trainAug7shuffle13_148500  with # of training iterations: 148500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:45, 30.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-148500\n",
      "Results for 148500  training iterations: 90 13 train error: 8.18 pixels. Test error: 8.2  pixels.\n",
      "With pcutoff of 0.8  train error: 3.86 pixels. Test error: 3.5 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_35_Yolo_1000_trainAug7shuffle13_149000  with # of training iterations: 149000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:46, 29.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149000\n",
      "Results for 149000  training iterations: 90 13 train error: 7.48 pixels. Test error: 8.57  pixels.\n",
      "With pcutoff of 0.8  train error: 3.48 pixels. Test error: 3.2 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_35_Yolo_1000_trainAug7shuffle13_149500  with # of training iterations: 149500\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:46, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-149500\n",
      "Results for 149500  training iterations: 90 13 train error: 7.67 pixels. Test error: 8.09  pixels.\n",
      "With pcutoff of 0.8  train error: 3.57 pixels. Test error: 3.21 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Running  DLC_mobnet_35_Yolo_1000_trainAug7shuffle13_150000  with # of training iterations: 150000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:45, 30.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-150000\n",
      "Results for 150000  training iterations: 90 13 train error: 8.11 pixels. Test error: 9.29  pixels.\n",
      "With pcutoff of 0.8  train error: 3.67 pixels. Test error: 4.33 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "deeplabcut.evaluate_network(path_config_file,Shuffles=[7,8,10,13], plotting=False)\n",
    "# param 'plotting': plots the preds on the train and test images.\n",
    "# https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#evaluate_network\n",
    "\n",
    "# Here you want to see a low pixel error! \n",
    "# Of course, it can only be as good as the labeler, \n",
    "# so be sure your labels are good! (And you have trained enough ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2049,
     "status": "ok",
     "timestamp": 1663598667761,
     "user": {
      "displayName": "Bowen Jumbo",
      "userId": "15176400549218670876"
     },
     "user_tz": -60
    },
    "id": "F0GLsDzxz9lR",
    "outputId": "2433f2fc-8b0a-4192-bfbe-6654f72391ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  /content/drive/My Drive/A_Final_project/new/resnet_101/Yolo_1000_train_150000_test/videos/part00010_0.mp4 and data.\n",
      "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.plot_trajectories(path_config_file, videofile_path_part00010_0,shuffle=10,videotype=VideoType)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsacsQ6QjF6otIyDs/ua04",
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
